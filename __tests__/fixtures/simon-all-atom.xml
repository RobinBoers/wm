<?xml version="1.0" encoding="utf-8"?>
<feed xml:lang="en-us" xmlns="http://www.w3.org/2005/Atom"><title>Simon Willison's Weblog: Entries</title><link href="http://simonwillison.net/" rel="alternate"></link><link href="http://simonwillison.net/atom/entries/" rel="self"></link><id>http://simonwillison.net/</id><updated>2019-05-19T22:15:56+00:00</updated><author><name>Simon Willison</name></author><entry><title>Datasette 0.28 - and why master should always be releasable</title><link href="http://simonwillison.net/2019/May/19/datasette-0-28/#atom-entries" rel="alternate"></link><published>2019-05-19T22:15:56+00:00</published><updated>2019-05-19T22:15:56+00:00</updated><id>http://simonwillison.net/2019/May/19/datasette-0-28/#atom-entries</id><summary type="html">&lt;p&gt;It's been quite a while since the last substantial release of Datasette. &lt;a href="https://datasette.readthedocs.io/en/stable/changelog.html#v0-27"&gt;Datasette 0.27&lt;/a&gt; came out all the way back in January.&lt;/p&gt;

&lt;p&gt;This isn't because development has slowed down. In fact, the project has had &lt;a href="https://github.com/simonw/datasette/compare/0.27...0.28"&gt;131 commits&lt;/a&gt; since then, covering a bewildering array of new functionality and with some significant contributions from developers who aren't me - Russ Garrett and Romain Primet deserve special recognition here.&lt;/p&gt;

&lt;p&gt;The problem has been one of discipline. I'm a big fan of the idea of keeping master shippable at all times in my professional work, but I hadn't quite adopted this policy for my open-source side projects. A couple of months ago I found myself in a situation where I had two major refactorings (of faceting and of Datasette's treatment of immutable files) going on in master at the same time, and untangling them turned out to take way longer than I had expected.&lt;/p&gt;

&lt;p&gt;So I've updated Datasette's &lt;a href="https://datasette.readthedocs.io/en/stable/contributing.html#general-guidelines"&gt;contribution guidelines&lt;/a&gt; to specify that &lt;strong&gt;master should always be releasable&lt;/strong&gt;, almost entirely as a reminder to myself.&lt;/p&gt;

&lt;p&gt;All of that said, I'm finally back out of the weeds and I'm excited to announce today's release of &lt;a href="https://pypi.org/project/datasette/0.28/"&gt;Datasette 0.28&lt;/a&gt;. It features a &lt;a href="https://adamj.eu/tech/2019/01/18/a-salmagundi-of-django-alpha-announcements/"&gt;salmagundi&lt;/a&gt; of new features! I'm replicating &lt;a href="https://datasette.readthedocs.io/en/stable/changelog.html#v0-28"&gt;the release notes&lt;/a&gt; below.&lt;/p&gt;

&lt;h3&gt;Supporting databases that change&lt;/h3&gt;

&lt;p&gt;From the beginning of the project, Datasette has been designed with read-only databases in mind. If a database is guaranteed not to change it opens up all kinds of interesting opportunities - from taking advantage of SQLite immutable mode and HTTP caching to bundling static copies of the database directly in a Docker container. &lt;a href="https://simonwillison.net/2018/Oct/4/datasette-ideas/"&gt;The interesting ideas in Datasette&lt;/a&gt; explores this idea in detail.&lt;/p&gt;

&lt;p&gt;As my goals for the project have developed, I realized that read-only databases are no longer the right default. SQLite actually supports concurrent access very well provided only one thread attempts to write to a database at a time, and I keep encountering sensible use-cases for running Datasette on top of a database that is processing inserts and updates.&lt;/p&gt;

&lt;p&gt;So, as-of version 0.28 Datasette no longer assumes that a database file will not change. It is now safe to point Datasette at a SQLite database which is being updated by another process.&lt;/p&gt;

&lt;p&gt;Making this change was a lot of work - see tracking tickets &lt;a href="https://github.com/simonw/datasette/issues/418"&gt;#418&lt;/a&gt;, &lt;a href="https://github.com/simonw/datasette/issues/419"&gt;#419&lt;/a&gt; and &lt;a href="https://github.com/simonw/datasette/issues/420"&gt;#420&lt;/a&gt;. It required new thinking around how Datasette should calculate table counts (an expensive operation against a large, changing database) and also meant reconsidering the “content hash” URLs Datasette has used in the past to optimize the performance of HTTP caches.&lt;/p&gt;

&lt;p&gt;Datasette can still run against immutable files and gains numerous performance benefits from doing so, but this is no longer the default behaviour. Take a look at the new &lt;a href="https://datasette.readthedocs.io/en/stable/performance.html#performance"&gt;Performance and caching&lt;/a&gt; documentation section for details on how to make the most of Datasette against data that you know will be staying read-only and immutable.&lt;/p&gt;

&lt;h3&gt;Faceting improvements, and faceting plugins&lt;/h3&gt;

&lt;p&gt;Datasette &lt;a href="https://datasette.readthedocs.io/en/stable/facets.html#facets"&gt;Facets&lt;/a&gt; provide an intuitive way to quickly summarize and interact with data. Previously the only supported faceting technique was column faceting, but 0.28 introduces two powerful new capibilities: facet-by-JSON-array and the ability to define further facet types using plugins.&lt;/p&gt;
&lt;p&gt;Facet by array (&lt;a href="https://github.com/simonw/datasette/issues/359"&gt;#359&lt;/a&gt;) is only available if your SQLite installation provides the &lt;code&gt;json1&lt;/code&gt; extension. Datasette will automatically detect columns that contain JSON arrays of values and offer a faceting interface against those columns - useful for modelling things like tags without needing to break them out into a new table. See &lt;a href="https://datasette.readthedocs.io/en/stable/facets.html#facet-by-json-array"&gt;Facet by JSON array&lt;/a&gt; for more.&lt;/p&gt;
&lt;p&gt;The new &lt;a href="https://datasette.readthedocs.io/en/stable/plugins.html#plugin-register-facet-classes"&gt;register_facet_classes()&lt;/a&gt; plugin hook (&lt;a href="https://github.com/simonw/datasette/pull/445"&gt;#445&lt;/a&gt;) can be used to register additional custom facet classes. Each facet class should provide two methods: &lt;code&gt;suggest()&lt;/code&gt; which suggests facet selections that might be appropriate for a provided SQL query, and &lt;code&gt;facet_results()&lt;/code&gt; which executes a facet operation and returns results. Datasette’s own faceting implementations have been refactored to use the same API as these plugins.&lt;/p&gt;

&lt;h3&gt;datasette publish cloudrun&lt;/h3&gt;

&lt;p&gt;&lt;a href="https://cloud.google.com/run/"&gt;Google Cloud Run&lt;/a&gt; is a brand new serverless hosting platform from Google, which allows you to build a Docker container which will run only when HTTP traffic is recieved and will shut down (and hence cost you nothing) the rest of the time. It’s similar to Zeit’s Now v1 Docker hosting platform which sadly is &lt;a href="https://hyperion.alpha.spectrum.chat/zeit/now/cannot-create-now-v1-deployments~d206a0d4-5835-4af5-bb5c-a17f0171fb25?m=MTU0Njk2NzgwODM3OA=="&gt;no longer accepting signups&lt;/a&gt; from new users.&lt;/p&gt;

&lt;p&gt;The new &lt;code&gt;datasette publish cloudrun&lt;/code&gt; command was contributed by Romain Primet (&lt;a href="https://github.com/simonw/datasette/pull/434"&gt;#434&lt;/a&gt;) and publishes selected databases to a new Datasette instance running on Google Cloud Run.&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://datasette.readthedocs.io/en/stable/publish.html#publish-cloud-run"&gt;Publishing to Google Cloud Run&lt;/a&gt; for full documentation.&lt;/p&gt;

&lt;h3&gt;register_output_renderer plugins&lt;/h3&gt;

&lt;p&gt;Russ Garrett implemented a new Datasette plugin hook called &lt;a href="https://datasette.readthedocs.io/en/stable/plugins.html#plugin-register-output-renderer"&gt;register_output_renderer&lt;/a&gt; (&lt;a href="https://github.com/simonw/datasette/pull/441"&gt;#441&lt;/a&gt;) which allows plugins to create additional output renderers in addition to Datasette’s default &lt;code&gt;.json&lt;/code&gt; and &lt;code&gt;.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Russ’s in-development &lt;a href="https://github.com/russss/datasette-geo"&gt;datasette-geo&lt;/a&gt; plugin includes &lt;a href="https://github.com/russss/datasette-geo/blob/d4cecc020848bbde91e9e17bf352f7c70bc3dccf/datasette_plugin_geo/geojson.py"&gt;an example&lt;/a&gt; of this hook being used to output &lt;code&gt;.geojson&lt;/code&gt; automatically converted from SpatiaLite.&lt;/p&gt;


&lt;h3&gt;Medium changes&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Datasette now conforms to the &lt;a href="https://github.com/python/black"&gt;Black coding style&lt;/a&gt; (&lt;a href="https://github.com/simonw/datasette/pull/449"&gt;#449&lt;/a&gt;) - and has a unit test to enforce this in the future&lt;/li&gt;
&lt;li&gt;New &lt;a href="https://datasette.readthedocs.io/en/stable/json_api.html#json-api-table-arguments"&gt;Special table arguments&lt;/a&gt;:
    &lt;ul&gt;
    &lt;li&gt;&lt;code&gt;?columnname__in=value1,value2,value3&lt;/code&gt; filter for executing SQL IN queries against a table, see &lt;a href="https://datasette.readthedocs.io/en/stable/json_api.html#table-arguments"&gt;Table arguments&lt;/a&gt; (&lt;a href="https://github.com/simonw/datasette/issues/433"&gt;#433&lt;/a&gt;)&lt;/li&gt;
    &lt;li&gt;&lt;code&gt;?columnname__date=yyyy-mm-dd&lt;/code&gt; filter which returns rows where the spoecified datetime column falls on the specified date (&lt;a href="https://github.com/simonw/datasette/commit/583b22aa28e26c318de0189312350ab2688c90b1"&gt;583b22a&lt;/a&gt;)&lt;/li&gt;
    &lt;li&gt;&lt;code&gt;?tags__arraycontains=tag&lt;/code&gt; filter which acts against a JSON array contained in a column (&lt;a href="https://github.com/simonw/datasette/commit/78e45ead4d771007c57b307edf8fc920101f8733"&gt;78e45ea&lt;/a&gt;)&lt;/li&gt;
    &lt;li&gt;&lt;code&gt;?_where=sql-fragment&lt;/code&gt; filter for the table view  (&lt;a href="https://github.com/simonw/datasette/issues/429"&gt;#429&lt;/a&gt;)&lt;/li&gt;
    &lt;li&gt;&lt;code&gt;?_fts_table=mytable&lt;/code&gt; and &lt;code&gt;?_fts_pk=mycolumn&lt;/code&gt; querystring options can be used to specify which FTS table to use for a search query - see &lt;a href="https://datasette.readthedocs.io/en/stable/full_text_search.html#full-text-search-table-or-view"&gt;Configuring full-text search for a table or view&lt;/a&gt; (&lt;a href="https://github.com/simonw/datasette/issues/428"&gt;#428&lt;/a&gt;)&lt;/li&gt;
    &lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;You can now pass the same table filter multiple times - for example, &lt;code&gt;?content__not=world&amp;amp;content__not=hello&lt;/code&gt; will return all rows where the content column is neither &lt;code&gt;hello&lt;/code&gt; or &lt;code&gt;world&lt;/code&gt; (&lt;a href="https://github.com/simonw/datasette/issues/288"&gt;#288&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;You can now specify &lt;code&gt;about&lt;/code&gt; and &lt;code&gt;about_url&lt;/code&gt; metadata (in addition to &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;license&lt;/code&gt;) linking to further information about a project - see &lt;a href="https://datasette.readthedocs.io/en/stable/metadata.html#metadata-source-license-about"&gt;Source, license and about&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New &lt;code&gt;?_trace=1&lt;/code&gt; parameter now adds debug information showing every SQL query that was executed while constructing the page (&lt;a href="https://github.com/simonw/datasette/issues/435"&gt;#435&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;datasette inspect&lt;/code&gt; now just calculates table counts, and does not introspect other database metadata (&lt;a href="https://github.com/simonw/datasette/issues/462"&gt;#462&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Removed &lt;code&gt;/-/inspect&lt;/code&gt; page entirely - this will be replaced by something similar in the future, see &lt;a href="https://github.com/simonw/datasette/issues/465"&gt;#465&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Datasette can now run against an in-memory SQLite database. You can do this by starting it without passing any files or by using the new &lt;code&gt;--memory&lt;/code&gt; option to &lt;code&gt;datasette serve&lt;/code&gt;. This can be useful for experimenting with SQLite queries that do not access any data, such as &lt;code&gt;SELECT 1+1&lt;/code&gt; or &lt;code&gt;SELECT sqlite_version()&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;Small changes&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;We now show the size of the database file next to the download link (&lt;a href="https://github.com/simonw/datasette/issues/172"&gt;#172&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;New &lt;code&gt;/-/databases&lt;/code&gt; introspection page shows currently connected databases (&lt;a href="https://github.com/simonw/datasette/issues/470"&gt;#470&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Binary data is no longer displayed on the table and row pages (&lt;a href="https://github.com/simonw/datasette/pull/442"&gt;#442&lt;/a&gt; - thanks, Russ Garrett)&lt;/li&gt;
&lt;li&gt;New show/hide SQL links on custom query pages (&lt;a href="https://github.com/simonw/datasette/issues/415"&gt;#415&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://datasette.readthedocs.io/en/stable/plugins.html#plugin-hook-extra-body-script"&gt;extra_body_script&lt;/a&gt; plugin hook now accepts an optional &lt;code&gt;view_name&lt;/code&gt; argument (&lt;a href="https://github.com/simonw/datasette/pull/443"&gt;#443&lt;/a&gt; - thanks, Russ Garrett)&lt;/li&gt;
&lt;li&gt;Bumped Jinja2 dependency to 2.10.1 (&lt;a href="https://github.com/simonw/datasette/pull/426"&gt;#426&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;All table filters are now documented, and documentation is enforced via unit tests (&lt;a href="https://github.com/simonw/datasette/commit/2c19a27d15a913e5f3dd443f04067169a6f24634"&gt;2c19a27&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;New project guideline: master should stay shippable at all times! (&lt;a href="https://github.com/simonw/datasette/commit/31f36e1b97ccc3f4387c80698d018a69798b6228"&gt;31f36e1&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Fixed a bug where &lt;code&gt;sqlite_timelimit()&lt;/code&gt; occasionally failed to clean up after itself (&lt;a href="https://github.com/simonw/datasette/commit/bac4e01f40ae7bd19d1eab1fb9349452c18de8f5"&gt;bac4e01&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;We no longer load additional plugins when executing pytest (&lt;a href="https://github.com/simonw/datasette/issues/438"&gt;#438&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Homepage now links to database views if there are less than five tables in a database (&lt;a href="https://github.com/simonw/datasette/issues/373"&gt;#373&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;--cors&lt;/code&gt; option is now respected by error pages (&lt;a href="https://github.com/simonw/datasette/issues/453"&gt;#453&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;datasette publish heroku&lt;/code&gt; now uses the &lt;code&gt;--include-vcs-ignore&lt;/code&gt; option, which means it works under Travis CI (&lt;a href="https://github.com/simonw/datasette/pull/407"&gt;#407&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;datasette publish heroku&lt;/code&gt; now publishes using Python 3.6.8 (&lt;a href="https://github.com/simonw/datasette/commit/666c37415a898949fae0437099d62a35b1e9c430"&gt;666c374&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Renamed &lt;code&gt;datasette publish now&lt;/code&gt; to &lt;code&gt;datasette publish nowv1&lt;/code&gt; (&lt;a href="https://github.com/simonw/datasette/issues/472"&gt;#472&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;datasette publish nowv1&lt;/code&gt; now accepts multiple &lt;code&gt;--alias&lt;/code&gt; parameters (&lt;a href="https://github.com/simonw/datasette/commit/09ef305c687399384fe38487c075e8669682deb4"&gt;09ef305&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Removed the &lt;code&gt;datasette skeleton&lt;/code&gt; command (&lt;a href="https://github.com/simonw/datasette/issues/476"&gt;#476&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;The &lt;a href="https://datasette.readthedocs.io/en/stable/contributing.html#contributing-documentation"&gt;documentation on how to build the documentation&lt;/a&gt; now recommends &lt;code&gt;sphinx-autobuild&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;</summary><category term="opensource"></category><category term="projects"></category><category term="datasette"></category></entry><entry><title>Running Datasette on Glitch</title><link href="http://simonwillison.net/2019/Apr/23/datasette-glitch/#atom-entries" rel="alternate"></link><published>2019-04-23T04:08:53+00:00</published><updated>2019-04-23T04:08:53+00:00</updated><id>http://simonwillison.net/2019/Apr/23/datasette-glitch/#atom-entries</id><summary type="html">&lt;p&gt;The worst part of any software project is setting up a development environment. It’s by far the biggest barrier for anyone trying to get started learning to code. I’ve been a developer for more than twenty years and I still feel the pain any time I want to do something new.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://glitch.com/"&gt;Glitch&lt;/a&gt; is the most promising attempt I’ve ever seen at tackling this problem. It provides an entirely browser-based development environment that allows you to edit code, see the results instantly and view and remix the source code of other people’s projects.&lt;/p&gt;
&lt;p&gt;It’s developed into a really fun, super-creative community and a fantastic resource for people looking to get started in the ever-evolving world of software development.&lt;/p&gt;
&lt;p&gt;This evening I decided to get &lt;a href="https://datasette.readthedocs.io/"&gt;Datasette&lt;/a&gt; running on it. I’m really impressed with how well it works, and I think Glitch provides an excellent environment for experimenting with Datasette and &lt;a href="https://datasette.readthedocs.io/en/stable/ecosystem.html"&gt;related tools&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;TLDR version: visit &lt;a href="https://glitch.com/edit/#!/remix/datasette-csvs"&gt;https://glitch.com/edit/#!/remix/datasette-csvs&lt;/a&gt; right now, drag-and-drop in a CSV file and watch it get served by Datasette on Glitch just a few seconds later.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Running_Python_on_Glitch_12"&gt;&lt;/a&gt;Running Python on Glitch&lt;/h3&gt;
&lt;p&gt;The Glitch documentation is all about Node.js and JavaScript, but they actually have very solid Python support as well.&lt;/p&gt;
&lt;p&gt;Every Glitch project runs in a container that includes Python 2.7.12 and Python 3.5.2, and you can use &lt;code&gt;pip install --user&lt;/code&gt; or &lt;code&gt;pip3 install --user&lt;/code&gt; to install Python dependencies.&lt;/p&gt;
&lt;p&gt;The key to running non-JavaScript projects on Glitch is the &lt;code&gt;glitch.json&lt;/code&gt; file format. You can use this to specify an &lt;code&gt;install&lt;/code&gt; script, which sets up your container, and a &lt;code&gt;start&lt;/code&gt; script, which starts your application running. Glitch will route HTTP traffic to port 3000, so your application server needs to listen on that port.&lt;/p&gt;
&lt;p&gt;This means the most basic Glitch project to run Datasette looks like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datasette-basic.glitch.me/"&gt;https://datasette-basic.glitch.me/&lt;/a&gt; (&lt;a href="https://glitch.com/edit/#!/datasette-basic"&gt;view source&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;It contains a single &lt;code&gt;glitch.json&lt;/code&gt; file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;install&amp;quot;: &amp;quot;pip3 install --user datasette&amp;quot;,
    &amp;quot;start&amp;quot;: &amp;quot;datasette -p 3000&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This installs Datasette using &lt;code&gt;pip3&lt;/code&gt;, then runs it on port 3000.&lt;/p&gt;
&lt;p&gt;Since there’s no actual data to serve, this is a pretty boring demo. The most interesting page is this one, which shows the installed versions of the software:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datasette-basic.glitch.me/-/versions"&gt;https://datasette-basic.glitch.me/-/versions&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="Something_more_interesting_datasettecsvs_37"&gt;&lt;/a&gt;Something more interesting: datasette-csvs&lt;/h3&gt;
&lt;p&gt;Let’s build one with some actual data.&lt;/p&gt;
&lt;p&gt;My &lt;a href="https://github.com/simonw/csvs-to-sqlite"&gt;csvs-to-sqlite&lt;/a&gt; tool converts CSV files into a SQLite database. Since it’s also written in Python we can run it against CSV files as part of the Glitch install script.&lt;/p&gt;
&lt;p&gt;Glitch provides a special directory called &lt;code&gt;.data/&lt;/code&gt; which can be used as a persistent file storage space that won’t be cleared in between restarts. The following &lt;code&gt;&amp;quot;install&amp;quot;&lt;/code&gt; script installs &lt;code&gt;datasette&lt;/code&gt; and &lt;code&gt;csvs-to-sqlite&lt;/code&gt;, then runs the latter to create a SQLite database from all available CSV files:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
    &amp;quot;install&amp;quot;:  &amp;quot;pip3 install --user datasette csvs-to-sqlite &amp;amp;&amp;amp; csvs-to-sqlite *.csv .data/csv-data.db&amp;quot;,
    &amp;quot;start&amp;quot;: &amp;quot;datasette .data/csv-data.db -p 3000&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can simply drag and drop CSV files into the root of the Glitch project and they will be automatically converted into a SQLite database and served using Datasette!&lt;/p&gt;
&lt;p&gt;We need a couple of extra details. Firstly, we want Datasette to automatically re-build the database file any time a new CSV file is added or an existing CSV file is changed. We can do that by adding a &lt;code&gt;&amp;quot;watch&amp;quot;&lt;/code&gt; block to &lt;code&gt;glitch.json&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;watch&amp;quot;: {
    &amp;quot;install&amp;quot;: {
        &amp;quot;include&amp;quot;: [
            &amp;quot;\\.csv$&amp;quot;
        ]
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This ensures that our &lt;code&gt;&amp;quot;install&amp;quot;&lt;/code&gt; script will run again any time a CSV file changes.&lt;/p&gt;
&lt;p&gt;Let’s tone down the rate at which the scripts execute, by using &lt;code&gt;throttle&lt;/code&gt; to set the polling interval to once a second:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;throttle&amp;quot;: 1000
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above almost worked, but I started seeing errors if I changed the number of columns in a CSV file, since doing so clashed with the schema that had already been created in the database.&lt;/p&gt;
&lt;p&gt;My solution was to add code to the install script that would delete the SQLite database file before attempting to recreate it - using the &lt;code&gt;rm ... || true&lt;/code&gt; idiom to prevent Glitch from failing the installation if the file it attempted to remove did not already exist.&lt;/p&gt;
&lt;p&gt;My final &lt;code&gt;glitch.json&lt;/code&gt; file looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;install&amp;quot;: &amp;quot;pip3 install --user datasette csvs-to-sqlite &amp;amp;&amp;amp; rm .data/csv-data.db || true &amp;amp;&amp;amp; csvs-to-sqlite *.csv .data/csv-data.db&amp;quot;,
  &amp;quot;start&amp;quot;: &amp;quot;datasette .data/csv-data.db -p 3000 -m metadata.json&amp;quot;,
  &amp;quot;watch&amp;quot;: {
    &amp;quot;install&amp;quot;: {
      &amp;quot;include&amp;quot;: [
        &amp;quot;\\.csv$&amp;quot;
      ]
    },
    &amp;quot;restart&amp;quot;: {
      &amp;quot;include&amp;quot;: [
        &amp;quot;^metadata.json$&amp;quot;
      ]
    },
    &amp;quot;throttle&amp;quot;: 1000
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I also set it up to use &lt;a href="https://datasette.readthedocs.io/en/stable/metadata.html"&gt;Datasette’s metadata.json format&lt;/a&gt;, and automatically restart the server any time the contents of that file changes.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datasette-csvs.glitch.me/"&gt;https://datasette-csvs.glitch.me/&lt;/a&gt;  (&lt;a href="https://glitch.com/edit/#!/datasette-csvs"&gt;view source&lt;/a&gt;) shows the results, running against a simple &lt;code&gt;example.csv&lt;/code&gt; file I created.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Remixing_98"&gt;&lt;/a&gt;Remixing!&lt;/h3&gt;
&lt;p&gt;Here’s where things get really fun: Glitch projects support “remixing”, whereby anyone can click a link to create their own editable copy of a project.&lt;/p&gt;
&lt;p&gt;Remixing works even if you aren’t logged in to Glitch! Anonymous projects expire after five days, so be sure to sign in with GitHub or Facebook if you want to keep yours around.&lt;/p&gt;
&lt;p&gt;Try it out now: Visit &lt;a href="https://glitch.com/edit/#!/remix/datasette-csvs"&gt;https://glitch.com/edit/#!/remix/datasette-csvs&lt;/a&gt; to create your own remix of my project. Then drag a new CSV file directly into the editor and within a few seconds Datasette on Glitch will be up and running against a converted copy of your file!&lt;/p&gt;
&lt;h3&gt;&lt;a id="Limitations_106"&gt;&lt;/a&gt;Limitations&lt;/h3&gt;
&lt;p&gt;The Glitch help center article &lt;a href="https://glitch.com/help/restrictions/"&gt;What technical restrictions are in place?&lt;/a&gt; describes their limits. Most importantly, projects are limited to 4,000 requests an hour - and there’s currently no way to increase that limit. They also limit projects to 200MB of disk space - easily enough to get started exploring some interesting CSV files with Datasette.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Next_steps_110"&gt;&lt;/a&gt;Next steps&lt;/h3&gt;
&lt;p&gt;I’m delighted at how easy this was to setup, and how much power the ability to remix these Datasette demos provides. I’m tempted to start creating remixable Glitch demos that illustrate other aspects of Datasette’s functionality such as &lt;a href="https://datasette.readthedocs.io/en/stable/plugins.html"&gt;plugins&lt;/a&gt; or &lt;a href="https://datasette.readthedocs.io/en/stable/full_text_search.html"&gt;full-text search&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Glitch is an exceptionally cool piece of software. I look forward to seeing their Python support continue to evolve.&lt;/p&gt;</summary><category term="glitch"></category><category term="hosting"></category><category term="projects"></category><category term="datasette"></category></entry><entry><title>Generating a commit log for San Francisco's official list of trees</title><link href="http://simonwillison.net/2019/Mar/13/tree-history/#atom-entries" rel="alternate"></link><published>2019-03-13T14:49:48+00:00</published><updated>2019-03-13T14:49:48+00:00</updated><id>http://simonwillison.net/2019/Mar/13/tree-history/#atom-entries</id><summary type="html">&lt;p&gt;San Francisco has a &lt;a href="https://datasf.org/"&gt;neat open data portal&lt;/a&gt; (as do an &lt;a href="https://opendatainception.io/"&gt;increasingly large number&lt;/a&gt; of cities these days). For a few years my favourite file on there has been &lt;a href="https://data.sfgov.org/City-Infrastructure/Street-Tree-List/tkzw-k3nq"&gt;Street Tree List&lt;/a&gt;, a list of all 190,000 trees in the city maintained by the Department of Public Works.&lt;/p&gt;
&lt;p&gt;I’ve been using that file for Datasette demos &lt;a href="https://simonwillison.net/2017/Nov/25/new-in-datasette/"&gt;for a while now&lt;/a&gt;, but last week I noticed something intriguing: the file had been recently updated. On closer inspection it turns out it’s updated on a regular basis! I had assumed it was a static snapshot of trees at a certain point in time, but I was wrong: &lt;code&gt;Street_Tree_List.csv&lt;/code&gt; is a living document.&lt;/p&gt;
&lt;p&gt;Back in September 2017 I built a &lt;a href="https://simonwillison.net/2017/Sep/10/scraping-irma/"&gt;scraping project relating to hurricane Irma&lt;/a&gt;. The idea was to take data sources like FEMA’s list of open shelters and track them over time, by scraping them into a git repository and committing after every fetch.&lt;/p&gt;
&lt;p&gt;I’ve been meaning to spend more time with this idea, and building a commit log for San Francisco’s trees looked like an ideal opportunity to do so.&lt;/p&gt;
&lt;h3&gt;&lt;a id="sftreehistory_8"&gt;&lt;/a&gt;sf-tree-history&lt;/h3&gt;
&lt;p&gt;Here’s the result: &lt;a href="https://github.com/simonw/sf-tree-history"&gt;sf-tree-history&lt;/a&gt;, a git repository dedicated to recording the history of changes made to the official list of San Francisco’s trees. The repo contains three things: the latest copy of &lt;code&gt;Street_Tree_List.csv&lt;/code&gt;, a &lt;code&gt;README&lt;/code&gt;, and a &lt;a href="https://github.com/simonw/sf-tree-history/blob/master/.circleci/config.yml"&gt;Circle CI configuration&lt;/a&gt; that grabs a new copy of the file every night and, if it has changed, commits it to git and pushes the result to GitHub.&lt;/p&gt;
&lt;p&gt;The most interesting part of the repo is the &lt;a href="https://github.com/simonw/sf-tree-history/commits/master"&gt;commit history&lt;/a&gt; itself. I’ve only been running the script for just over a week, but I already have some useful illustrative commits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/simonw/sf-tree-history/commit/7ab432cdcb8d7914cfea4a5b59803f38cade532b"&gt;7ab432cdcb8d7914cfea4a5b59803f38cade532b&lt;/a&gt; from March 6th records three new trees added to the file: two Monterey Pines and a Blackwood Acacia.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/simonw/sf-tree-history/commit/d6b258959af9546909b2eee836f0156ed88cd45d"&gt;d6b258959af9546909b2eee836f0156ed88cd45d&lt;/a&gt; from March 12th shows four changes made to existing records. Of particular interest: TreeID 235981 (a Cherry Plum) had its address updated from 412 Webster St to 410 Webster St and its latitude and longitude tweaked a little bit as well.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/simonw/sf-tree-history/commit/ca66d9a5fdd632549301d249c487004a5b68abf2"&gt;ca66d9a5fdd632549301d249c487004a5b68abf2&lt;/a&gt; lists 2151 rows changed, 1280 rows added! I found an old copy of &lt;code&gt;Street_Tree_List.csv&lt;/code&gt; on my laptop from April 2018, so for fun I loaded it into the repository and used &lt;code&gt;git commit amend&lt;/code&gt; to back-date the commit to almost a year ago. I generated a commit message between that file and the version from 9 days ago which came in at around 10,000 lines of text. Git handled that just fine, but GitHub’s web view &lt;a href="https://github.com/simonw/sf-tree-history/commit/ca66d9a5fdd632549301d249c487004a5b68abf2"&gt;sadly truncates it&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;a id="csvdiff_18"&gt;&lt;/a&gt;csv-diff&lt;/h3&gt;
&lt;p&gt;One of the things I learned from my hurricane Irma project was the importance of human-readable commit messages that summarize the detected changes. I initially wrote some code to generate those by hand, but then realized that this could be extracted into a reusable tool.&lt;/p&gt;
&lt;p&gt;The result is &lt;a href="https://github.com/simonw/csv-diff"&gt;csv-diff&lt;/a&gt;, a tiny Python CLI tool which can generate a human (or machine) readable version of the differences between two CSV files.&lt;/p&gt;
&lt;p&gt;Using it looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ csv-diff one.csv two.csv --key=id
1 row added, 1 row removed, 1 row changed

1 row added

  {&amp;quot;id&amp;quot;: &amp;quot;3&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Bailey&amp;quot;, &amp;quot;age&amp;quot;: &amp;quot;1&amp;quot;}

1 row removed

  {&amp;quot;id&amp;quot;: &amp;quot;2&amp;quot;, &amp;quot;name&amp;quot;: &amp;quot;Pancakes&amp;quot;, &amp;quot;age&amp;quot;: &amp;quot;2&amp;quot;}

1 row changed

  Row 1
    age: &amp;quot;4&amp;quot; =&amp;gt; &amp;quot;5&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href="https://github.com/simonw/csv-diff/blob/master/README.md"&gt;csv-diff README&lt;/a&gt; has further details on the tool.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Circle_CI_44"&gt;&lt;/a&gt;Circle CI&lt;/h3&gt;
&lt;p&gt;My favourite thing about the &lt;code&gt;sf-tree-history&lt;/code&gt; project is that it costs me nothing to run - either in hosting costs or (hopefully) in terms of ongoing maintenance.&lt;/p&gt;
&lt;p&gt;The git repository is hosted for free on GitHub. Because it’s a public project, &lt;a href="https://circleci.com/"&gt;Circle CI&lt;/a&gt; will run tasks against it for free.&lt;/p&gt;
&lt;p&gt;My &lt;a href="https://github.com/simonw/sf-tree-history/blob/master/.circleci/config.yml"&gt;.circleci/config.yml&lt;/a&gt; does the rest. It uses Circle’s &lt;a href="https://circleci.com/docs/2.0/workflows/#scheduling-a-workflow"&gt;cron syntax&lt;/a&gt; to schedule a task that runs every night. The task then runs this script (embedded in the YAML configuration):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cp Street_Tree_List.csv Street_Tree_List-old.csv
curl -o Street_Tree_List.csv &amp;quot;https://data.sfgov.org/api/views/tkzw-k3nq/rows.csv?accessType=DOWNLOAD&amp;quot;
git add Street_Tree_List.csv
git config --global user.email &amp;quot;treebot@example.com&amp;quot;
git config --global user.name &amp;quot;Treebot&amp;quot;
sudo pip install csv-diff
csv-diff Street_Tree_List-old.csv Street_Tree_List.csv --key=TreeID &amp;gt; message.txt
git commit -F message.txt &amp;amp;&amp;amp; \
  git push -q https://${GITHUB_PERSONAL_TOKEN}@github.com/simonw/sf-tree-history.git master \
  || true
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This script does all of the work.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First it backs up the existing &lt;code&gt;Street_Tree_list.csv&lt;/code&gt; as &lt;code&gt;Street_Tree_List-old.csv&lt;/code&gt;, in order to be able to run a comparison later.&lt;/li&gt;
&lt;li&gt;It downloads the latest copy of &lt;code&gt;Street_Tree_List.csv&lt;/code&gt; from the San Francisco data portal&lt;/li&gt;
&lt;li&gt;It adds the file to the git index and sets itself an identity for use in the commit&lt;/li&gt;
&lt;li&gt;It installs my &lt;code&gt;csv-diff&lt;/code&gt; utility &lt;a href="https://pypi.org/project/csv-diff/"&gt;from PyPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;It uses &lt;code&gt;csv-diff&lt;/code&gt; to create a diff of the two files, and writes that diff to a new file called &lt;code&gt;message.txt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finally, it attempts to create a new commit using &lt;code&gt;message.txt&lt;/code&gt; as the commit message, then pushes the result to GitHub&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The last line is the most complex. Circle CI will mark a build as failed if any of the commands in the &lt;code&gt;run&lt;/code&gt; block return a non-0 exit code. &lt;code&gt;git commit&lt;/code&gt; returns a non-0 exit code if you attempt to run it but none of the files have changed.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git commit ... &amp;amp;&amp;amp; git push ... || true&lt;/code&gt; ensures that if &lt;code&gt;git commit&lt;/code&gt; succeeds the &lt;code&gt;git push&lt;/code&gt; command will be run, BUT if it fails the &lt;code&gt;|| true&lt;/code&gt; will still return a 0 exit code for the overall line - so Circle CI will not mark the build as failed.&lt;/p&gt;
&lt;p&gt;There’s one last trick here: I’m using &lt;code&gt;git push -q https://${GITHUB_PERSONAL_TOKEN}@github.com/simonw/sf-tree-history.git master&lt;/code&gt; to push my changes to GitHub. This takes advantage of Circle CI environment variables, which are &lt;a href="https://circleci.com/docs/2.0/env-vars/"&gt;the recommended way&lt;/a&gt; to configure secrets such that they cannot be viewed by anyone browsing &lt;a href="https://circleci.com/gh/simonw/sf-tree-history"&gt;your Circle CI builds&lt;/a&gt;. I created a &lt;a href="https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line"&gt;personal GitHub auth token&lt;/a&gt; for this project, which I’m using to allow Circle CI to push commits to GitHub on my behalf.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Next_steps_78"&gt;&lt;/a&gt;Next steps&lt;/h3&gt;
&lt;p&gt;I’m really excited about this pattern of using GitHub in combination with Circle CI to track changes to any file that is being posted on the internet. I’m opening up the code (and my &lt;a href="https://github.com/simonw/csv-diff"&gt;csv-diff utility&lt;/a&gt;) in the hope that other people will use them to set up their own tracking projects. Who knows, maybe there’s a file out there that’s even more exciting than San Francisco’s official list of trees!&lt;/p&gt;</summary><category term="csv"></category><category term="datajournalism"></category><category term="git"></category><category term="projects"></category><category term="sanfrancisco"></category></entry><entry><title>I commissioned an oil painting of Barbra Streisand’s cloned dogs</title><link href="http://simonwillison.net/2019/Mar/7/oil-painting/#atom-entries" rel="alternate"></link><published>2019-03-07T15:09:18+00:00</published><updated>2019-03-07T15:09:18+00:00</updated><id>http://simonwillison.net/2019/Mar/7/oil-painting/#atom-entries</id><summary type="html">&lt;p&gt;
    &lt;figure style="margin: 0"&gt;
        &lt;img src="https://static.simonwillison.net/static/2019/oil-painting.jpg" alt="Two dogs in a stroller looking at a gravestone, as an oil painting" style="max-width: 100%" /&gt;
        &lt;figcaption style="font-style: italic; text-align: center"&gt;Two identical puffs of white fur, gazing at the tombstone of the dog they are&lt;/figcaption&gt;
    &lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Last year, Barbra Streisand &lt;a href="https://www.nytimes.com/2018/02/28/science/barbra-streisand-clone-dogs.html" title="Barbra Streisand Cloned Her Dog. For $50,000, You Can Clone Yours."&gt;cloned her dog&lt;/a&gt;, Sammie.&lt;/p&gt;

&lt;p&gt;The story is fascinating, as is the background reading on dog cloning &lt;a href="https://www.vanityfair.com/style/2018/08/dog-cloning-animal-sooam-hwang" title="Inside the Very Big, Very Controversial Business of Dog Cloning"&gt;from Vanity Fair&lt;/a&gt;. But the thing that really stuck with me was the photograph that accompanied &lt;a href="https://www.nytimes.com/2018/03/02/style/barbra-streisand-cloned-her-dog.html"&gt;"Barbra Streisand Explains: Why I Cloned My Dog"&lt;/a&gt; in the New York Times:&lt;/p&gt;

&lt;img src="https://static.simonwillison.net/static/2019/04STREISAND-clones-facebookJumbo.jpg" alt="Two dogs in a stroller looking at a gravestone" style="max-width: 100%" /&gt;

&lt;p&gt;David Ewing Duncan in Vanity Fair &lt;a href="https://www.vanityfair.com/style/2018/08/dog-cloning-animal-sooam-hwang"&gt;described the scenario like this&lt;/a&gt;: &lt;em&gt;Barbra Streisand, visiting the grave of her beloved Sammie, with Miss Violet and Miss Scarlett perched next to her in their stroller—two identical puffs of white fur, gazing at the tombstone of the dog they are&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This photograph says so much about the age that we live in. I couldn't get it out of my head.&lt;/p&gt;

&lt;p&gt;I've long been fascinated by Dafen, the town in a China that &lt;a href="https://www.artsy.net/article/artsy-editorial-village-60-worlds-paintings-future-jeopardy" title="The World’s Art Factory Is in Jeopardy"&gt;was once responsible&lt;/a&gt; for 60% of the world's oil paintings - mostly replicas, but today &lt;a href="http://global.chinadaily.com.cn/a/201903/07/WS5c806aeba3106c65c34ed34f.html" title="Dafen moves from producing art replicas to being a hub of creation"&gt;increasingly original artwork&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I always wanted to commission an oil painting from China, but I never quite found the right subject... until now.&lt;/p&gt;

&lt;p&gt;There's something deliciously appropriate about using a painting cloning service to clone a photograph of some cloned dogs.&lt;/p&gt;

&lt;p&gt;So I uploaded a copy of the photo to &lt;a href="https://www.instapainting.com/"&gt;Instapainting&lt;/a&gt; and entered a few extra instructions:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Please paint the stroller closer to the gravestone - adjust the composition so that it fits the 12x16 dimensions while maintaining the two key elements of the image: the stroller with the two dogs in it and the gravestone that they are looking at&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;A couple of months later, a tube arrived in the mail. I opened it up... and they had nailed it. If anything the painting is an improvement on the original photograph: the composition is tighter, the stroller no longer has its wheel cut off, some dead plants in the background (which I had not even noticed) are given a bit more prominence, and the little doggy faces have exactly the right expressions of mild existential dread.&lt;/p&gt;

&lt;p&gt;So thank you Alice Wu at &lt;a href="https://www.instapainting.com/artists/dearchicarts"&gt;Xiamen Dearchic Arts&lt;/a&gt; - I could not be happier.&lt;/p&gt;

&lt;p&gt;With a painting this good, obviously it needed to be framed. I took it to &lt;a href="http://www.underglassframing.com/"&gt;Underglass Framing&lt;/a&gt; in San Francisco's Hayes Valley and told them I was looking for something with an air of existential dread. "I think we can do that" they said.&lt;/p&gt;

&lt;img src="https://static.simonwillison.net/static/2019/oil-painting-framed.jpg" alt="Two dogs in a stroller looking at a gravestone, as an oil painting in an intimidating frame" style="max-width: 100%" /&gt;

&lt;p&gt;Natalie says I can keep it in the guest bathroom.&lt;/p&gt;</summary><category term="art"></category><category term="projects"></category></entry><entry><title>sqlite-utils: a Python library and CLI tool for building SQLite databases</title><link href="http://simonwillison.net/2019/Feb/25/sqlite-utils/#atom-entries" rel="alternate"></link><published>2019-02-25T03:29:20+00:00</published><updated>2019-02-25T03:29:20+00:00</updated><id>http://simonwillison.net/2019/Feb/25/sqlite-utils/#atom-entries</id><summary type="html">&lt;p&gt;&lt;a href="https://github.com/simonw/sqlite-utils"&gt;sqlite-utils&lt;/a&gt; is a combination Python library and command-line tool I’ve been building over the past six months which aims to make creating new SQLite databases as quick and easy as possible.&lt;/p&gt;
&lt;p&gt;It’s part of &lt;a href="https://datasette.readthedocs.io/en/stable/ecosystem.html"&gt;the ecosystem of tools&lt;/a&gt; I’m building around my &lt;a href="https://datasette.readthedocs.io/"&gt;Datasette&lt;/a&gt; project.&lt;/p&gt;
&lt;p&gt;I spent the weekend adding all kinds of exciting command-line options to it, so I’m ready to describe it to the world.&lt;/p&gt;
&lt;h3&gt;&lt;a id="A_Python_library_for_quickly_creating_databases_8"&gt;&lt;/a&gt;A Python library for quickly creating databases&lt;/h3&gt;
&lt;p&gt;A core idea behind Datasette is that &lt;a href="https://www.sqlite.org/"&gt;SQLite&lt;/a&gt; is the ideal format for publishing all kinds of interesting structured data. Datasette takes any SQLite database and adds a browsable web interface, &lt;a href="https://datasette.readthedocs.io/en/stable/json_api.html"&gt;a JSON API&lt;/a&gt; and the ability to &lt;a href="https://datasette.readthedocs.io/en/stable/csv_export.html"&gt;export tables and queries as CSV&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The other half of the equation then is tools to create SQLite databases. &lt;a href="https://github.com/simonw/csvs-to-sqlite"&gt;csvs-to-sqlite&lt;/a&gt; was my first CLI attempt at this. &lt;code&gt;sqlite-utils&lt;/code&gt; takes a much more flexible and comprehensive approach.&lt;/p&gt;
&lt;p&gt;I started working on &lt;code&gt;sqlite-utils&lt;/code&gt; last year as part of my project to &lt;a href="https://simonwillison.net/2018/Aug/6/russian-facebook-ads/"&gt;Analyze US Election Russian Facebook Ads&lt;/a&gt;. The initial aim was to build a library that made constructing new SQLite databases inside of a &lt;a href="https://jupyter.org/"&gt;Jupyter notebook&lt;/a&gt; as productive as possible.&lt;/p&gt;
&lt;p&gt;The core idea behind the library is that you can give it a list of Python dictionaries (equivalent to JSON objects) and it will automatically create a SQLite table with the correct schema, then insert those items into the new table.&lt;/p&gt;
&lt;p&gt;To illustrate, let’s create a database using &lt;a href="https://data.nasa.gov/resource/y77d-th95.json"&gt;this JSON file of meteorite landings&lt;/a&gt; released by NASA (discovered via &lt;a href="https://github.com/jdorfman/awesome-json-datasets"&gt;awesome-json-datasets&lt;/a&gt; curated by Justin Dorfman).&lt;/p&gt;
&lt;p&gt;Here’s the quickest way in code to turn that into a database:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import requests
import sqlite_utils

db = sqlite_utils.Database(&amp;quot;meteorites.db&amp;quot;)
db[&amp;quot;meteorites&amp;quot;].insert_all(
    requests.get(
        &amp;quot;https://data.nasa.gov/resource/y77d-th95.json&amp;quot;
    ).json(),
    pk=&amp;quot;id&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This two lines of code creates a new SQLite database on disk called &lt;code&gt;meteorites.db&lt;/code&gt;, creates a table in that file called &lt;code&gt;meteorites&lt;/code&gt;, detects the necessary columns based on the incoming data, inserts all of the rows and sets the &lt;code&gt;id&lt;/code&gt; column up as the primary key.&lt;/p&gt;
&lt;p&gt;To see the resulting database, run &lt;code&gt;datasette meteorites.db&lt;/code&gt; and browse to &lt;code&gt;http://127.0.0.1:8001/&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can do a &lt;em&gt;lot more&lt;/em&gt; with the library. You can create tables, insert and upsert data in bulk, configure foreign key relationships, configure SQLite full-text search and much more. I encourage you to &lt;a href="https://sqlite-utils.readthedocs.io/en/latest/python-api.html"&gt;consult the documentation&lt;/a&gt; for all of the details.&lt;/p&gt;
&lt;h3&gt;&lt;a id="The_sqliteutils_commandline_tool_39"&gt;&lt;/a&gt;The sqlite-utils command-line tool&lt;/h3&gt;
&lt;p&gt;This is the new stuff built over the past few days, and I think it’s really fun.&lt;/p&gt;
&lt;p&gt;First install the tool &lt;a href="https://pypi.org/project/sqlite-utils/"&gt;from PyPI&lt;/a&gt;, using &lt;code&gt;pip3 install sqlite-utils&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Let’s start by grabbing a copy of &lt;a href="https://static.simonwillison.net/static/2019/russian-ads.db"&gt;the russian-ads.db database&lt;/a&gt; I created in &lt;a href="https://simonwillison.net/2018/Aug/6/russian-facebook-ads/"&gt;Analyzing US Election Russian Facebook Ads&lt;/a&gt; (4MB):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd /tmp
$ wget https://static.simonwillison.net/static/2019/russian-ads.db
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see a list of tables in the database and their counts using the &lt;code&gt;tables&lt;/code&gt; command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sqlite-utils tables russian-ads.db --counts
[{&amp;quot;table&amp;quot;: &amp;quot;ads&amp;quot;, &amp;quot;count&amp;quot;: 3498},
 {&amp;quot;table&amp;quot;: &amp;quot;targets&amp;quot;, &amp;quot;count&amp;quot;: 1665},
 {&amp;quot;table&amp;quot;: &amp;quot;ad_targets&amp;quot;, &amp;quot;count&amp;quot;: 36559},
 {&amp;quot;table&amp;quot;: &amp;quot;ads_fts&amp;quot;, &amp;quot;count&amp;quot;: 3498},
 {&amp;quot;table&amp;quot;: &amp;quot;ads_fts_segments&amp;quot;, &amp;quot;count&amp;quot;: 120},
 {&amp;quot;table&amp;quot;: &amp;quot;ads_fts_segdir&amp;quot;, &amp;quot;count&amp;quot;: 1},
 {&amp;quot;table&amp;quot;: &amp;quot;ads_fts_docsize&amp;quot;, &amp;quot;count&amp;quot;: 3498},
 {&amp;quot;table&amp;quot;: &amp;quot;ads_fts_stat&amp;quot;, &amp;quot;count&amp;quot;: 1}]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By default, &lt;code&gt;sqlite-utils&lt;/code&gt; outputs data as neatly formatted JSON. You can get CSV instead using the &lt;code&gt;--csv&lt;/code&gt; option:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sqlite-utils tables russian-ads.db --counts --csv
table,count
ads,3498
targets,1665
ad_targets,36559
ads_fts,3498
ads_fts_segments,120
ads_fts_segdir,1
ads_fts_docsize,3498
ads_fts_stat,1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or if you want a pretty ASCII-art table, use &lt;code&gt;--table&lt;/code&gt; (or the shortcut, &lt;code&gt;-t&lt;/code&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sqlite-utils tables russian-ads.db --counts -t
table               count
----------------  -------
ads                  3498
targets              1665
ad_targets          36559
ads_fts              3498
ads_fts_segments      120
ads_fts_segdir          1
ads_fts_docsize      3498
ads_fts_stat            1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The table view is built on top of &lt;a href="https://pypi.org/project/tabulate/"&gt;tabulate&lt;/a&gt;, which offers dozens of table variations. Run &lt;code&gt;sqlite-utils tables --help&lt;/code&gt; for the full list - try &lt;code&gt;--table -fmt=rst&lt;/code&gt; for output that can be pasted directly into a reStructuredText document (handy for writing documentation).&lt;/p&gt;
&lt;p&gt;So far we’ve just looked at a list of tables. Lets run a SQL query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sqlite-utils russian-ads.db &amp;quot;select category, count(*) from targets group by category&amp;quot;
[{&amp;quot;category&amp;quot;: &amp;quot;accessing_facebook_on&amp;quot;, &amp;quot;count(*)&amp;quot;: 1},
 {&amp;quot;category&amp;quot;: &amp;quot;age&amp;quot;, &amp;quot;count(*)&amp;quot;: 82},
 {&amp;quot;category&amp;quot;: &amp;quot;and_must_also_match&amp;quot;, &amp;quot;count(*)&amp;quot;: 228},
 {&amp;quot;category&amp;quot;: &amp;quot;army_reserve_industry&amp;quot;, &amp;quot;count(*)&amp;quot;: 3},
 {&amp;quot;category&amp;quot;: &amp;quot;behaviors&amp;quot;, &amp;quot;count(*)&amp;quot;: 16},
 ...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, this can be output as CSV using &lt;code&gt;--csv&lt;/code&gt;, or a table with &lt;code&gt;--table&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The default JSON output is objects wrapped in an array. Use &lt;code&gt;--arrays&lt;/code&gt; to get an array of arrays instead. More interestingly: &lt;code&gt;--nl&lt;/code&gt; causes the data to be output as &lt;a href="http://ndjson.org/"&gt;newline-delimited JSON&lt;/a&gt;, like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sqlite-utils russian-ads.db &amp;quot;select category, count(*) from targets group by category&amp;quot; --nl
{&amp;quot;category&amp;quot;: &amp;quot;accessing_facebook_on&amp;quot;, &amp;quot;count(*)&amp;quot;: 1}
{&amp;quot;category&amp;quot;: &amp;quot;age&amp;quot;, &amp;quot;count(*)&amp;quot;: 82}
{&amp;quot;category&amp;quot;: &amp;quot;and_must_also_match&amp;quot;, &amp;quot;count(*)&amp;quot;: 228}
{&amp;quot;category&amp;quot;: &amp;quot;army_reserve_industry&amp;quot;, &amp;quot;count(*)&amp;quot;: 3}
{&amp;quot;category&amp;quot;: &amp;quot;behaviors&amp;quot;, &amp;quot;count(*)&amp;quot;: 16}
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is a really interesting format for piping to other tools.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Creating_databases_from_JSON_on_the_commandline_115"&gt;&lt;/a&gt;Creating databases from JSON on the command-line&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;sqlite-utils insert&lt;/code&gt; command can be used to create new tables by piping JSON or CSV directly into the tool. It’s the command-line equivalent of the &lt;code&gt;.insert_all()&lt;/code&gt; Python function I demonstrated earlier.&lt;/p&gt;
&lt;p&gt;Here’s how to create that meteorite database directly from the command-line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl &amp;quot;https://data.nasa.gov/resource/y77d-th95.json&amp;quot; | \
    sqlite-utils insert meteorites.db meteorites - --pk=id
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will use a SQLite database file called &lt;code&gt;meteorites.db&lt;/code&gt; (creating one if it does not yet exist), create or use a table called &lt;code&gt;meteorites&lt;/code&gt; and read the data from standard in (hence the pipe). You can pass a filename instead of a &lt;code&gt;-&lt;/code&gt; here to read data from a file on disk.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;insert&lt;/code&gt; command accepts multiple formats - it defaults to expecting a JSON array of objects, but you can use &lt;code&gt;--nl&lt;/code&gt; to accept newline-delimited JSON and &lt;code&gt;--csv&lt;/code&gt; to accept CSV.&lt;/p&gt;
&lt;p&gt;This means you can combine the tools! Let’s create a brand new database by exporting data from the old one, using newline-delimited JSON as the intermediary format:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ sqlite-utils russian-ads.db \
    &amp;quot;select * from ads where text like '%veterans%'&amp;quot; --nl | \
    sqlite-utils insert veterans.db ads - --nl
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This creates a new file called &lt;code&gt;veterans.db&lt;/code&gt; containing an &lt;code&gt;ads&lt;/code&gt; table with just the ads that mentioned veterans somewhere in their body text.&lt;/p&gt;
&lt;p&gt;Since we’re working with JSON, we can introduce other command-line tools into the mix.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://stedolan.github.io/jq/"&gt;jq&lt;/a&gt; is a neat little tool for extracting data from a JSON file using its own mini domain-specific language.&lt;/p&gt;
&lt;p&gt;The Nobel Prize API offers &lt;a href="http://api.nobelprize.org/v1/laureate.json"&gt;a JSON file&lt;/a&gt; listing all of the Nobel laureates - but they are contained as an array in a top level &lt;code&gt;&amp;quot;laureates&amp;quot;&lt;/code&gt; key. &lt;code&gt;sqlite-utils&lt;/code&gt; needs a flat array - so we can use &lt;code&gt;jq&lt;/code&gt; to get exactly that:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl &amp;quot;http://api.nobelprize.org/v1/laureate.json&amp;quot; | \
  jq &amp;quot;.laureates&amp;quot; | \
  sqlite-utils insert nobel.db laureates -
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a file called &lt;code&gt;nobel.db&lt;/code&gt; containing all of the Nobel laureates.&lt;/p&gt;
&lt;p&gt;Since Datasette recently &lt;a href="https://datasette.readthedocs.io/en/stable/changelog.html#v0-27"&gt;grew the ability to export newline-delimited JSON&lt;/a&gt;, we can also use this ability to directly consume data from Datasette. Lets grab &lt;a href="https://fivethirtyeight.datasettes.com/fivethirtyeight-aa93d24/bob-ross%2Felements-by-episode?BEACH=1"&gt;every episode of the Joy of Painting in which Bob Ross painted a beach&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ curl &amp;quot;https://fivethirtyeight.datasettes.com/fivethirtyeight-aa93d24/bob-ross%2Felements-by-episode.json?_facet=BEACH&amp;amp;BEACH=1&amp;amp;_shape=array&amp;amp;_nl=on&amp;quot; \
| sqlite-utils insert bob.db beach_episodes - --nl
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="Plenty_more_features_153"&gt;&lt;/a&gt;Plenty more features&lt;/h3&gt;
&lt;p&gt;As with the Python API, the &lt;code&gt;sqlite-utils&lt;/code&gt; CLI tool has dozens of other options and &lt;a href="https://sqlite-utils.readthedocs.io/en/latest/cli.html"&gt;extensive documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve been really enjoying growing an &lt;a href="https://datasette.readthedocs.io/en/stable/ecosystem.html"&gt;ecosystem of tools around Datasette&lt;/a&gt;. &lt;code&gt;sqlite-utils&lt;/code&gt; is the keystone here: it’s fundamental to other tools I’m building, such as &lt;a href="https://github.com/simonw/db-to-sqlite"&gt;db-to-sqlite&lt;/a&gt; (which can export any SQLAlchemy-supported database directly to a SQLite file on disk).&lt;/p&gt;
&lt;p&gt;I’ve found myself increasingly turning to SQLite first for all kinds of ad-hoc analysis, and I’m excited to try out these new command-line abilities of &lt;code&gt;sqlite-utils&lt;/code&gt; for real-world data spelunking tasks.&lt;/p&gt;</summary><category term="opensource"></category><category term="projects"></category><category term="sqlite"></category><category term="datasette"></category></entry><entry><title>Exploring search relevance algorithms with SQLite</title><link href="http://simonwillison.net/2019/Jan/7/exploring-search-relevance-algorithms-sqlite/#atom-entries" rel="alternate"></link><published>2019-01-07T03:29:29+00:00</published><updated>2019-01-07T03:29:29+00:00</updated><id>http://simonwillison.net/2019/Jan/7/exploring-search-relevance-algorithms-sqlite/#atom-entries</id><summary type="html">&lt;p&gt;&lt;a href="https://www.sqlite.org/index.html"&gt;SQLite&lt;/a&gt; isn’t just a fast, high quality embedded database: it also incorporates a powerful full-text search engine in the form of the &lt;a href="https://www.sqlite.org/fts3.html"&gt;FTS4&lt;/a&gt; and &lt;a href="https://sqlite.org/fts5.html"&gt;FTS5&lt;/a&gt; extensions. You’ve probably used these a bunch of times already: many iOS, Android and desktop applications use SQLite under-the-hood and use it to implement their built-in search.&lt;/p&gt;
&lt;p&gt;I’ve been using these capabilities for &lt;a href="https://datasette.readthedocs.io/en/stable/full_text_search.html"&gt;basic search in Datasette&lt;/a&gt; for over a year now, but I’ve recently started digging into some of their more advanced features. It turns out hacking around with SQLite is a great way to learn more about how fundamental information retrieval algorithms work under the hood.&lt;/p&gt;
&lt;p&gt;Today I’m releasing &lt;a href="https://github.com/simonw/sqlite-fts4"&gt;sqlite-fts4&lt;/a&gt; - a Python package that provides a collection of custom SQL functions for working with SQLite’s FTS4 module. It includes some neat tools for introspecting how relevancy ranking algorithms actually work.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Why_not_just_use_FTS5_8"&gt;&lt;/a&gt;Why not just use FTS5?&lt;/h3&gt;
&lt;p&gt;If it’s available to you FTS5 is usually the best option: it has a good ranking algorithm built in. I described how to use it to build &lt;a href="https://24ways.org/2018/fast-autocomplete-search-for-your-website/"&gt;fast autocomplete search for your website&lt;/a&gt; for the 2018 &lt;a href="https://24ways.org/"&gt;24 ways advent calendar&lt;/a&gt;. You can join directly against a virtual table and order by a pre-calculated relevance score accessible through that table.&lt;/p&gt;
&lt;p&gt;What makes FTS4 interesting is that it doesn’t include a scoring mechanism: it instead exposes raw statistical data to you in a way that lets you build your own ranking functions.&lt;/p&gt;
&lt;p&gt;You probably don’t &lt;em&gt;need&lt;/em&gt; to do this - unless you are stuck on an older SQLite version that doesn’t support the latest features. But… if you’re interested in understanding more about how search actually works, the need to implement a ranking function is an excellent learning learning opportunity.&lt;/p&gt;
&lt;p&gt;I’ll be demonstrating these functions using a hosted Datasette instance running at &lt;a href="https://datasette-sqlite-fts4.datasette.io/"&gt;datasette-sqlite-fts4.datasette.io&lt;/a&gt; (with the data from &lt;a href="https://24ways.org/2018/fast-autocomplete-search-for-your-website/"&gt;my 24 ways article&lt;/a&gt;). You can play with them out there, or if you want to use your own Datasette instance you can enable these custom SQL functions by pip installing my new &lt;a href="https://github.com/simonw/datasette-sqlite-fts4"&gt;datasette-sqlite-fts4&lt;/a&gt; plugin.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Raw_FTS4_matchinfo_data_18"&gt;&lt;/a&gt;Raw FTS4 matchinfo() data&lt;/h3&gt;
&lt;p&gt;When using FTS4, the only scoring help SQLite gives you is the bulit-in &lt;a href="https://www.sqlite.org/fts3.html#matchinfo"&gt;matchinfo() function&lt;/a&gt;. For each document in your search result set, this function will expose raw statistical data that can be used to calculate a score.&lt;/p&gt;
&lt;p&gt;Let’s try it out using the following query:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;select
    *, matchinfo(articles_fts, &amp;quot;pcx&amp;quot;)
from
    articles_fts
where
    articles_fts match :search
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://datasette-sqlite-fts4.datasette.io/24ways-fts4-52e8a02?sql=select%0D%0A++++*%2C+matchinfo%28articles_fts%2C+%22pcx%22%29%0D%0Afrom%0D%0A++++articles_fts%0D%0Awhere%0D%0A++++articles_fts+match+%3Asearch&amp;amp;search=jquery+maps"&gt;Run matchinfo() in Datasette&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;pcx&lt;/code&gt; here is called the format string - it lets SQLite know what information about the match you would like to see.&lt;/p&gt;
&lt;p&gt;The results are returned as a binary string! For the first matching document, we get back the following:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;\x02\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x01\x00\x00\x00\xa3\x00\x00\x00\x1f\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\x02\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x00\x00\\\x00\x00\x00\x15\x00\x00\x00&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;SQLite’s C heritage is showing through here.&lt;/p&gt;
&lt;h3&gt;&lt;a id="decode_matchinfo_to_decode_the_binary_41"&gt;&lt;/a&gt;decode_matchinfo() to decode the binary&lt;/h3&gt;
&lt;p&gt;The first step in working with matchinfo is to decode that binary string. It’s actually a sequence of unsigned 32 bit integers. We can turn it into a Python list of integers using the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;struct.unpack(&amp;quot;I&amp;quot; * (len(matchinfo) // 4), matchinfo)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;sqlite-fts4&lt;/code&gt; exposes a SQL function called &lt;code&gt;decode_matchinfo()&lt;/code&gt; which does exactly this. Let’s expand our example to use it:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;select
    title, author,
    decode_matchinfo(matchinfo(articles_fts, &amp;quot;pcx&amp;quot;)),
    matchinfo(articles_fts, &amp;quot;pcx&amp;quot;)
from
    articles_fts
where
    articles_fts match :search
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://datasette-sqlite-fts4.datasette.io/24ways-fts4-52e8a02?sql=select%0D%0A++++title%2C+author%2C%0D%0A++++decode_matchinfo%28matchinfo%28articles_fts%2C+%22pcx%22%29%29%2C%0D%0A++++matchinfo%28articles_fts%2C+%22pcx%22%29%0D%0Afrom%0D%0A++++articles_fts%0D%0Awhere%0D%0A++++articles_fts+match+%3Asearch&amp;amp;search=jquery+maps"&gt;Run decode_matchinfo() in Datasette&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The matchinfo for our first matching document now looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[2, 3, 0, 2, 2, 0, 0, 0, 1, 163, 31, 0, 2, 2, 0, 0, 0, 2, 92, 21]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Better, but still obscure. What does it mean?&lt;/p&gt;
&lt;p&gt;The anwser lies in the &lt;a href="https://www.sqlite.org/fts3.html#matchinfo"&gt;SQLite matchinfo documentation&lt;/a&gt;. In our format string, we requested &lt;code&gt;p&lt;/code&gt;, &lt;code&gt;c&lt;/code&gt; and &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;p&lt;/code&gt; requests a single integer reprenting the number of search terms we are matching. Since our search query is &lt;code&gt;jquery maps&lt;/code&gt; this is 2 - it’s the first integer in the list.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;c&lt;/code&gt; requests the number of searchable columns in our table. We created &lt;code&gt;articles_fts&lt;/code&gt; with 3 columns, so it’s 3. That’s the second integer in the list.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;x&lt;/code&gt; is much more interesting: it returns 3 integer values for each term/column combination. Since we have 2 terms and 3 columns that means we get back 6 * 3 = 18 integers. If you count the items in the array above you’ll see there are 18 left after you remove the first two. Each triple represents the number of times the term appears in the current column, the number of times it appears in this column across every row and the number of total documents that match the term in this column at least once.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Search relevancy scores are usually calculated against exactly this kind of collection of statistics: we rank based on how rare the matching terms are across the rest of the corpus.&lt;/p&gt;
&lt;h3&gt;&lt;a id="annotate_matchinfo_to_annotate_the_integers_74"&gt;&lt;/a&gt;annotate_matchinfo() to annotate the integers&lt;/h3&gt;
&lt;p&gt;Having a list of integers made things easier, but still not easy enough. That’s where &lt;code&gt;annotate_matchinfo()&lt;/code&gt; comes in. This custom SQL function expands the matchinfo list of integers into a giant JSON object describing exactly what each of the results means.&lt;/p&gt;
&lt;p&gt;We’ll try it out like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;select
    title, author,
    decode_matchinfo(matchinfo(articles_fts, &amp;quot;pcx&amp;quot;)),
    json_object(&amp;quot;pre&amp;quot;, annotate_matchinfo(matchinfo(articles_fts, &amp;quot;pcx&amp;quot;), &amp;quot;pcx&amp;quot;))
from
    articles_fts
where
    articles_fts match :search
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://datasette-sqlite-fts4.datasette.io/24ways-fts4-52e8a02?sql=select%0D%0A++++title%2C+author%2C%0D%0A++++decode_matchinfo%28matchinfo%28articles_fts%2C+%22pcx%22%29%29%2C%0D%0A++++json_object%28%22pre%22%2C+annotate_matchinfo%28matchinfo%28articles_fts%2C+%22pcx%22%29%2C+%22pcx%22%29%29%0D%0Afrom%0D%0A++++articles_fts%0D%0Awhere%0D%0A++++articles_fts+match+%3Asearch&amp;amp;search=jquery+maps"&gt;Run annotate_matchinfo() in Datasette&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note that we have to provide the format string twice, so that &lt;code&gt;annotate_matchinfo()&lt;/code&gt; knows the requested order of the binary matchinfo data.&lt;/p&gt;
&lt;p&gt;This returns a JSON object that looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;p&amp;quot;: {
    &amp;quot;title&amp;quot;: &amp;quot;Number of matchable phrases in the query&amp;quot;,
    &amp;quot;value&amp;quot;: 2,
    &amp;quot;idx&amp;quot;: 0
  },
  &amp;quot;c&amp;quot;: {
    &amp;quot;title&amp;quot;: &amp;quot;Number of user defined columns in the FTS table&amp;quot;,
    &amp;quot;value&amp;quot;: 3,
    &amp;quot;idx&amp;quot;: 1
  },
  &amp;quot;x&amp;quot;: {
    &amp;quot;title&amp;quot;: &amp;quot;Details for each phrase/column combination&amp;quot;
    &amp;quot;value&amp;quot;: [
      ...
      {
        &amp;quot;phrase_index&amp;quot;: 0,
        &amp;quot;column_index&amp;quot;: 2,
        &amp;quot;hits_this_column_this_row&amp;quot;: 1,
        &amp;quot;hits_this_column_all_rows&amp;quot;: 163,
        &amp;quot;docs_with_hits&amp;quot;: 31,
        &amp;quot;idxs&amp;quot;: [8, 9, 10]
      },
      {
        &amp;quot;phrase_index&amp;quot;: 1,
        &amp;quot;column_index&amp;quot;: 0,
        &amp;quot;hits_this_column_this_row&amp;quot;: 0,
        &amp;quot;hits_this_column_all_rows&amp;quot;: 2,
        &amp;quot;docs_with_hits&amp;quot;: 2,
        &amp;quot;idxs&amp;quot;: [11, 12, 13]
      }...
    ],
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Try it out &lt;a href="https://datasette-sqlite-fts4.datasette.io/24ways-fts4-52e8a02?sql=select%0D%0A++++title%2C+author%2C%0D%0A++++decode_matchinfo%28matchinfo%28articles_fts%2C+%22pcx%22%29%29%2C%0D%0A++++json_object%28%22pre%22%2C+annotate_matchinfo%28matchinfo%28articles_fts%2C+%22pcxnalyb%22%29%2C+%22pcxnalyb%22%29%29%0D%0Afrom%0D%0A++++articles_fts%0D%0Awhere%0D%0A++++articles_fts+match+%3Asearch&amp;amp;search=jquery+maps"&gt;with pcxnalyb&lt;/a&gt; to see the complete set of format string options.&lt;/p&gt;
&lt;p&gt;You may be wondering why I wrapped that function call in &lt;code&gt;json_object(&amp;quot;pre&amp;quot;, ...)&lt;/code&gt;. This is a Datasette trick: I recently added the ability to pretty-print JSON to my &lt;code&gt;datasette-html-json&lt;/code&gt; plugin - see that package’s README &lt;a href="https://github.com/simonw/datasette-json-html#preformatted-text"&gt;for details&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Building_ranking_functions_134"&gt;&lt;/a&gt;Building ranking functions&lt;/h3&gt;
&lt;p&gt;These statistics are everything we need to calculate relevance scores. &lt;code&gt;sqlite-fts4&lt;/code&gt; implements two such functions: &lt;code&gt;rank_score()&lt;/code&gt; is a simple TF/IDF function. &lt;code&gt;rank_bm25()&lt;/code&gt; is much more interesting - it’s an implementation of the &lt;a href="https://en.wikipedia.org/wiki/Okapi_BM25"&gt;Okapi BM25&lt;/a&gt;, inspired by the one that ships &lt;a href="https://github.com/coleifer/peewee/blob/fb538c2b2fcc835b9eb7226e56682fbbce49de0f/playhouse/sqlite_ext.py#L1134"&gt;with the peewee ORM&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let’s try them both out:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;select
    title, author,
    rank_score(matchinfo(articles_fts, &amp;quot;pcx&amp;quot;)) as score,
    rank_bm25(matchinfo(articles_fts, &amp;quot;pcnalx&amp;quot;)) as bm25,
    json_object(&amp;quot;pre&amp;quot;, annotate_matchinfo(matchinfo(articles_fts, &amp;quot;pcxnalyb&amp;quot;), &amp;quot;pcxnalyb&amp;quot;))
from
    articles_fts
where
    articles_fts match :search
order by bm25
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://datasette-sqlite-fts4.datasette.io/24ways-fts4-52e8a02?sql=select%0D%0A++++title%2C+author%2C%0D%0A++++rank_score%28matchinfo%28articles_fts%2C+%22pcx%22%29%29+as+score%2C%0D%0A++++rank_bm25%28matchinfo%28articles_fts%2C+%22pcnalx%22%29%29+as+bm25%2C%0D%0A++++json_object%28%22pre%22%2C+annotate_matchinfo%28matchinfo%28articles_fts%2C+%22pcxnalyb%22%29%2C+%22pcxnalyb%22%29%29%0D%0Afrom%0D%0A++++articles_fts%0D%0Awhere%0D%0A++++articles_fts+match+%3Asearch%0D%0Aorder+by+bm25&amp;amp;search=jquery+maps"&gt;Try rank_score() and rank_bm25() in Datasette&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can switch the &lt;code&gt;order by&lt;/code&gt; clause between &lt;a href="https://datasette-sqlite-fts4.datasette.io/24ways-fts4-52e8a02?sql=select%0D%0A++++title%2C+author%2C%0D%0A++++rank_score%28matchinfo%28articles_fts%2C+%22pcx%22%29%29+as+score%2C%0D%0A++++rank_bm25%28matchinfo%28articles_fts%2C+%22pcnalx%22%29%29+as+bm25%0D%0Afrom%0D%0A++++articles_fts%0D%0Awhere%0D%0A++++articles_fts+match+%3Asearch%0D%0Aorder+by+bm25&amp;amp;search=jquery+maps"&gt;bm25&lt;/a&gt; and &lt;a href="https://datasette-sqlite-fts4.datasette.io/24ways-fts4-52e8a02?sql=select%0D%0A++++title%2C+author%2C%0D%0A++++rank_score%28matchinfo%28articles_fts%2C+%22pcx%22%29%29+as+score%2C%0D%0A++++rank_bm25%28matchinfo%28articles_fts%2C+%22pcnalx%22%29%29+as+bm25%0D%0Afrom%0D%0A++++articles_fts%0D%0Awhere%0D%0A++++articles_fts+match+%3Asearch%0D%0Aorder+by+score&amp;amp;search=jquery+maps"&gt;score&lt;/a&gt; to compare the two.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bm25()&lt;/code&gt; is definitely a better option. It’s the default algorithm used these days by Elasticsearch, and they wrote up &lt;a href="https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables"&gt;an excellent explanation of how it works&lt;/a&gt;  on their blog.&lt;/p&gt;
&lt;p&gt;Take a look at &lt;a href="https://github.com/simonw/sqlite-fts4/blob/2316cca52ff2a23ebbb09cf655609ea9e59ceb1b/sqlite_fts4/__init__.py#L197-L266"&gt;the source code for the ranking functions&lt;/a&gt; to see how they are implemented. They work against the data structure returned by &lt;code&gt;annotate_matchinfo()&lt;/code&gt; to try and make it clear what is going on.&lt;/p&gt;
&lt;p&gt;Building the &lt;code&gt;rank_bm25()&lt;/code&gt; function took me longer than I expected: I was comparing my results against &lt;code&gt;bm25()&lt;/code&gt; from &lt;a href="https://github.com/coleifer/peewee"&gt;peewee&lt;/a&gt; to ensure I was getting them right, but I couldn’t get them to match. After &lt;a href="https://gist.github.com/simonw/e0b9156d66b41b172a66d0cfe32d9391"&gt;some furious debugging&lt;/a&gt; I finally figured out the problem: peewee had a rare bug! I &lt;a href="https://github.com/coleifer/peewee/issues/1826"&gt;reported it to Charles Leifer&lt;/a&gt; and he analyzed it and turned around a fix in a matter of hours - it turns out the C library that peewee had ported to Python &lt;a href="https://github.com/rads/sqlite-okapi-bm25/issues/2"&gt;had the same problem&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Next_steps_161"&gt;&lt;/a&gt;Next steps&lt;/h3&gt;
&lt;p&gt;I’m really impressed with the flexibility that FTS4 provides - it turns out FTS5 isn’t the only worthwhile option for search in SQLite&lt;/p&gt;
&lt;p&gt;I’m thinking about ways to expose some of the bm25 tuning parameters (in particular the magic B and K1 constants &lt;a href="https://www.elastic.co/blog/practical-bm25-part-2-the-bm25-algorithm-and-its-variables"&gt;explained by the Elasticsearch article&lt;/a&gt;) and I plan to upgrade Datasette’s search functionality to make ranking available as a first-class feature on the &lt;a href="https://datasette.readthedocs.io/en/stable/pages.html#table"&gt;searchable table view&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’m also generally excited about SQLite as a learning tool for exploring different search ranking mechanisms. Once you’ve decoded that binary matchinfo string it’s impressive how much you can get done with the underlying data.&lt;/p&gt;</summary><category term="fulltextsearch"></category><category term="projects"></category><category term="search"></category><category term="sqlite"></category><category term="datasette"></category></entry><entry><title>Fast Autocomplete Search for Your Website</title><link href="http://simonwillison.net/2018/Dec/19/fast-autocomplete-search/#atom-entries" rel="alternate"></link><published>2018-12-19T04:11:09+00:00</published><updated>2018-12-19T04:11:09+00:00</updated><id>http://simonwillison.net/2018/Dec/19/fast-autocomplete-search/#atom-entries</id><summary type="html">&lt;p&gt;Every website deserves a great search engine - but building a search engine can be a lot of work, and hosting it can quickly get expensive.&lt;/p&gt;
&lt;p&gt;I’m going to build a search engine for 24 ways that’s fast enough to support autocomplete (a.k.a. typeahead) search queries and can be hosted for free. I’ll be using wget, Python, SQLite, Jupyter, sqlite-utils and my open source &lt;a href="https://datasette.readthedocs.io/"&gt;Datasette&lt;/a&gt; tool to build the API backend, and a few dozen lines of modern vanilla JavaScript to build the interface.&lt;/p&gt;
&lt;figure&gt;&lt;img style="max-width: 100%" src="https://media.24ways.org/2018/willison/24ways-autocomplete.gif" alt="Animated demo of autocomplete search against 24 ways" /&gt;&lt;/figure&gt;
&lt;p&gt;&lt;a href="https://media.24ways.org/2018/willison/"&gt;Try it out here&lt;/a&gt;, then read on to see how I built it.&lt;/p&gt;
&lt;h3&gt;First step: crawling the data&lt;/h3&gt;
&lt;p&gt;The first step in building a search engine is to grab a copy of the data that you plan to make searchable.&lt;/p&gt;
&lt;p&gt;There are plenty of potential ways to do this: you might be able to pull it directly from a database, or extract it using an API. If you don’t have access to the raw data, you can imitate Google and write a crawler to extract the data that you need.&lt;/p&gt;
&lt;p&gt;I’m going to do exactly that against 24 ways: I’ll build a simple crawler using &lt;a href="https://en.wikipedia.org/wiki/Wget"&gt;wget&lt;/a&gt;, a command-line tool that features a powerful “recursive” mode that’s ideal for scraping websites.&lt;/p&gt;
&lt;p&gt;We’ll start at the &lt;code&gt;https://24ways.org/archives/&lt;/code&gt; page, which links to an archived index for every year that 24 ways has been running.&lt;/p&gt;
&lt;p&gt;Then we’ll tell &lt;code&gt;wget&lt;/code&gt; to recursively crawl the website, using the &lt;code&gt;--recursive&lt;/code&gt; flag.&lt;/p&gt;
&lt;p&gt;We don’t want to fetch every single page on the site - we’re only interested in the actual articles. Luckily, 24 ways has nicely designed URLs, so we can tell &lt;code&gt;wget&lt;/code&gt; that we only care about pages that start with one of the years it has been running, using the &lt;code&gt;-I&lt;/code&gt; argument like this: &lt;code&gt;-I /2005,/2006,/2007,/2008,/2009,/2010,/2011,/2012,/2013,/2014,/2015,/2016,/2017&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We want to be polite, so let’s wait for 2 seconds between each request rather than hammering the site as fast as we can: &lt;code&gt;--wait 2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The first time I ran this, I accidentally downloaded the comments pages as well. We don’t want those, so let’s exclude them from the crawl using &lt;code&gt;-X "/*/*/comments"&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Finally, it’s useful to be able to run the command multiple times without downloading pages that we have already fetched. We can use the &lt;code&gt;--no-clobber&lt;/code&gt; option for this.&lt;/p&gt;
&lt;p&gt;Tie all of those options together and we get this command:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wget --recursive --wait 2 --no-clobber
  -I /2005,/2006,/2007,/2008,/2009,/2010,/2011,/2012,/2013,/2014,/2015,/2016,/2017
  -X "/*/*/comments"
  https://24ways.org/archives/ &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you leave this running for a few minutes, you’ll end up with a folder structure something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;$ find 24ways.org
24ways.org
24ways.org/2013
24ways.org/2013/why-bother-with-accessibility
24ways.org/2013/why-bother-with-accessibility/index.html
24ways.org/2013/levelling-up
24ways.org/2013/levelling-up/index.html
24ways.org/2013/project-hubs
24ways.org/2013/project-hubs/index.html
24ways.org/2013/credits-and-recognition
24ways.org/2013/credits-and-recognition/index.html
...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As a quick sanity check, let’s count the number of HTML pages we have retrieved:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;$ find 24ways.org | grep index.html | wc -l
328&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s one last step! We got everything up to 2017, but we need to fetch the articles for 2018 (so far) as well. They aren’t linked in the &lt;code&gt;/archives/&lt;/code&gt; yet so we need to point our crawler at the site’s front page instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;wget --recursive --wait 2 --no-clobber
  -I /2018
  -X "/*/*/comments"
  https://24ways.org/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;code&gt;--no-clobber&lt;/code&gt;, this is safe to run every day in December to pick up any new content.&lt;/p&gt;
&lt;p&gt;We now have a folder on our computer containing an HTML file for every article that has ever been published on the site! Let’s use them to build ourselves a search index.&lt;/p&gt;
&lt;h3&gt;Building a search index using SQLite&lt;/h3&gt;
&lt;p&gt;There are many tools out there that can be used to build a search engine. You can use an open-source search server like &lt;a href="https://www.elastic.co/products/elasticsearch"&gt;Elasticsearch&lt;/a&gt; or &lt;a href="http://lucene.apache.org/solr/"&gt;Solr&lt;/a&gt;, a hosted option like &lt;a href="https://www.algolia.com/"&gt;Algolia&lt;/a&gt; or &lt;a href="https://aws.amazon.com/cloudsearch/"&gt;Amazon CloudSearch&lt;/a&gt; or you can tap into the built-in search features of relational databases like MySQL or PostgreSQL.&lt;/p&gt;
&lt;p&gt;I’m going to use something that’s less commonly used for web applications but makes for a powerful and extremely inexpensive alternative: &lt;a href="https://sqlite.org/index.html"&gt;SQLite&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SQLite is the world’s most widely deployed database, even though many people have never even heard of it. That’s because it’s designed to be used as an embedded database: it’s commonly used by native mobile applications and even runs as part of the default set of apps on the Apple Watch!&lt;/p&gt;
&lt;p&gt;SQLite has one major limitation: unlike databases like MySQL and PostgreSQL, it isn’t really designed to handle large numbers of concurrent writes. For this reason, most people avoid it for building web applications.&lt;/p&gt;
&lt;p&gt;This doesn’t matter nearly so much if you are building a search engine for infrequently updated content - say one for &lt;a href="https://24ways.org/"&gt;a site&lt;/a&gt; that only publishes new content on 24 days every year.&lt;/p&gt;
&lt;p&gt;It turns out SQLite has very powerful full-text search functionality built into the core database - the &lt;a href="https://sqlite.org/fts5.html"&gt;FTS5 extension&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’ve been doing &lt;a href="https://simonwillison.net/tags/sqlite/"&gt;a lot of work with SQLite&lt;/a&gt; recently, and as part of that, I’ve been building a Python utility library to make building new SQLite databases as easy as possible, called &lt;a href="https://sqlite-utils.readthedocs.io"&gt;sqlite-utils&lt;/a&gt;. It’s designed to be used within a &lt;a href="https://jupyter.org/"&gt;Jupyter notebook&lt;/a&gt; - an enormously productive way of interacting with Python code that’s similar to the Observable notebooks Natalie &lt;a href="https://24ways.org/2018/observable-notebooks-and-inaturalist/"&gt;described on 24 ways yesterday&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you haven’t used Jupyter before, here’s the fastest way to get up and running with it - assuming you have Python 3 installed on your machine. We can use a Python &lt;a href="https://docs.python.org/3/tutorial/venv.html"&gt;virtual environment&lt;/a&gt; to ensure the software we are installing doesn’t clash with any other installed packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;$ python3 -m venv ./jupyter-venv
$ ./jupyter-venv/bin/pip install jupyter
# ... lots of installer output
# Now lets install some extra packages we will need later
$ ./jupyter-venv/bin/pip install beautifulsoup4 sqlite-utils html5lib
# And start the notebook web application
$ ./jupyter-venv/bin/jupyter-notebook
# This will open your browser to Jupyter at http://localhost:8888/&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You should now be in the Jupyter web application. Click &lt;em&gt;New -&amp;gt; Python 3&lt;/em&gt; to start a new notebook.&lt;/p&gt;
&lt;p&gt;A neat thing about Jupyter notebooks is that if you publish them to GitHub (either in a regular repository or as a Gist), it will render them as HTML. This makes them a very powerful way to share annotated code. I’ve published &lt;a href="https://nbviewer.jupyter.org/github/simonw/24ways-datasette/blob/master/24-ways-search-index.ipynb"&gt;the notebook I used to build the search index&lt;/a&gt; on my GitHub account. &lt;/p&gt;
&lt;figure&gt;&lt;picture&gt;&lt;source srcset='​"https://media.24ways.org/2018/willison/24ways-jupyter.webp"' type='​"image/​webp"'&gt;​
&lt;img style="max-width: 100%" src="https://media.24ways.org/2018/willison/24ways-jupyter.png" alt="Juptyer notebook with my scraping code" /&gt;&lt;/source&gt;&lt;/picture&gt;&lt;/figure&gt;
&lt;p&gt;Here’s the Python code I used to scrape the relevant data from the downloaded HTML files. Check out &lt;a href="https://nbviewer.jupyter.org/github/simonw/24ways-datasette/blob/master/24-ways-search-index.ipynb"&gt;the notebook&lt;/a&gt; for a line-by-line explanation of what’s going on.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from pathlib import Path
from bs4 import BeautifulSoup as Soup
base = Path("/Users/simonw/Dropbox/Development/24ways-search")
articles = list(base.glob("*/*/*/*.html"))
# articles is now a list of paths that look like this:
# PosixPath('...24ways-search/24ways.org/2013/why-bother-with-accessibility/index.html')
docs = []
for path in articles:
    year = str(path.relative_to(base)).split("/")[1]
    url = 'https://' + str(path.relative_to(base).parent) + '/'
    soup = Soup(path.open().read(), "html5lib")
    author = soup.select_one(".c-continue")["title"].split(
        "More information about"
    )[1].strip()
    author_slug = soup.select_one(".c-continue")["href"].split(
        "/authors/"
    )[1].split("/")[0]
    published = soup.select_one(".c-meta time")["datetime"]
    contents = soup.select_one(".e-content").text.strip()
    title = soup.find("title").text.split(" ◆")[0]
    try:
        topic = soup.select_one(
            '.c-meta a[href^="/topics/"]'
        )["href"].split("/topics/")[1].split("/")[0]
    except TypeError:
        topic = None
    docs.append({
        "title": title,
        "contents": contents,
        "year": year,
        "author": author,
        "author_slug": author_slug,
        "published": published,
        "url": url,
        "topic": topic,
    })&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After running this code, I have a list of Python dictionaries representing each of the documents that I want to add to the index. The list looks something like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;[
  {
    "title": "Why Bother with Accessibility?",
    "contents": "Web accessibility (known in other fields as inclus...",
    "year": "2013",
    "author": "Laura Kalbag",
    "author_slug": "laurakalbag",
    "published": "2013-12-10T00:00:00+00:00",
    "url": "https://24ways.org/2013/why-bother-with-accessibility/",
    "topic": "design"
  },
  {
    "title": "Levelling Up",
    "contents": "Hello, 24 ways. Iu2019m Ashley and I sell property ins...",
    "year": "2013",
    "author": "Ashley Baxter",
    "author_slug": "ashleybaxter",
    "published": "2013-12-06T00:00:00+00:00",
    "url": "https://24ways.org/2013/levelling-up/",
    "topic": "business"
  },
  ...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;My &lt;code&gt;sqlite-utils&lt;/code&gt; library has the ability to take a list of objects like this and automatically create a SQLite database table with the right schema to store the data. Here’s how to do that using this list of dictionaries.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import sqlite_utils
db = sqlite_utils.Database("/tmp/24ways.db")
db["articles"].insert_all(docs)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s all there is to it! The library will create a new database and add a table to it called &lt;code&gt;articles&lt;/code&gt; with the necessary columns, then insert all of the documents into that table.&lt;/p&gt;
&lt;p&gt;(I put the database in &lt;code&gt;/tmp/&lt;/code&gt; for the moment - you can move it to a more sensible location later on.)&lt;/p&gt;
&lt;p&gt;You can inspect the table using the &lt;code&gt;sqlite3&lt;/code&gt; command-line utility (which comes with OS X) like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;$ sqlite3 /tmp/24ways.db
sqlite&amp;gt; .headers on
sqlite&amp;gt; .mode column
sqlite&amp;gt; select title, author, year from articles;
title                           author        year
------------------------------  ------------  ----------
Why Bother with Accessibility?  Laura Kalbag  2013
Levelling Up                    Ashley Baxte  2013
Project Hubs: A Home Base for   Brad Frost    2013
Credits and Recognition         Geri Coady    2013
Managing a Mind                 Christopher   2013
Run Ragged                      Mark Boulton  2013
Get Started With GitHub Pages   Anna Debenha  2013
Coding Towards Accessibility    Charlie Perr  2013
...
&amp;lt;Ctrl+D to quit&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s one last step to take in our notebook. We know we want to use SQLite’s full-text search feature, and &lt;code&gt;sqlite-utils&lt;/code&gt; has a simple convenience method for enabling it for a specified set of columns in a table. We want to be able to search by the &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;author&lt;/code&gt; and &lt;code&gt;contents&lt;/code&gt; fields, so we call the &lt;code&gt;enable_fts()&lt;/code&gt; method like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;db["articles"].enable_fts(["title", "author", "contents"])&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Introducing Datasette&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://datasette.readthedocs.io/"&gt;Datasette&lt;/a&gt; is the open-source tool I’ve been building that makes it easy to both explore SQLite databases and publish them to the internet.&lt;/p&gt;
&lt;p&gt;We’ve been exploring our new SQLite database using the &lt;code&gt;sqlite3&lt;/code&gt; command-line tool. Wouldn’t it be nice if we could use a more human-friendly interface for that?&lt;/p&gt;
&lt;p&gt;If you don’t want to install Datasette right now, you can visit &lt;a href="https://search-24ways.herokuapp.com/"&gt;https://search-24ways.herokuapp.com/&lt;/a&gt; to try it out against the 24 ways search index data. I’ll show you how to deploy Datasette to Heroku like this later in the article.&lt;/p&gt;
&lt;p&gt;If you want to install Datasette locally, you can reuse the virtual environment we created to play with Jupyter:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;./jupyter-venv/bin/pip install datasette&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will install Datasette in the &lt;code&gt;./jupyter-venv/bin/&lt;/code&gt; folder. You can also install it system-wide using regular &lt;code&gt;pip install datasette&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now you can run Datasette against the &lt;code&gt;24ways.db&lt;/code&gt; file we created earlier like so:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;./jupyter-venv/bin/datasette /tmp/24ways.db&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will start a local webserver running. Visit &lt;code&gt;http://localhost:8001/&lt;/code&gt; to start interacting with the Datasette web application.&lt;/p&gt;
&lt;p&gt;If you want to try out Datasette without creating your own &lt;code&gt;24ways.db&lt;/code&gt; file you can download the one I created directly from &lt;a href="https://search-24ways.herokuapp.com/24ways-ae60295.db"&gt;https://search-24ways.herokuapp.com/24ways-ae60295.db&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Publishing the database to the internet&lt;/h3&gt;
&lt;p&gt;One of the goals of the Datasette project is to make deploying data-backed APIs to the internet as easy as possible. Datasette has a built-in command for this, &lt;a href="https://datasette.readthedocs.io/en/stable/publish.html"&gt;datasette publish&lt;/a&gt;. If you have an account with &lt;a href="https://www.heroku.com/"&gt;Heroku&lt;/a&gt; or &lt;a href="https://zeit.co/now"&gt;Zeit Now&lt;/a&gt;, you can deploy a database to the internet with a single command. Here’s how I deployed &lt;a href="https://search-24ways.herokuapp.com/"&gt;https://search-24ways.herokuapp.com/&lt;/a&gt; (running on Heroku’s free tier) using &lt;code&gt;datasette publish&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;$ ./jupyter-venv/bin/datasette publish heroku /tmp/24ways.db --name search-24ways
-----&amp;gt; Python app detected
-----&amp;gt; Installing requirements with pip

-----&amp;gt; Running post-compile hook
-----&amp;gt; Discovering process types
       Procfile declares types -&amp;gt; web

-----&amp;gt; Compressing...
       Done: 47.1M
-----&amp;gt; Launching...
       Released v8
       https://search-24ways.herokuapp.com/ deployed to Heroku&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you try this out, you’ll need to pick a different &lt;code&gt;--name&lt;/code&gt;, since I’ve already taken &lt;code&gt;search-24ways&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can run this command as many times as you like to deploy updated versions of the underlying database.&lt;/p&gt;
&lt;h3&gt;Searching and faceting&lt;/h3&gt;
&lt;p&gt;Datasette can detect tables with SQLite full-text search configured, and will add a search box directly to the page. Take a look at &lt;a href="http://search-24ways.herokuapp.com/24ways-b607e21/articles"&gt;http://search-24ways.herokuapp.com/24ways-b607e21/articles&lt;/a&gt; to see this in action.&lt;/p&gt;
&lt;figure&gt;&lt;picture&gt;&lt;source srcset='​"https://media.24ways.org/2018/willison/24ways-facets.webp"' type='​"image/​webp"'&gt;​
&lt;img style="max-width: 100%" src="https://media.24ways.org/2018/willison/24ways-facets.png" alt="Datasette faceted browse" /&gt;&lt;/source&gt;&lt;/picture&gt;&lt;/figure&gt;
&lt;p&gt;SQLite search supports wildcards, so if you want autocomplete-style search where you don’t need to enter full words to start getting results you can add a &lt;code&gt;*&lt;/code&gt; to the end of your search term. Here’s a search for &lt;code&gt;access*&lt;/code&gt; which returns articles on accessibility:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://search-24ways.herokuapp.com/24ways-ae60295/articles?_search=acces%2A"&gt;&lt;code&gt;http://search-24ways.herokuapp.com/24ways-ae60295/articles?_search=acces%2A&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A neat feature of Datasette is the ability to &lt;a href="https://datasette.readthedocs.io/en/stable/facets.html"&gt;calculate facets&lt;/a&gt; against your data. Here’s a page showing search results for &lt;code&gt;svg&lt;/code&gt; with facet counts calculated against both the &lt;code&gt;year&lt;/code&gt; and the &lt;code&gt;topic&lt;/code&gt; columns:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://search-24ways.herokuapp.com/24ways-ae60295/articles?_search=svg&amp;amp;_facet=year&amp;amp;_facet=topic"&gt;&lt;code&gt;http://search-24ways.herokuapp.com/24ways-ae60295/articles?_search=svg&amp;amp;_facet=year&amp;amp;_facet=topic&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Every page visible via Datasette has a corresponding JSON API, which can be accessed using the JSON link on the page - or by adding a &lt;code&gt;.json&lt;/code&gt; extension to the URL:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://search-24ways.herokuapp.com/24ways-ae60295/articles.json?_search=acces%2A"&gt;&lt;code&gt;http://search-24ways.herokuapp.com/24ways-ae60295/articles.json?_search=acces%2A&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Better search using custom SQL&lt;/h3&gt;
&lt;p&gt;The search results we get back from &lt;code&gt;../articles?_search=svg&lt;/code&gt; are OK, but the order they are returned in is not ideal - they’re actually being returned in the order they were inserted into the database! You can see why this is happening by clicking the &lt;a href="http://search-24ways.herokuapp.com/24ways-ae60295?sql=select+rowid%2C+%2A+from+articles+where+rowid+in+%28select+rowid+from+articles_fts+where+articles_fts+match+%3Asearch%29+order+by+rowid+limit+101&amp;amp;search=svg"&gt;View and edit SQL&lt;/a&gt; link on that search results page.&lt;/p&gt;
&lt;p&gt;This exposes the underlying SQL query, which looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;select rowid, * from articles where rowid in (
  select rowid from articles_fts where articles_fts match :search
) order by rowid limit 101&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can do better than this by constructing a custom SQL query. Here’s the query we will use instead:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;select
  snippet(articles_fts, -1, 'b4de2a49c8', '8c94a2ed4b', '...', 100) as snippet,
  articles_fts.rank, articles.title, articles.url, articles.author, articles.year
from articles
  join articles_fts on articles.rowid = articles_fts.rowid
where articles_fts match :search || "*"
  order by rank limit 10;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can &lt;a href="http://search-24ways.herokuapp.com/24ways-ae60295?sql=select%0D%0A++snippet%28articles_fts%2C+-1%2C+%27b4de2a49c8%27%2C+%278c94a2ed4b%27%2C+%27...%27%2C+100%29+as+snippet%2C%0D%0A++articles_fts.rank%2C+articles.title%2C+articles.url%2C+articles.author%2C+articles.year%0D%0Afrom+articles%0D%0A++join+articles_fts+on+articles.rowid+%3D+articles_fts.rowid%0D%0Awhere+articles_fts+match+%3Asearch+%7C%7C+%22*%22%0D%0A++order+by+rank+limit+10%3B&amp;amp;search=svg"&gt;try this query out&lt;/a&gt; directly - since Datasette opens the underling SQLite database in read-only mode and enforces a one second time limit on queries, it’s safe to allow users to provide arbitrary SQL select queries for Datasette to execute.&lt;/p&gt;
&lt;p&gt;There’s a lot going on here! Let’s break the SQL down line-by-line:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;select
  snippet(articles_fts, -1, 'b4de2a49c8', '8c94a2ed4b', '...', 100) as snippet,&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re using &lt;code&gt;snippet()&lt;/code&gt;, a &lt;a href="https://sqlite.org/fts5.html#the_snippet_function"&gt;built-in SQLite function&lt;/a&gt;, to generate a snippet highlighting the words that matched the query. We use two unique strings that I made up to mark the beginning and end of each match - you’ll see why in the JavaScript later on.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;  articles_fts.rank, articles.title, articles.url, articles.author, articles.year&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These are the other fields we need back - most of them are from the &lt;code&gt;articles&lt;/code&gt; table but we retrieve the &lt;code&gt;rank&lt;/code&gt; (representing the strength of the search match) from the magical &lt;code&gt;articles_fts&lt;/code&gt; table.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;from articles
  join articles_fts on articles.rowid = articles_fts.rowid&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;articles&lt;/code&gt; is the table containing our data. &lt;code&gt;articles_fts&lt;/code&gt; is a magic SQLite virtual table which implements full-text search - we need to join against it to be able to query it.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;where articles_fts match :search || "*"
  order by rank limit 10;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;:search || "*"&lt;/code&gt; takes the &lt;code&gt;?search=&lt;/code&gt; argument from the page querystring and adds a &lt;code&gt;*&lt;/code&gt; to the end of it, giving us the wildcard search that we want for autocomplete. We then match that against the &lt;code&gt;articles_fts&lt;/code&gt; table using the &lt;code&gt;match&lt;/code&gt; operator. Finally, we &lt;code&gt;order by rank&lt;/code&gt; so that the best matching results are returned at the top - and limit to the first 10 results.&lt;/p&gt;
&lt;p&gt;How do we turn this into an API? As before, the secret is to add the &lt;code&gt;.json&lt;/code&gt; extension. Datasette actually supports &lt;a href="https://datasette.readthedocs.io/en/stable/json_api.html#different-shapes"&gt;multiple shapes of JSON&lt;/a&gt; - we’re going to use &lt;code&gt;?_shape=array&lt;/code&gt; to get back a plain array of objects:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://search-24ways.herokuapp.com/24ways-ae60295.json?sql=select%0D%0A++snippet(articles_fts%2C+-1%2C+%27b4de2a49c8%27%2C+%278c94a2ed4b%27%2C+%27...%27%2C+100)+as+snippet%2C%0D%0A++articles_fts.rank%2C+articles.title%2C+articles.url%2C+articles.author%2C+articles.year%0D%0Afrom+articles%0D%0A++join+articles_fts+on+articles.rowid+%3D+articles_fts.rowid%0D%0Awhere+articles_fts+match+%3Asearch+||+%22*%22%0D%0A++order+by+rank+limit+10%3B&amp;amp;search=svg&amp;amp;_shape=array"&gt;JSON API call to search for articles matching SVG&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The HTML version of that page shows the time taken to execute the SQL in the footer. Hitting refresh a few times, I get response times between 2 and 5ms - easily fast enough to power a responsive autocomplete feature.&lt;/p&gt;
&lt;h3&gt;A simple JavaScript autocomplete search interface&lt;/h3&gt;
&lt;p&gt;I considered building this using &lt;a href="https://svelte.technology/"&gt;React&lt;/a&gt; or &lt;a href="https://svelte.technology/"&gt;Svelte&lt;/a&gt; or another of the myriad of JavaScript framework options available today, but then I remembered that vanilla JavaScript in 2018 is a very productive environment all on its own.&lt;/p&gt;
&lt;p&gt;We need a few small utility functions: first, a classic debounce function adapted from &lt;a href="https://davidwalsh.name/javascript-debounce-function"&gt;this one by David Walsh&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-js"&gt;function debounce(func, wait, immediate) {
  let timeout;
  return function() {
    let context = this, args = arguments;
    let later = () =&amp;gt; {
      timeout = null;
      if (!immediate) func.apply(context, args);
    };
    let callNow = immediate &amp;amp;&amp;amp; !timeout;
    clearTimeout(timeout);
    timeout = setTimeout(later, wait);
    if (callNow) func.apply(context, args);
  };
};&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll use this to only send &lt;code&gt;fetch()&lt;/code&gt; requests a maximum of once every 100ms while the user is typing.&lt;/p&gt;
&lt;p&gt;Since we’re rendering data that might include HTML tags (24 ways is a site about web development after all), we need an HTML escaping function. I’m amazed that browsers still don’t bundle a default one of these:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-js"&gt;const htmlEscape = (s) =&amp;gt; s.replace(
  /&amp;gt;/g, '&amp;amp;gt;'
).replace(
  /&amp;lt;/g, '&amp;amp;lt;'
).replace(
  /&amp;amp;/g, '&amp;amp;'
).replace(
  /"/g, '&amp;amp;quot;'
).replace(
  /'/g, '&amp;amp;#039;'
);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We need some HTML for the search form, and a &lt;code&gt;div&lt;/code&gt; in which to render the results:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;h1&amp;gt;Autocomplete search&amp;lt;/h1&amp;gt;
&amp;lt;form&amp;gt;
  &amp;lt;p&amp;gt;&amp;lt;input id="searchbox" type="search" placeholder="Search 24ways" style="width: 60%"&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/form&amp;gt;
&amp;lt;div id="results"&amp;gt;&amp;lt;/div&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And now the autocomplete implementation itself, as a glorious, messy stream-of-consciousness of JavaScript:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-js"&gt;// Embed the SQL query in a multi-line backtick string:
const sql = `select
  snippet(articles_fts, -1, 'b4de2a49c8', '8c94a2ed4b', '...', 100) as snippet,
  articles_fts.rank, articles.title, articles.url, articles.author, articles.year
from articles
  join articles_fts on articles.rowid = articles_fts.rowid
where articles_fts match :search || "*"
  order by rank limit 10`;

// Grab a reference to the &amp;lt;input type="search"&amp;gt;
const searchbox = document.getElementById("searchbox");

// Used to avoid race-conditions:
let requestInFlight = null;

searchbox.onkeyup = debounce(() =&amp;gt; {
  const q = searchbox.value;
  // Construct the API URL, using encodeURIComponent() for the parameters
  const url = (
    "https://search-24ways.herokuapp.com/24ways-866073b.json?sql=" +
    encodeURIComponent(sql) +
    `&amp;amp;search=${encodeURIComponent(q)}&amp;amp;_shape=array`
  );
  // Unique object used just for race-condition comparison
  let currentRequest = {};
  requestInFlight = currentRequest;
  fetch(url).then(r =&amp;gt; r.json()).then(d =&amp;gt; {
    if (requestInFlight !== currentRequest) {
      // Avoid race conditions where a slow request returns
      // after a faster one.
      return;
    }
    let results = d.map(r =&amp;gt; `
      &amp;lt;div class="result"&amp;gt;
        &amp;lt;h3&amp;gt;&amp;lt;a href="${r.url}"&amp;gt;${htmlEscape(r.title)}&amp;lt;/a&amp;gt;&amp;lt;/h3&amp;gt;
        &amp;lt;p&amp;gt;&amp;lt;small&amp;gt;${htmlEscape(r.author)} - ${r.year}&amp;lt;/small&amp;gt;&amp;lt;/p&amp;gt;
        &amp;lt;p&amp;gt;${highlight(r.snippet)}&amp;lt;/p&amp;gt;
      &amp;lt;/div&amp;gt;
    `).join("");
    document.getElementById("results").innerHTML = results;
  });
}, 100); // debounce every 100ms&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There’s just one more utility function, used to help construct the HTML results:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-js"&gt;const highlight = (s) =&amp;gt; htmlEscape(s).replace(
  /b4de2a49c8/g, '&amp;lt;b&amp;gt;'
).replace(
  /8c94a2ed4b/g, '&amp;lt;/b&amp;gt;'
);&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is what those unique strings passed to the &lt;code&gt;snippet()&lt;/code&gt; function were for.&lt;/p&gt;
&lt;h3&gt;Avoiding race conditions in autocomplete&lt;/h3&gt;
&lt;p&gt;One trick in this code that you may not have seen before is the way race-conditions are handled. Any time you build an autocomplete feature, you have to consider the following case:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User types &lt;code&gt;acces&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Browser sends &lt;strong&gt;request A&lt;/strong&gt; - querying documents matching &lt;code&gt;acces*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;User continues to type &lt;code&gt;accessibility&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Browser sends &lt;strong&gt;request B&lt;/strong&gt; - querying documents matching &lt;code&gt;accessibility*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Request B&lt;/strong&gt; returns. It was fast, because there are fewer documents matching the full term&lt;/li&gt;
&lt;li&gt;The results interface updates with the documents from &lt;strong&gt;request B&lt;/strong&gt;, matching &lt;code&gt;accessibility*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Request A&lt;/strong&gt; returns results (this was the slower of the two requests)&lt;/li&gt;
&lt;li&gt;The results interface updates with the documents from &lt;strong&gt;request A&lt;/strong&gt; - results matching &lt;code&gt;access*&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a terrible user experience: the user saw their desired results for a brief second, and then had them snatched away and replaced with those results from earlier on.&lt;/p&gt;
&lt;p&gt;Thankfully there’s an easy way to avoid this. I set up a variable in the outer scope called &lt;code&gt;requestInFlight&lt;/code&gt;, initially set to &lt;code&gt;null&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Any time I start a new &lt;code&gt;fetch()&lt;/code&gt; request, I create a new &lt;code&gt;currentRequest = {}&lt;/code&gt; object and assign it to the outer &lt;code&gt;requestInFlight&lt;/code&gt; as well.&lt;/p&gt;
&lt;p&gt;When the &lt;code&gt;fetch()&lt;/code&gt; completes, I use &lt;code&gt;requestInFlight !== currentRequest&lt;/code&gt; to sanity check that the &lt;code&gt;currentRequest&lt;/code&gt; object is strictly identical to the one that was in flight. If a new request has been triggered since we started the current request we can detect that and avoid updating the results.&lt;/p&gt;
&lt;h3&gt;It’s not a lot of code, really&lt;/h3&gt;
&lt;p&gt;And that’s the whole thing! The code is pretty ugly, but when the entire implementation clocks in at fewer than 70 lines of JavaScript, I honestly don’t think it matters. You’re welcome to refactor it as much you like.&lt;/p&gt;
&lt;p&gt;How good is this search implementation? I’ve been building search engines for a long time using a wide variety of technologies and I’m happy to report that using SQLite in this way is genuinely a really solid option. It scales happily up to hundreds of MBs (or even GBs) of data, and the fact that it’s based on SQL makes it easy and flexible to work with.&lt;/p&gt;
&lt;p&gt;A surprisingly large number of desktop and mobile applications you use every day implement their search feature on top of SQLite.&lt;/p&gt;
&lt;p&gt;More importantly though, I hope that this demonstrates that using Datasette for an API means you can build relatively sophisticated API-backed applications with very little backend programming effort. If you’re working with a small-to-medium amount of data that changes infrequently, you may not need a more expensive database. Datasette-powered applications easily fit within the free tier of both Heroku and Zeit Now.&lt;/p&gt;
&lt;p&gt;For more of my writing on Datasette, check out &lt;a href="https://simonwillison.net/tags/datasette/"&gt;the datasette tag on my blog&lt;/a&gt;. And if you do build something fun with it, please &lt;a href="https://twitter.coml/simonw"&gt;let me know on Twitter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;This article &lt;a href="https://24ways.org/2018/fast-autocomplete-search-for-your-website/"&gt;originally appeared on 24ways&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;</summary><category term="24ways"></category><category term="autocomplete"></category><category term="javascript"></category><category term="datasette"></category></entry><entry><title>Zeit 2.0, and building smaller Python Docker images</title><link href="http://simonwillison.net/2018/Nov/19/smaller-python-docker-images/#atom-entries" rel="alternate"></link><published>2018-11-19T03:13:40+00:00</published><updated>2018-11-19T03:13:40+00:00</updated><id>http://simonwillison.net/2018/Nov/19/smaller-python-docker-images/#atom-entries</id><summary type="html">&lt;p&gt;Changes are afoot at &lt;a href="https://zeit.co/now"&gt;Zeit Now&lt;/a&gt;, my preferred hosting provider for the past year (see &lt;a href="https://simonwillison.net/tags/zeitnow/"&gt;previous posts&lt;/a&gt;). They have &lt;a href="https://zeit.co/blog/now-2"&gt;announced Now 2.0&lt;/a&gt;, an intriguing new approach to providing auto-scaling immutable deployments. It’s built on top of lambdas, and comes with a whole host of new constraints: code needs to fit into a 5MB bundle for example (though it looks like this restriction will soon be &lt;a href="https://spectrum.chat/?t=0ab38384-5aa3-4b04-899a-5b056f9b83b9"&gt;relaxed a little&lt;/a&gt; - &lt;strong&gt;update November 19th&lt;/strong&gt; you can now &lt;a href="https://zeit.co/blog/customizable-lambda-sizes"&gt;bump this up to 50MB&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Unfortunately, they have also announced their intent to deprecate the existing Now v1 Docker-based solution.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“We will only start thinking about deprecation plans once we are able to accommodate the most common and critical use cases of v1 on v2” - &lt;a href="https://spectrum.chat/thread/96985341-e17f-4af4-a330-c726774ed436?m=MTU0MTcwOTU1ODIwNA=="&gt;Matheus Fernandes&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;“When we reach feature parity, we still intend to give customers plenty of time to upgrade (we are thinking at the very least 6 months from the time we announce it)” - &lt;a href="https://spectrum.chat/thread/46d54a53-f58d-4e8f-bce2-047a6ac93305?m=MTU0MjUyMDMwMzc5NQ=="&gt;Guillermo Rauch&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is pretty disastrous news for many of my projects, most crucially &lt;a href="https://github.com/simonw/datasette"&gt;Datasette&lt;/a&gt; and &lt;a href="https://simonwillison.net/2018/Jan/17/datasette-publish/"&gt;Datasette Publish&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Datasette should be fine - it supports Heroku as an alternative to Zeit Now &lt;a href="https://datasette.readthedocs.io/en/stable/publish.html"&gt;out of the box&lt;/a&gt;, and the &lt;a href="https://datasette.readthedocs.io/en/stable/plugins.html#publish-subcommand-publish"&gt;publish_subcommand plugin hook&lt;/a&gt; makes it easy to add further providers (I’m exploring several new options at the moment).&lt;/p&gt;
&lt;p&gt;Datasette Publish is a bigger problem. The whole point of that project is to make it easy for less-technical users to deploy their data as an interactive API to a Zeit Now account that they own themselves. Talking these users through what they need to do to upgrade should v1 be shut down in the future is not an exciting prospect.&lt;/p&gt;
&lt;p&gt;So I’m going to start hunting for an alternative backend for Datasette Publish, but in the meantime I’ve had to make some changes to how it works in order to handle a new size limit of 100MB for Docker images deployed by free users.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Building_smaller_Docker_images_18"&gt;&lt;/a&gt;Building smaller Docker images&lt;/h3&gt;
&lt;p&gt;Zeit &lt;a href="https://twitter.com/ppival/status/1063464380057055232"&gt;appear to have introduced a new limit&lt;/a&gt; for free users of their Now v1 platform: Docker images need to be no larger than 100MB.&lt;/p&gt;
&lt;p&gt;Datasette Publish was creating final image sizes of around 350MB, blowing way past that limit. I spent some time today figuring out how to get it to produce images within the new limit, and learned a lot about Docker image optimization in the process.&lt;/p&gt;
&lt;p&gt;I ended up using Docker’s &lt;a href="https://docs.docker.com/develop/develop-images/multistage-build/"&gt;multi-stage build feature&lt;/a&gt;, which allows you to create temporary images during a build, use them to  compile dependencies, then copy just the compiled assets into the final image.&lt;/p&gt;
&lt;p&gt;An example of the previous Datasette Publish generated Dockerfile &lt;a href="https://gist.github.com/simonw/365294fb51765fb07bc99fe5eb7fee22"&gt;can be seen here&lt;/a&gt;. Here’s a rough outline of what it does:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with the &lt;code&gt;python:3.6-slim-stretch&lt;/code&gt; image&lt;/li&gt;
&lt;li&gt;apt-installs &lt;code&gt;python3-dev&lt;/code&gt; and &lt;code&gt;gcc&lt;/code&gt; so it can compile Python libraries with binary dependencies (pandas and uvloop for example)&lt;/li&gt;
&lt;li&gt;Use &lt;code&gt;pip&lt;/code&gt; to install &lt;code&gt;csvs-to-sqlite&lt;/code&gt; and &lt;code&gt;datasette&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Add the uploaded CSV files, then run &lt;code&gt;csvs-to-sqlite&lt;/code&gt; to convert them into a SQLite database&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;datasette inspect&lt;/code&gt; to cache a JSON file with information about the different tables&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;datasette serve&lt;/code&gt; to serve the resulting web application&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There’s a lot of scope for improvement here. The final image has all sorts of cruft that’s not actually needed for serving the image: it has &lt;code&gt;csvs-to-sqlite&lt;/code&gt; and all of its dependencies, plus the original uploaded CSV files.&lt;/p&gt;
&lt;p&gt;Here’s the workflow I used to build a Dockerfile and check the size of the resulting image. My work-in-progress can be found in the &lt;a href="https://github.com/simonw/datasette-small"&gt;datasette-small repo&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Build the Dockerfile in the current directory and tag as datasette-small
$ docker build . -t datasette-small
# Inspect the size of the resulting image
$ docker images | grep datasette-small
# Start the container running
$ docker run -d -p 8006:8006 datasette-small
654d3fc4d3343c6b73414c6fb4b2933afc56fbba1f282dde9f515ac6cdbc5339
# Now visit http://localhost:8006/ to see it running
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="Alpine_Linux_48"&gt;&lt;/a&gt;Alpine Linux&lt;/h3&gt;
&lt;p&gt;When you start looking for ways to build smaller Dockerfiles, the first thing you will encounter is &lt;a href="https://en.wikipedia.org/wiki/Alpine_Linux"&gt;Alpine Linux&lt;/a&gt;. Alpine is a Linux distribution that’s perfect for containers: it builds on top of &lt;a href="https://en.wikipedia.org/wiki/BusyBoxAlpine_Linux"&gt;BusyBox&lt;/a&gt; to strip down to the smallest possible image that can still do useful things.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;python:3.6-alpine&lt;/code&gt; container should be perfect: it gives you the smallest possible container that can run Python 3.6 applications (including the ability to &lt;code&gt;pip install&lt;/code&gt; additional dependencies).&lt;/p&gt;
&lt;p&gt;There’s just one problem: in order to install C-based dependencies like &lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt; (used by csvs-to-sqlite) and &lt;a href="https://github.com/huge-success/sanic"&gt;Sanic&lt;/a&gt; (used by Datasette) you need a compiler toolchain. Alpine doesn’t have this out-of-the-box, but you can install one using Alpine’s &lt;code&gt;apk&lt;/code&gt; package manager. Of course, now you’re bloating your container with a bunch of compilation tools that you don’t need to serve the final image.&lt;/p&gt;
&lt;p&gt;This is what makes multi-stage builds so useful! We can spin up an Alpine image with the compilers installed, build our modules, then copy the resulting binary blobs into a fresh container.&lt;/p&gt;
&lt;p&gt;Here’s the basic recipe for doing that:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;FROM python:3.6-alpine as builder

# Install and compile Datasette + its dependencies
RUN apk add --no-cache gcc python3-dev musl-dev alpine-sdk
RUN pip install datasette

# Now build a fresh container, copying across the compiled pieces
FROM python:3.6-alpine

COPY --from=builder /usr/local/lib/python3.6 /usr/local/lib/python3.6
COPY --from=builder /usr/local/bin/datasette /usr/local/bin/datasette
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This pattern works really well, and produces delightfully slim images. My first attempt at this wasn’t quite slim enough to fit the 100MB limit though, so I had to break out some Docker tools to figure out exactly what was going on.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Inspecting_docker_image_layers_74"&gt;&lt;/a&gt;Inspecting docker image layers&lt;/h3&gt;
&lt;p&gt;Part of the magic of Docker is the concept of &lt;a href="https://medium.com/@jessgreb01/digging-into-docker-layers-c22f948ed612"&gt;layers&lt;/a&gt;. When Docker builds a container it uses a layered filesystem (&lt;a href="https://en.wikipedia.org/wiki/UnionFS"&gt;UnionFS&lt;/a&gt;) and creates a new layer for every executable line in the Dockerfile. This dramatically speeds up future builds (since layers can be reused if they have already been built) and also provides a powerful tool for inspecting different stages of the build.&lt;/p&gt;
&lt;p&gt;When you run &lt;code&gt;docker build&lt;/code&gt; part of the output is IDs of the different image layers as they are constructed:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;datasette-small $ docker build . -t datasette-small
Sending build context to Docker daemon  2.023MB
Step 1/21 : FROM python:3.6-slim-stretch as csvbuilder
 ---&amp;gt; 971a5d5dad01
Step 2/21 : RUN apt-get update &amp;amp;&amp;amp; apt-get install -y python3-dev gcc wget
 ---&amp;gt; Running in f81485df62dd
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given a layer ID, like &lt;code&gt;971a5d5dad01&lt;/code&gt;, it’s possible to spin up a new container that exposes the exact state of that layer (&lt;a href="https://stackoverflow.com/a/26222636/6083"&gt;thanks, Stack Overflow&lt;/a&gt;). Here’s how do to that:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -it --rm 971a5d5dad01 sh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;-it&lt;/code&gt; argument attaches standard input to the container (&lt;code&gt;-i&lt;/code&gt;) and allocates a pseudo-TTY (&lt;code&gt;-t&lt;/code&gt;). The &lt;code&gt;-rm&lt;/code&gt; option means that the container will be removed when you Ctrl+D back out of it. &lt;code&gt;sh&lt;/code&gt; is the command we want to run in the container - using a shell lets us start interacting with it.&lt;/p&gt;
&lt;p&gt;Now that we have a shell against that layer, we can use regular unix commands to start exploring it. &lt;code&gt;du -m&lt;/code&gt; (&lt;code&gt;m&lt;/code&gt; for &lt;code&gt;MB&lt;/code&gt;) is particularly useful here, as it will show us the largest directories in the filesystem. I pipe it through &lt;code&gt;sort&lt;/code&gt; like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker run -it --rm abc63755616b sh
# du -m | sort -n
...
58  ./usr/local/lib/python3.6
70  ./usr/local/lib
71  ./usr/local
76  ./usr/lib/python3.5
188 ./usr/lib
306 ./usr
350 .
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Straight away we can start seeing where the space is being taken up in our image.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Deleting_unnecessary_files_108"&gt;&lt;/a&gt;Deleting unnecessary files&lt;/h3&gt;
&lt;p&gt;I spent quite a while inspecting different stages of my builds to try and figure out where the space was going. The alpine copy recipe worked neatly, but I was still a little over the limit. When I started to dig around in my final image I spotted some interesting patterns - in particular, the &lt;code&gt;/usr/local/lib/python3.6/site-packages/uvloop&lt;/code&gt; directory was 17MB!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# du -m /usr/local | sort -n -r | head -n 5
96  /usr/local
95  /usr/local/lib
83  /usr/local/lib/python3.6
36  /usr/local/lib/python3.6/site-packages
17  /usr/local/lib/python3.6/site-packages/uvloop
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That seems like a lot of disk space for a compiled C module, so I dug in further…&lt;/p&gt;
&lt;p&gt;It turned out the &lt;code&gt;uvloop&lt;/code&gt; folder still contained a bunch of files that were used as part of the compilation, including a 6.7MB &lt;code&gt;loop.c&lt;/code&gt; file and a bunch of &lt;code&gt;.pxd&lt;/code&gt; and &lt;code&gt;.pyd&lt;/code&gt; files that are compiled by &lt;a href="https://cython.org/"&gt;Cython&lt;/a&gt;. None of these files are needed after the extension has been compiled, but they were there, taking up a bunch of precious space.&lt;/p&gt;
&lt;p&gt;So I added the following to my Dockerfile:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN find /usr/local/lib/python3.6 -name '*.c' -delete
RUN find /usr/local/lib/python3.6 -name '*.pxd' -delete
RUN find /usr/local/lib/python3.6 -name '*.pyd' -delete
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I noticed that there were &lt;code&gt;__pycache__&lt;/code&gt; files that weren’t needed either, so I added this as well:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;RUN find /usr/local/lib/python3.6 -name '__pycache__' | xargs rm -r
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(The &lt;code&gt;-delete&lt;/code&gt; flag didn’t work correctly for that one, so I used &lt;code&gt;xargs&lt;/code&gt; instead.)&lt;/p&gt;
&lt;p&gt;This shaved off around 15MB, putting me safely under the limit.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Running_csvstosqlite_in_its_own_stage_137"&gt;&lt;/a&gt;Running csvs-to-sqlite in its own stage&lt;/h3&gt;
&lt;p&gt;The above tricks had got me the smallest Alpine Linux image I could create that would still run Datasette… but Datasette Publish also needs to run &lt;code&gt;csvs-to-sqlite&lt;/code&gt; in order to convert the user’s uploaded CSV files to SQLite.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;csvs-to-sqlite&lt;/code&gt; has some pretty heavy dependencies of its own in the form of &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; and &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt;. Even with the build chain installed I was having trouble installing these under Alpine, especially since building numpy for Alpine is &lt;a href="https://stackoverflow.com/questions/49037742/why-does-it-take-ages-to-install-pandas-on-alpine-linux"&gt;notoriously slow&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then I realized that thanks to multi-stage builds there’s no need for me to use Alpine at all for this step. I switched back to &lt;code&gt;python:3.6-slim-stretch&lt;/code&gt; and used it to install &lt;code&gt;csvs-to-sqlite&lt;/code&gt; and compile the CSV files into a SQLite database. I also ran &lt;code&gt;datasette inspect&lt;/code&gt; there for good measure.&lt;/p&gt;
&lt;p&gt;Then in my final Alpine container I could use the following to copy in just those compiled assets:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;COPY --from=csvbuilder inspect-data.json inspect-data.json
COPY --from=csvbuilder data.db data.db
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="Tying_it_all_together_150"&gt;&lt;/a&gt;Tying it all together&lt;/h3&gt;
&lt;p&gt;Here’s an example of &lt;a href="https://gist.github.com/simonw/ee63bc5e7feb6e8bb3af82f67a7a36fe"&gt;a full Dockerfile generated by Datasette Publish&lt;/a&gt; that combines all of these tricks. To summarize, here’s what it does:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spin up a &lt;code&gt;python:3.6-slim-stretch&lt;/code&gt; - call it &lt;code&gt;csvbuilder&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;apt-get install -y python3-dev gcc&lt;/code&gt; so we can install compiled dependencies&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pip install csvs-to-sqlite datasette&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy in the uploaded CSV files&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;csvs-to-sqlite&lt;/code&gt; to convert them into a SQLite database&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;datasette inspect data.db&lt;/code&gt; to generate an &lt;code&gt;inspect-data.json&lt;/code&gt; file with statistics about the tables. This can later be used to reduce startup time for &lt;code&gt;datasette serve&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spin up a &lt;code&gt;python:3.6-alpine&lt;/code&gt; - call it &lt;code&gt;buildit&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;We need a build chain to compile a copy of datasette for Alpine Linux…&lt;/li&gt;
&lt;li&gt;&lt;code&gt;apk add --no-cache gcc python3-dev musl-dev alpine-sdk&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Now we can &lt;code&gt;pip install datasette&lt;/code&gt;, plus any requested plugins&lt;/li&gt;
&lt;li&gt;Reduce the final image size by deleting any &lt;code&gt;__pycache__&lt;/code&gt; or &lt;code&gt;*.c&lt;/code&gt;, &lt;code&gt;*.pyd&lt;/code&gt; and &lt;code&gt;*.pxd&lt;/code&gt; files.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Spin up a fresh &lt;code&gt;python:3.6-alpine&lt;/code&gt; for our final image
&lt;ul&gt;
&lt;li&gt;Copy in &lt;code&gt;data.db&lt;/code&gt; and &lt;code&gt;inspect-data.json&lt;/code&gt; from &lt;code&gt;csvbuilder&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Copy across &lt;code&gt;/usr/local/lib/python3.6&lt;/code&gt; and &lt;code&gt;/usr/local/bin/datasette&lt;/code&gt; from &lt;code&gt;bulidit&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;… and we’re done! Expose port 8006 and set &lt;code&gt;datasette serve&lt;/code&gt; to run when the container is started&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now that I’ve finally learned how to take advantage of multi-stage builds I expect I’ll be using them for all sorts of interesting things in the future.&lt;/p&gt;</summary><category term="projects"></category><category term="zeitnow"></category><category term="docker"></category><category term="datasette"></category></entry><entry><title>Automatically playing science communication games with transfer learning and fastai</title><link href="http://simonwillison.net/2018/Oct/29/transfer-learning/#atom-entries" rel="alternate"></link><published>2018-10-29T03:16:33+00:00</published><updated>2018-10-29T03:16:33+00:00</updated><id>http://simonwillison.net/2018/Oct/29/transfer-learning/#atom-entries</id><summary type="html">&lt;p&gt;This weekend was the 9th annual &lt;a href="https://sf.sciencehackday.org/"&gt;Science Hack Day San Francisco&lt;/a&gt;, which was also the 100th Science Hack Day held worldwide.&lt;/p&gt;
&lt;p&gt;Natalie and I decided to combine our interests and build something fun.&lt;/p&gt;
&lt;p&gt;I’m currently enrolled in Jeremy Howard’s &lt;a href="http://course.fast.ai/"&gt;Deep Learning course&lt;/a&gt; so I figured this was a great opportunity to try out some computer vision.&lt;/p&gt;
&lt;p&gt;Natalie runs the &lt;a href="https://natbat.github.io/scicomm-calendar/"&gt;SciComm Games calendar&lt;/a&gt; and accompanying &lt;a href="https://twitter.com/SciCommGames"&gt;@SciCommGames&lt;/a&gt; bot to promote and catalogue science communication hashtag games on Twitter.&lt;/p&gt;
&lt;p&gt;Hashtag games? Natalie &lt;a href="https://natbat.github.io/scicomm-calendar/"&gt;explains them here&lt;/a&gt; - essentially they are games run by scientists on Twitter to foster public engagement around an animal or topic by challenging people to identify if a photo is a #cougarOrNot or participate in a #TrickyBirdID or identify #CrowOrNo or many others.&lt;/p&gt;
&lt;p&gt;Combining the two… we decided to build a bot that automatically plays these games using computer vision. So far it’s just trying #cougarOrNot - you can see the bot in action at &lt;a href="https://twitter.com/critter_vision/with_replies"&gt;@critter_vision&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Training_data_from_iNaturalist_14"&gt;&lt;/a&gt;Training data from iNaturalist&lt;/h3&gt;
&lt;p&gt;In order to build a machine learning model, you need to start out with some training data.&lt;/p&gt;
&lt;p&gt;I’m a big fan of &lt;a href="https://www.inaturalist.org/"&gt;iNaturalist&lt;/a&gt;, a citizen science project that encourages users to upload photographs of wildlife (and plants) they have seen and have their observations verified by a community. Natalie and I used it to build &lt;a href="https://www.owlsnearme.com/"&gt;owlsnearme.com&lt;/a&gt; earlier this year - the API in particular is fantastic.&lt;/p&gt;
&lt;p&gt;iNaturalist has &lt;a href="https://www.inaturalist.org/observations?place_id=1&amp;amp;taxon_id=41944"&gt;over 5,000 verified sightings&lt;/a&gt; of felines (cougars, bobcats, domestic cats and more) in the USA.&lt;/p&gt;
&lt;p&gt;The raw data is available as &lt;a href="http://api.inaturalist.org/v1/observations?identified=true&amp;amp;photos=true&amp;amp;identifications=most_agree&amp;amp;quality_grade=research&amp;amp;order=desc&amp;amp;order_by=created_at&amp;amp;taxon_id=41944&amp;amp;place_id=1&amp;amp;per_page=200"&gt;a paginated JSON API&lt;/a&gt;. The &lt;a href="https://static.inaturalist.org/photos/27333309/medium.jpg"&gt;medium sized photos&lt;/a&gt; are just the right size for training a neural network.&lt;/p&gt;
&lt;p&gt;I started by grabbing 5,000 images and saving them to disk with a filename that reflected their identified species:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Bobcat_9005106.jpg
Domestic-Cat_10068710.jpg
Bobcat_15713672.jpg
Domestic-Cat_6755280.jpg
Mountain-Lion_9075705.jpg
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="Building_a_model_32"&gt;&lt;/a&gt;Building a model&lt;/h3&gt;
&lt;p&gt;I’m only one week into the &lt;a href="http://www.fast.ai/"&gt;fast.ai&lt;/a&gt; course so this really isn’t particularly sophisticated yet, but it was just about good enough to power our hack.&lt;/p&gt;
&lt;p&gt;The main technique we are learning in the course is called &lt;a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/"&gt;transfer learning&lt;/a&gt;, and it really is shockingly effective. Instead of training a model from scratch you start out with a pre-trained model and use some extra labelled images to train a small number of extra layers.&lt;/p&gt;
&lt;p&gt;The initial model we are using is &lt;a href="https://www.kaggle.com/pytorch/resnet34"&gt;ResNet-34&lt;/a&gt;, a 34-layer neural network trained on 1,000 labelled categories in the &lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt; corpus.&lt;/p&gt;
&lt;p&gt;In class, we learned to use this technique to get 94% accuracy against the &lt;a href="http://www.robots.ox.ac.uk/~vgg/data/pets/"&gt;Oxford-IIIT Pet Dataset&lt;/a&gt; - around 7,000 images covering 12 cat breeds and 25 dog breeds. In 2012 the researchers at Oxford were able to get 59.21% using a sophisticated model - it 2018 we can get 94% with transfer learning and just a few lines of code.&lt;/p&gt;
&lt;p&gt;I started with an example provided in class, which loads and trains images from files on disk using a regular expression that extracts the labels from the filenames.&lt;/p&gt;
&lt;p&gt;My full Jupyter notebook is &lt;a href="https://github.com/simonw/cougar-or-not/blob/master/inaturalist-cats.ipynb"&gt;inaturalist-cats.ipynb&lt;/a&gt; - the key training code is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;from fastai import *
from fastai.vision import *
cat_images_path = Path('/home/jupyter/.fastai/data/inaturalist-usa-cats/images')
cat_fnames = get_image_files(cat_images_path)
cat_data = ImageDataBunch.from_name_re(
    cat_images_path,
    cat_fnames,
    r'/([^/]+)_\d+.jpg$',
    ds_tfms=get_transforms(),
    size=224
)
cat_data.normalize(imagenet_stats)
cat_learn = ConvLearner(cat_data, models.resnet34, metrics=error_rate)
cat_learn.fit_one_cycle(4)
# Save the generated model to disk
cat_learn.save(&amp;quot;usa-inaturalist-cats&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Calling &lt;code&gt;cat_learn.save(&amp;quot;usa-inaturalist-cats&amp;quot;)&lt;/code&gt; created an 84MB file on disk at &lt;code&gt;/home/jupyter/.fastai/data/inaturalist-usa-cats/images/models/usa-inaturalist-cats.pth&lt;/code&gt; - I used &lt;code&gt;scp&lt;/code&gt; to copy that model down to my laptop.&lt;/p&gt;
&lt;p&gt;This model gave me a 24% error rate which is pretty terrible - others on the course have been getting error rates less than 10% for all kinds of interesting problems. My focus was to get a model deployed as an API though so I haven’t spent any additional time fine-tuning things yet.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Deploying_the_model_as_an_API_67"&gt;&lt;/a&gt;Deploying the model as an API&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://github.com/fastai/fastai"&gt;fastai library&lt;/a&gt; strongly encourages training against a GPU, using &lt;a href="https://pytorch.org/"&gt;pytorch&lt;/a&gt; and &lt;a href="https://mathema.tician.de/software/pycuda/"&gt;PyCUDA&lt;/a&gt;. I’ve been using n1-highmem-8 Google Cloud Platform instance with an attached Tesla P4, then running everything in a Jupyter notebook there. This costs around $0.38 an hour - fine for a few hours of training, but way too expensive to permanently host a model.&lt;/p&gt;
&lt;p&gt;Thankfully, while a GPU is essential for productively training models it’s not nearly as important for evaluating them against new data. pytorch can run in CPU mode for that just fine on standard hardware, and the &lt;a href="https://github.com/fastai/fastai/blob/master/README.md"&gt;fastai README&lt;/a&gt; includes instructions on installing it for a CPU using pip.&lt;/p&gt;
&lt;p&gt;I started out by ensuring I could execute my generated model on my own laptop (since pytorch doesn’t yet work with the GPU built into the Macbook Pro). Once I had that working, I used the resulting code to write a tiny Starlette-powered API server. The code for that can be found in &lt;a href="https://github.com/simonw/cougar-or-not/blob/8adafac571aad3385317c76bd229448b3cdaa0ac/cougar.py"&gt;in cougar.py&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;fastai is under very heavy development and the latest version doesn’t quite have a clean way of loading a model from disk without also including the initial training images, so I had to hack around quite a bit to get this working using clues from &lt;a href="https://forums.fast.ai/"&gt;the fastai forums&lt;/a&gt;. I expect this to get much easier over the next few weeks as the library continues to evolve based on feedback from the current course.&lt;/p&gt;
&lt;p&gt;To deploy the API I wrote &lt;a href="https://github.com/simonw/cougar-or-not/blob/8adafac571aad3385317c76bd229448b3cdaa0ac/Dockerfile"&gt;a Dockerfile&lt;/a&gt; and shipped it to &lt;a href="https://zeit.co/now"&gt;Zeit Now&lt;/a&gt;. Now remains my go-to choice for this kind of project, though unfortunately their new (and brilliant) v2 platform imposes &lt;a href="https://github.com/zeit/now-cli/issues/1523"&gt;a 100MB image size limit&lt;/a&gt; - not nearly enough when the model file itself weights in at 83 MB. Thankfully it’s still possible to &lt;a href="https://github.com/simonw/cougar-or-not/commit/5ad3d5b49c6419e4c2440291bc5fb204625aae83"&gt;specify their v1 cloud&lt;/a&gt; which is more forgiving for larger applications.&lt;/p&gt;
&lt;p&gt;Here’s the result: an API which can accept either the URL to an image or an uploaded image file: &lt;a href="https://cougar-or-not.now.sh/"&gt;https://cougar-or-not.now.sh/&lt;/a&gt; - try it out with &lt;a href="https://cougar-or-not.now.sh/classify-url?url=https://upload.wikimedia.org/wikipedia/commons/9/9a/Oregon_Cougar_ODFW.JPG"&gt;a cougar&lt;/a&gt; and &lt;a href="https://cougar-or-not.now.sh/classify-url?url=https://upload.wikimedia.org/wikipedia/commons/thumb/d/dc/Bobcat2.jpg/1200px-Bobcat2.jpg"&gt;a bobcat&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="The_Twitter_Bot_81"&gt;&lt;/a&gt;The Twitter Bot&lt;/h3&gt;
&lt;p&gt;Natalie built &lt;a href="https://github.com/natbat/CritterVision"&gt;the Twitter bot&lt;/a&gt;. It runs as a scheduled task on Heroku and works by checking for new #cougarOrNot tweets from &lt;a href="https://twitter.com/drmichellelarue"&gt;Dr. Michelle LaRue&lt;/a&gt;, extracting any images, passing them to my API and replying with a tweet that summarizes the results. Take a look at &lt;a href="https://twitter.com/critter_vision/with_replies"&gt;its recent replies&lt;/a&gt; to get a feel for how well it is doing.&lt;/p&gt;
&lt;p&gt;Amusingly, Dr. LaRue frequently tweets memes to promote upcoming competitions and marks them with the same hashtag. The bot appears to think that most of the memes are bobcats! I should definitely spend some time tuning that model.&lt;/p&gt;
&lt;p&gt;Science Hack Day was great fun. A big thanks to the organizing team, and congrats to all of the other participants. I’m really looking forward to the next one.&lt;/p&gt;
&lt;p&gt;Plus… we won a medal!&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;Enjoyed &lt;a href="https://twitter.com/hashtag/scienceHackday?src=hash&amp;amp;ref_src=twsrc%5Etfw"&gt;#scienceHackday&lt;/a&gt; this weekend, made &amp;amp; launched a cool machine learning hack to process images &amp;amp; work out if they have a cougar in them or not! &lt;a href="https://twitter.com/hashtag/CougarOrNot?src=hash&amp;amp;ref_src=twsrc%5Etfw"&gt;#CougarOrNot&lt;/a&gt; &lt;a href="https://twitter.com/critter_vision?ref_src=twsrc%5Etfw"&gt;@critter_vision&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;... we won a medal!&lt;br /&gt;&lt;br /&gt;Bot code: &lt;a href="https://t.co/W2jZcGCnFr"&gt;https://t.co/W2jZcGCnFr&lt;/a&gt;&lt;br /&gt;Machine learning API: &lt;a href="https://t.co/swNiKlcTp0"&gt;https://t.co/swNiKlcTp0&lt;/a&gt; &lt;a href="https://t.co/dcdIhNZy63"&gt;pic.twitter.com/dcdIhNZy63&lt;/a&gt;&lt;/p&gt;&amp;#8212; Natbat (@Natbat) &lt;a href="https://twitter.com/Natbat/status/1056717060116369410?ref_src=twsrc%5Etfw"&gt;October 29, 2018&lt;/a&gt;&lt;/blockquote&gt;</summary><category term="computervision"></category><category term="machinelearning"></category><category term="fastai"></category><category term="transferlearning"></category></entry><entry><title>How to Instantly Publish Data to the Internet with Datasette</title><link href="http://simonwillison.net/2018/Oct/25/how-instantly-publish-data-internet-datasette/#atom-entries" rel="alternate"></link><published>2018-10-25T17:18:43+00:00</published><updated>2018-10-25T17:18:43+00:00</updated><id>http://simonwillison.net/2018/Oct/25/how-instantly-publish-data-internet-datasette/#atom-entries</id><summary type="html">&lt;p&gt;I spoke about my &lt;a href="https://github.com/simonw/datasette"&gt;Datasette&lt;/a&gt; project at &lt;a href="https://pybay.com/"&gt;PyBay&lt;/a&gt; in August and they've just posted &lt;a href="https://www.youtube.com/watch?v=lmP75mp3-Rg"&gt;the video&lt;/a&gt; of my talk.&lt;/p&gt;

&lt;div class="videoWrapper" style="margin-bottom: 1em"&gt;
    &lt;iframe width="560" height="315" src="https://www.youtube.com/embed/lmP75mp3-Rg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen="allowfullscreen"&gt;&amp;#160;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;I've also published the &lt;a href="https://static.simonwillison.net/static/2018/pybay-datasette/"&gt;annotated slides&lt;/a&gt; from the talk, using a similar format to &lt;a href="https://simonwillison.net/2010/Apr/25/redis/"&gt;my Redis tutorial&lt;/a&gt; from back in 2010.&lt;/p&gt;</summary><category term="datajournalism"></category><category term="talks"></category><category term="datasette"></category></entry><entry><title>How I moderated the State of Django panel at DjangoCon US.</title><link href="http://simonwillison.net/2018/Oct/22/moderating-the-state-of-django/#atom-entries" rel="alternate"></link><published>2018-10-22T19:48:53+00:00</published><updated>2018-10-22T19:48:53+00:00</updated><id>http://simonwillison.net/2018/Oct/22/moderating-the-state-of-django/#atom-entries</id><summary type="html">&lt;p&gt;On Wednesday last week I moderated the &lt;a href="https://2018.djangocon.us/talk/state-of-django-panel/"&gt;State of Django panel&lt;/a&gt; as the closing session for &lt;a href="https://2018.djangocon.us/"&gt;DjangoCon US 2018&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I think it went well, so I’m writing some notes on exactly how we did it. In my experience it’s worth doing this for things like public speaking: in six months time I might moderate another panel and I’ll be desperately trying to remember what went right last time.&lt;/p&gt;
&lt;p&gt;Panels are hard. Bad panels are way too common, to the point that some good conferences actively avoid having panels at all.&lt;/p&gt;
&lt;p&gt;In my opinion, a good panel has a number of important attributes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The panel needs a coherent theme. It shouldn’t just be several independent speakers that happen to be sharing the same time slot.&lt;/li&gt;
&lt;li&gt;Panels need to be balanced. Having just one or two of the speakers monopolize the conversation is bad for the audience and bad for the panelists themselves.&lt;/li&gt;
&lt;li&gt;The moderator is there to facilitate the conversation, not to be the center of attention. I love public speaking so I feel the need to be particularly cautious here.&lt;/li&gt;
&lt;li&gt;Panelists need to have diverse perspectives on the topics under discussion. A panel where everyone agrees with each other and makes the same points is a very boring panel indeed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I originally pitched the panel to the DjangoCon US organizing committee as a “State of Django” conversation where core maintainers would talk about the current state of the project.&lt;/p&gt;
&lt;p&gt;They countered with a much better idea: a panel that encompassed both the state of the Django framework and the community and ecosystem that it exists within. Since DjangoCon is primarily about bringing that community together this was a much better fit for the conference, and would make for a much more interesting and relevant discussion.&lt;/p&gt;
&lt;p&gt;I worked together with the conference organizers to find our panelists. Nicholle James in particular was the driving force behind assembling the panelists and ensuring everything was in place for the panel to succeed.&lt;/p&gt;
&lt;p&gt;We ended up with a panel representing a comprehensive cross-section of the organizations that make the Django community work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Andrew Godwin, representing Django Core&lt;/li&gt;
&lt;li&gt;Anna Makarudze, representing the &lt;a href="https://www.djangoproject.com/foundation/" title="Django Software Foundation"&gt;DSF&lt;/a&gt;, &lt;a href="https://djangogirls.org/"&gt;Django Girls&lt;/a&gt; and &lt;a href="https://africa.python.org/en/"&gt;Python Africa&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Frank Wiles, President of the &lt;a href="https://www.djangoproject.com/foundation/" title="Django Software Foundation"&gt;DSF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jeff Triplett, President of &lt;a href="https://www.defna.org/" title="Django Events Foundation North America"&gt;DEFNA&lt;/a&gt;, member of the Board of Directors for the &lt;a href="https://www.python.org/psf/" title="Python Software Foundation"&gt;PSF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Josue Balandrano Coronel, representing &lt;a href="https://www.defna.org/" title="Django Events Foundation North America"&gt;DEFNA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Katherine Michel, representing &lt;a href="https://www.defna.org/" title="Django Events Foundation North America"&gt;DEFNA&lt;/a&gt; and DjangoCon US Website Chair&lt;/li&gt;
&lt;li&gt;Kojo Idrissa, &lt;a href="https://www.defna.org/" title="Django Events Foundation North America"&gt;DEFNA&lt;/a&gt; North American Ambassador&lt;/li&gt;
&lt;li&gt;Rachell Calhoun, representing &lt;a href="https://djangogirls.org/"&gt;Django Girls&lt;/a&gt; and &lt;a href="https://www.pyladies.com/"&gt;PyLadies&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As it was the closing session for the conference, I wanted the panel to both celebrate the progress of the community and project and to explore what we need to do next: what should we be working on to improve Django and it’s community in the future?&lt;/p&gt;
&lt;p&gt;I had some initial thoughts on topics, but since the panel was scheduled for the last session of the conference I decided to spend the conference itself firming up the topics that would be discussed. This was a really good call: we got to create an agenda for the panel that was almost entirely informed by the other conference sessions combined with hot topics from the halfway track. We also asked conference attendees for their suggestions via an online form, and used those suggestions to further inform the topics that were discussed.&lt;/p&gt;
&lt;p&gt;I made sure to have a 10-15 minute conversation one-on-one with each of the panelists during the conference. We then got together for an hour at lunch before the panel to sync up with the topics and themes we would be discussing.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Topic_and_themes_38"&gt;&lt;/a&gt;Topic and themes&lt;/h3&gt;
&lt;p&gt;Our pre-panel conversations highlighted a powerful theme for the panel itself, which I ended up summarizing as “What can the Django project learn from the Django community?” This formed the framework for the other themes of the panel.&lt;/p&gt;
&lt;p&gt;The themes themselves were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Diversity and education - inspired by the work of &lt;a href="https://djangogirls.org/"&gt;Django Girls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Events and international focus, lead by &lt;a href="https://www.defna.org/" title="Django Events Foundation North America"&gt;DEFNA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Django governance: In particular James Bennett’s &lt;a href="https://github.com/django/deps/blob/89712df93daeea9fdf7a993342e7087164620eea/draft/XXXX-dissolve-core.rst"&gt;proposal to split up core&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Big feature ideas for Django (guided by Andrew Godwin’s proposed &lt;a href="https://www.aeracode.org/2018/06/04/django-async-roadmap/"&gt;Django async roadmap&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Money: who has it, and who needs it - understanding the role of &lt;a href="https://www.djangoproject.com/foundation/"&gt;the DSF&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the hardest parts for me was figuring out the order in which we would tackle these themes. I ended up settling on the above order about half an hour before the panel started.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Opening_and_closing_52"&gt;&lt;/a&gt;Opening and closing&lt;/h3&gt;
&lt;p&gt;With eight panelists, ensuring that introductions didn’t drag on too long was particularly important. I asked each panelist to introduce themselves with a couple of sentences that highlighted the organizations they were affiliated with that were relevant to the panel. For our chosen group of panelists this was definitely the right framing.&lt;/p&gt;
&lt;p&gt;I then asked each panelist to be prepared to close the panel with a call to action: something an audience member could actively do that would support the community going forward. This worked really well: it provided a great, actionable note to end both the panel and the conference.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Preparing_and_running_the_panel_58"&gt;&lt;/a&gt;Preparing and running the panel&lt;/h3&gt;
&lt;p&gt;We used our panel lunch together to check that no one would have calls to action that overlapped too much, and to provide a rough indication of who had things to say about each topic we planned to discuss.&lt;/p&gt;
&lt;p&gt;This turned out to be essential: I’ve been on smaller panels where the panelists have been able to riff easily on each other’s points, but with eight panelists it turned out not everyone could even see each other, so as panel moderator it fell on me to direct questions to individuals and then prompt others for follow-ups. Thankfully the panel lunch combined with the one-to-one conversations gave me the information I needed for this.&lt;/p&gt;
&lt;p&gt;I had written down a selection of questions for each of the themes. Having a selection turned out to be crucial: a few times the panelists talked about material that I had planned to cover in a later section, so I had to adapt as we went along. In the future I’ll spend more time on this: these written ideas were a crucial component in keeping the panel flowing in the right direction.&lt;/p&gt;
&lt;p&gt;With everything in place the panel itself was a case of concentrating on what everyone was saying and using the selection of the next questions (plus careful ad-libbing) to guide the conversation along the preselected themes. I also tried to keep mental track of who had done the most speaking so I could ensure the conversation stayed as balanced as possible by inviting other panelists into the conversation.&lt;/p&gt;
&lt;p&gt;The video of the panel should be out in around three weeks time, at which point you can evaluate for yourself if we managed to do a good job of it. I’m really happy with the feedback we got after the panel, and I plan to use a similar process for panels I’m involved with in the future.&lt;/p&gt;</summary><category term="django"></category><category term="djangocon"></category><category term="panels"></category><category term="speaking"></category></entry><entry><title>The interesting ideas in Datasette</title><link href="http://simonwillison.net/2018/Oct/4/datasette-ideas/#atom-entries" rel="alternate"></link><published>2018-10-04T02:28:45+00:00</published><updated>2018-10-04T02:28:45+00:00</updated><id>http://simonwillison.net/2018/Oct/4/datasette-ideas/#atom-entries</id><summary type="html">&lt;p&gt;&lt;a href="https://github.com/simonw/datasette"&gt;Datasette&lt;/a&gt; (&lt;a href="https://simonwillison.net/tags/datasette/"&gt;previously&lt;/a&gt;) is my open source tool for exploring and publishing structured data. There are a lot of ideas embedded in Datasette. I realized that I haven’t put many of them into writing.&lt;/p&gt;
&lt;p&gt;
&lt;a href="#Publishing_readonly_data"&gt;Publishing read-only data&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Bundling_the_data_with_the_code"&gt;Bundling the data with the code&lt;/a&gt;&lt;br /&gt;
&lt;a href="#SQLite_as_the_underlying_data_engine"&gt;SQLite as the underlying data engine&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Farfuture_cache_expiration"&gt;Far-future cache expiration&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Publishing_as_a_core_feature"&gt;Publishing as a core feature&lt;/a&gt;&lt;br /&gt;
&lt;a href="#License_and_source_metadata"&gt;License and source metadata&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Facet_everything"&gt;Facet everything&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Respect_for_CSV"&gt;Respect for CSV&lt;/a&gt;&lt;br /&gt;
&lt;a href="#SQL_as_an_API_language"&gt;SQL as an API language&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Optimistic_query_execution_with_time_limits"&gt;Optimistic query execution with time limits&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Keyset_pagination"&gt;Keyset pagination&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Interactive_demos_based_on_the_unit_tests"&gt;Interactive demos based on the unit tests&lt;/a&gt;&lt;br /&gt;
&lt;a href="#Documentation_unit_tests"&gt;Documentation unit tests&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="Publishing_readonly_data"&gt;&lt;/a&gt;Publishing read-only data&lt;/h3&gt;
&lt;p&gt;Datasette provides a read-only API to your data. It makes no attempt to deal with writes. Avoiding writes entirely is fundamental to a plethora of interesting properties, many of which are expanded on further below. In brief:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hosting web applications with no read/write persistence requirements is incredibly cheap in 2018 - often free (both &lt;a href="https://zeit.co/now"&gt;ZEIT Now&lt;/a&gt; and a &lt;a href="https://www.heroku.com/"&gt;Heroku&lt;/a&gt; have generous free tiers). This is a big deal: even having to pay a few dollars a month is enough to dicentivise sharing data, since now you have to figure out who will pay and ensure the payments don’t expire in the future.&lt;/li&gt;
&lt;li&gt;Being read-only makes it trivial to scale: just add more instances, each with their own copy of the data. All of the hard problems in scaling web applications that relate to writable data stores can be skipped entirely.&lt;/li&gt;
&lt;li&gt;Since the database file is opened using SQLite’s &lt;a href="https://www.sqlite.org/uri.html#uriimmutable"&gt;immutable mode&lt;/a&gt;, we can accept arbitrary SQL queries with no risk of them corrupting the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Any time your data changes, you need to publish a brand new copy of the whole database. With the right hosting this is easy: deploy a brand new copy of your data and application in parallel to your existing live deployment, then switch over incoming HTTP traffic to your API at the load balancer level. Heroku and Zeit Now both support this strategy out of the box.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Bundling_the_data_with_the_code"&gt;&lt;/a&gt;Bundling the data with the code&lt;/h3&gt;
&lt;p&gt;Since the data is read-only and is encapsulated in a single binary SQLite database file, we can bundle the data as part of the app. This means we can trivially create and publish Docker images that provide both the data and the API and UI for accessing it. We can also publish to any hosting provider that will allow us to run a Python application, without also needing to provision a mutable database.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://datasette.readthedocs.io/en/stable/publish.html#datasette-package"&gt;datasette package&lt;/a&gt; command takes one or more SQLite databases and bundles them together with the Datasette application in a single Docker image, ready to be deployed anywhere that can run Docker containers.&lt;/p&gt;
&lt;h3&gt;&lt;a id="SQLite_as_the_underlying_data_engine"&gt;&lt;/a&gt;SQLite as the underlying data engine&lt;/h3&gt;
&lt;p&gt;Datasette encourages people to use &lt;a href="https://www.sqlite.org/"&gt;SQLite&lt;/a&gt; as a standard format for publishing data.&lt;/p&gt;
&lt;p&gt;Relational database are great: once you know how to use them, you can represent any data you can imagine using a carefully designed schema.&lt;/p&gt;
&lt;p&gt;What about data that’s too unstructured to fit a relational schema? SQLite includes excellent &lt;a href="https://www.sqlite.org/json1.html"&gt;support for JSON data&lt;/a&gt; - so if you can’t shape your data to fit a table schema you can instead store it as text blobs of JSON - and use SQLite’s JSON functions to filter by or extract specific fields.&lt;/p&gt;
&lt;p&gt;What about binary data? Even that’s covered: SQLite will happily store binary blobs. My &lt;a href="https://github.com/simonw/datasette-render-images"&gt;datasette-render-images plugin&lt;/a&gt; (&lt;a href="https://datasette-render-images-demo.datasette.io/favicons-6a27915/favicons"&gt;live demo here&lt;/a&gt;) is one example of a tool that works with binary image data stored in SQLite blobs.&lt;/p&gt;
&lt;p&gt;What if my data is too big? Datasette is not a “big data” tool, but if your definition of big data is something that won’t fit in RAM  that threshold is growing all the time (2TB of RAM on a single AWS instance &lt;a href="https://aws.amazon.com/about-aws/whats-new/2016/05/now-available-x1-instances-the-largest-amazon-ec2-memory-optimized-instance-with-2-tb-of-memory/"&gt;now costs less than $4/hour&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I’ve personally had great results from multiple GB SQLite databases and Datasette. The theoretical maximum size of a single SQLite database is &lt;a href="https://www.sqlite.org/limits.html"&gt;around 140TB&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SQLite also has built-in support for &lt;a href="https://www.sqlite.org/fts5.html"&gt;surprisingly good full-text search&lt;/a&gt;, and thanks to being extensible via modules has excellent geospatial functionality in the form of the &lt;a href="https://www.gaia-gis.it/fossil/libspatialite/index"&gt;SpatiaLite extension&lt;/a&gt;. Datasette benefits enormously from this wider ecosystem.&lt;/p&gt;
&lt;p&gt;The reason most developers avoid SQLite for production web applications is that it doesn’t deal brilliantly with large volumes of concurrent writes. Since Datasette is read-only we can entirely ignore this limitation.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Farfuture_cache_expiration"&gt;&lt;/a&gt;Far-future cache expiration&lt;/h3&gt;
&lt;p&gt;Since the data in a Datasette instance never changes, why not cache calls to it forever?&lt;/p&gt;
&lt;p&gt;Datasette sends a far future HTTP cache expiry header with every API response. This means that browsers will only ever fetch data the first time a specific URL is accessed, and if you host Datasette behind a CDN such as &lt;a href="https://www.fastly.com/"&gt;Fastly&lt;/a&gt; or &lt;a href="https://www.cloudflare.com/"&gt;Cloudflare&lt;/a&gt; each unique API call will hit Datasette just once and then be cached essentially forever by the CDN.&lt;/p&gt;
&lt;p&gt;This means it’s safe to deploy a JavaScript app using an inexpensively hosted Datasette-backed API to the front page of even a high traffic site - the CDN will easily take the load.&lt;/p&gt;
&lt;p&gt;Zeit added Cloudflare to every deployment (even their free tier) &lt;a href="https://zeit.co/blog/now-cdn"&gt;back in July&lt;/a&gt;, so if you are hosted there you get this CDN benefit for free.&lt;/p&gt;
&lt;p&gt;What if you re-publish an updated copy of your data? Datasette has that covered too. You may have noticed that every Datasette database gets a hashed suffix automatically when it is deployed:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://fivethirtyeight.datasettes.com/fivethirtyeight-c9e67c4"&gt;https://fivethirtyeight.datasettes.com/fivethirtyeight-c9e67c4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This suffix is based on the SHA256 hash of the entire database file contents - so any change to the data will result in new URLs. If you query a previous suffix Datasette will notice and redirect you to the new one.&lt;/p&gt;
&lt;p&gt;If you know you’ll be changing your data, you can build your application against the non-suffixed URL. This will not be cached and will always 302 redirect to the correct version (and these redirects are extremely fast).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://fivethirtyeight.datasettes.com/fivethirtyeight/alcohol-consumption%2Fdrinks.json"&gt;https://fivethirtyeight.datasettes.com/fivethirtyeight/alcohol-consumption%2Fdrinks.json&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The redirect sends an HTTP/2 push header such that if you are running behind a CDN that understands push (&lt;a href="https://blog.cloudflare.com/announcing-support-for-http-2-server-push-2/"&gt;such as Cloudflare&lt;/a&gt;) your browser won’t have to make two requests to follow the redirect. You can use the Chrome DevTools to see this in action:&lt;/p&gt;
&lt;p&gt;&lt;img  style="max-width: 100%" src="https://static.simonwillison.net/static/2018/http2-push.png" alt="Chrome DevTools showing a redirect initiated by an HTTP/2 push" /&gt;&lt;/p&gt;
&lt;p&gt;And finally, if you need to opt out of HTTP caching for some reason you can disable it on a per-request basis by including &lt;code&gt;?_ttl=0&lt;/code&gt; &lt;a href="https://datasette.readthedocs.io/en/stable/json_api.html#special-json-arguments"&gt;in the URL query string&lt;/a&gt;.  - for example, if you want to return a random member of the Avengers it doesn’t make sense to cache the response:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://fivethirtyeight.datasettes.com/fivethirtyeight?sql=select+*+from+%5Bavengers%2Favengers%5D+order+by+random()+limit+1&amp;amp;_ttl=0"&gt;https://fivethirtyeight.datasettes.com/fivethirtyeight?sql=select+*+from+[avengers%2Favengers]+order+by+random()+limit+1&amp;amp;_ttl=0&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="Publishing_as_a_core_feature"&gt;&lt;/a&gt;Publishing as a core feature&lt;/h3&gt;
&lt;p&gt;Datasette aims to reduce the friction for publishing interesting data online as much as possible.&lt;/p&gt;
&lt;p&gt;To this end, Datasette includes &lt;a href="https://datasette.readthedocs.io/en/stable/publish.html"&gt;a “publish” subcommand&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# deploy to Heroku
datasette publish heroku mydatabase.db
# Or deploy to Zeit Now
datasette publish now mydatabase.db
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These commands take one or more SQLite databases, upload them to a hosting provider, configure a Datasette instance to serve them and return the public URL of the newly deployed application.&lt;/p&gt;
&lt;p&gt;Out of the box, Datasette can publish to either Heroku or to Zeit Now. The &lt;a href="https://datasette.readthedocs.io/en/stable/plugins.html#publish-subcommand-publish"&gt;publish_subcommand plugin hook&lt;/a&gt; means other providers can be supported by writing plugins.&lt;/p&gt;
&lt;h3&gt;&lt;a id="License_and_source_metadata"&gt;&lt;/a&gt;License and source metadata&lt;/h3&gt;
&lt;p&gt;Datasette believes that data should be accompanied by source information and a license, whenever possible. The &lt;a href="https://datasette.readthedocs.io/en/stable/metadata.html"&gt;metadata.json file&lt;/a&gt; that can be bundled with your data supports these. You can also provide source and license information when you run &lt;code&gt;datasette publish&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;datasette publish fivethirtyeight.db \
    --source=&amp;quot;FiveThirtyEight&amp;quot; \
    --source_url=&amp;quot;https://github.com/fivethirtyeight/data&amp;quot; \
    --license=&amp;quot;CC BY 4.0&amp;quot; \
    --license_url=&amp;quot;https://creativecommons.org/licenses/by/4.0/&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you use these options Datasette will create the corresponding &lt;code&gt;metadata.json&lt;/code&gt; file for you as part of the deployment.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Facet_everything"&gt;&lt;/a&gt;Facet everything&lt;/h3&gt;
&lt;p&gt;I really love faceted search: it’s the first tool I turn to whenever I want to start understanding a collection of data. I’ve built faceted search engines on top of Solr, Elasticsearch and PostgreSQL and many of my favourite tools (like Splunk and Datadog) have it as a core feature.&lt;/p&gt;
&lt;p&gt;Datasette automatically attempts to calculate facets against every table. You can read &lt;a href="https://simonwillison.net/2018/May/20/datasette-facets/"&gt;more about the Datasette Facets feature here&lt;/a&gt; - as a huge faceted search fan it’s one of my all-time favourite features of the project. Now I can add SQLite to the list of technologies I’ve used to build faceted search!&lt;/p&gt;
&lt;h3&gt;&lt;a id="Respect_for_CSV"&gt;&lt;/a&gt;Respect for CSV&lt;/h3&gt;
&lt;p&gt;CSV is by far the most common format for sharing and publishing data online. Almost every useful data tool has the ability to export to it, and it remains the lingua franca of spreadsheet import and export.&lt;/p&gt;
&lt;p&gt;It has many flaws: it can’t easily represent nested data structures, escaping rules for values containing commas are inconsistently implemented and it doesn’t have a standard way of representing character encoding.&lt;/p&gt;
&lt;p&gt;Datasette aims to promote SQLite as a much better default format for publishing data. I would much rather download a .db file full of pre-structured data than download a .csv and then have to re-structure it as a separate piece of work.&lt;/p&gt;
&lt;p&gt;But interacting well with the enormous CSV ecosystem is essential. Datasette has &lt;a href="https://datasette.readthedocs.io/en/stable/csv_export.html"&gt;deep CSV export functionality&lt;/a&gt;: any data you can see, you can export - including the results of arbitrary SQL queries. If your query can be paginated Datasette can stream down every page in a single CSV file for you.&lt;/p&gt;
&lt;p&gt;Datasette’s sister-tool &lt;a href="https://github.com/simonw/csvs-to-sqlite"&gt;csvs-to-sqlite&lt;/a&gt; handles the other side of the equation: importing data from CSV into SQLite tables. And the &lt;a href="https://simonwillison.net/2018/Jan/17/datasette-publish/"&gt;Datasette Publish web application&lt;/a&gt; allows users to upload their CSVs and have them deployed directly to their own fresh Datasette instance - no command line required.&lt;/p&gt;
&lt;h3&gt;&lt;a id="SQL_as_an_API_language"&gt;&lt;/a&gt;SQL as an API language&lt;/h3&gt;
&lt;p&gt;A lot of people these days are excited about &lt;a href="https://graphql.org/"&gt;GraphQL&lt;/a&gt;, because it allows API clients to request exactly the data they need, including traversing into related objects in a single query.&lt;/p&gt;
&lt;p&gt;Guess what? SQL has been able to do that since the 1970s!&lt;/p&gt;
&lt;p&gt;There are a number of reasons most APIs don’t allow people to pass them arbitrary SQL queries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Security: we don’t want people messing up our data&lt;/li&gt;
&lt;li&gt;Performance: what if someone sends an accidental (or deliberate) expensive query that exhausts our resources?&lt;/li&gt;
&lt;li&gt;Hiding implementation details: if people write SQL against our API we can never change the structure of our database tables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Datasette has answers to all three.&lt;/p&gt;
&lt;p&gt;On security: the data is read-only, using SQLite’s immutable mode. You can’t damage it with a query - INSERT and UPDATEs will simply throw harmless errors.&lt;/p&gt;
&lt;p&gt;On performance: SQLite has a mechanism for canceling queries that take longer than a certain threshold. Datasette sets this to one second by default, though you can &lt;a href="https://datasette.readthedocs.io/en/stable/config.html#sql-time-limit-ms"&gt;alter that configuration&lt;/a&gt; if you need to (I often bump it up to ten seconds when exploring multi-GB data on my laptop).&lt;/p&gt;
&lt;p&gt;On hidden implementation details: since we are publishing static data rather than maintaining an evolving API, we can mostly ignore this issue. If you are really worried about it you can take advantage of &lt;a href="https://datasette.readthedocs.io/en/stable/sql_queries.html#canned-queries"&gt;canned queries&lt;/a&gt; and &lt;a href="https://datasette.readthedocs.io/en/stable/sql_queries.html#views"&gt;SQL view definitions&lt;/a&gt; to expose a carefully selected forward-compatible view into your data.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Optimistic_query_execution_with_time_limits"&gt;&lt;/a&gt;Optimistic query execution with time limits&lt;/h3&gt;
&lt;p&gt;I mentioned Datasette’s SQL time limits above. These aren’t just there to avoid malicious queries: the idea of “optimistic SQL evaluation” is baked into some of Datasette’s core features.&lt;/p&gt;
&lt;p&gt;Consider &lt;a href="https://datasette.readthedocs.io/en/stable/facets.html#suggested-facets"&gt;suggested facets&lt;/a&gt; - where Datasette inspects any table you view and tries to suggest columns that are worth faceting against.&lt;/p&gt;
&lt;p&gt;The way this works is Datasette loops over &lt;em&gt;every&lt;/em&gt; column in the table and runs a query to see if there are less than 20 unique values for that column. On a large table this could take a prohibitive amount of time, so Datasette sets an aggressive timeout on those queries: &lt;a href="https://datasette.readthedocs.io/en/stable/config.html#facet-suggest-time-limit-ms"&gt;just 50ms&lt;/a&gt;. If the query fails to run in that time it is silently dropped and the column is not listed as a suggested facet.&lt;/p&gt;
&lt;p&gt;Datasette’s JSON API provides a mechanism for JavaScript applications to use that same pattern. If you add &lt;code&gt;?_timelimit=20&lt;/code&gt; to any Datasette API call, the underlying query will only get 20ms to run. If it goes over you’ll get a very fast error response from the API. This means you can design your own features that attempt to optimistically run expensive queries without damaging the  performance of your app.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Keyset_pagination"&gt;&lt;/a&gt;Keyset pagination&lt;/h3&gt;
&lt;p&gt;SQL pagination using OFFSET/LIMIT has a fatal flaw: if you request page number 300 at 20 per page the underlying SQL engine needs to calculate and sort all 6,000 preceding rows before it can return the 20 you have requested.&lt;/p&gt;
&lt;p&gt;This does not scale at all well.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://use-the-index-luke.com/sql/partial-results/fetch-next-page"&gt;Keyset pagination&lt;/a&gt; (often known by other names, including cursor-based pagination) is a far more efficient way to paginate through data. It works against ordered data. Each page is returned with a token representing the last record you saw, then when you request the next page the engine merely has to filter for records that are greater than that tokenized value and scan through the next 20 of them.&lt;/p&gt;
&lt;p&gt;(Actually, it scans through 21. By requesting one more record than you intend to display you can detect if another page of results exists - if you ask for 21 but get back 20 or less you know you are on the last page.)&lt;/p&gt;
&lt;p&gt;Datasette’s table view includes a sophisticated implementation of keyset pagination.&lt;/p&gt;
&lt;p&gt;Datasette defaults to sorting by primary key (or SQLite rowid). This is perfect for efficient pagination: running a select against the primary key column for values greater than X is one of the fastest range scan queries any database can support. This allows users to paginate as deep as they like without paying the offset/limit performance penalty.&lt;/p&gt;
&lt;p&gt;This is also how the “export all rows as CSV” option works: when you select that option, Datasette opens a stream to your browser and internally starts keyset-pagination over the entire table. This keeps resource usage in check even while streaming back millions of rows.&lt;/p&gt;
&lt;p&gt;Here’s where Datasette gets fancy: it handles keyset pagination for any other sort order as well. If you sort by any column and click “next” you’ll be requesting the next set of rows after the last value you saw. And this even works for columns containing duplicate values: If you sort by such a column, Datasette actually sorts by that column combined with the primary key. The “next” pagination token it generates encodes both the sorted value and the primary key, allowing it to correctly serve you the next page when you click the link.&lt;/p&gt;
&lt;p&gt;Try clicking “next” &lt;a href="https://latest.datasette.io/fixtures-dd88475/sortable?_sort_desc=sortable"&gt;on this page&lt;/a&gt; to see keyset pagination against a sorted column in action.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Interactive_demos_based_on_the_unit_tests"&gt;&lt;/a&gt;Interactive demos based on the unit tests&lt;/h3&gt;
&lt;p&gt;I love interactive demos. I decided it would be useful if every single release of Datasette had a permanent interactive demo illustrating its features.&lt;/p&gt;
&lt;p&gt;Thanks to Zeit Now, this was pretty easy to set up. I’ve actually taken it a step further: every successful push to master on GitHub is also deployed to a permanent URL.&lt;/p&gt;
&lt;p&gt;Some examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://latest.datasette.io/"&gt;https://latest.datasette.io/&lt;/a&gt; - the most recent commit to Datasette master. You can see the currently deployed commit hash on &lt;a href="https://latest.datasette.io/-/versions"&gt;https://latest.datasette.io/-/versions&lt;/a&gt; and compare it to &lt;a href="https://github.com/simonw/datasette/commits"&gt;https://github.com/simonw/datasette/commits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://v0-25.datasette.io/"&gt;https://v0-25.datasette.io/&lt;/a&gt; is a permanent URL to the 0.25 tagged release of Datasette. See also &lt;a href="https://v0-24.datasette.io/"&gt;https://v0-24.datasette.io/&lt;/a&gt; and &lt;a href="https://v0-23-2.datasette.io/"&gt;https://v0-23-2.datasette.io/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://700d83d.datasette.io/-/versions"&gt;https://700d83d.datasette.io/-/versions&lt;/a&gt; is a permanent URL to the code from this commit: &lt;a href="https://github.com/simonw/datasette/commit/700d83d"&gt;https://github.com/simonw/datasette/commit/700d83d&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The database that is used for this demo is the exact same database that is created by Datasette’s &lt;a href="https://github.com/simonw/datasette/blob/master/tests/fixtures.py"&gt;unit test fixtures&lt;/a&gt;. The unit tests are already designed to exercise every feature, so reusing them for a live demo makes a lot of sense.&lt;/p&gt;
&lt;p&gt;You can view this test database on your own machine by checking out the full Datasette repository from GitHub and running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python tests/fixtures.py fixtures.db metadata.json
datasette fixtures.db -m metadata.json
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s &lt;a href="https://github.com/simonw/datasette/blob/96af802352e49e35751e295e9846aa39c5e22311/.travis.yml#L23-L42"&gt;the code in the Datasette Travis CI configuration&lt;/a&gt; that deploys a live demo for every commit and every released tag.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Documentation_unit_tests"&gt;&lt;/a&gt;Documentation unit tests&lt;/h3&gt;
&lt;p&gt;I wrote about the &lt;a href="https://simonwillison.net/2018/Jul/28/documentation-unit-tests/"&gt;Documentation unit tests&lt;/a&gt; pattern back in July.&lt;/p&gt;
&lt;p&gt;Datasette’s unit tests &lt;a href="https://github.com/simonw/datasette/blob/master/tests/test_docs.py"&gt;include some assertions&lt;/a&gt; that ensure that every plugin hook, configuration setting and underlying view class is mentioned in the documentation. A commit or pull request that adds or modifies these without also updating the documentation (or at least ensuring there is a corresponding heading in the docs) will fail its tests.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Learning_more"&gt;&lt;/a&gt;Learning more&lt;/h3&gt;
&lt;p&gt;Datasette’s &lt;a href="http://datasette.readthedocs.io/"&gt;documentation&lt;/a&gt; is in pretty good shape now, and &lt;a href="https://datasette.readthedocs.io/en/stable/changelog.html"&gt;the changelog&lt;/a&gt; provides a detailed overview of new features that I’ve added to the project. I presented Datasette at the PyBay conference in August and I’ve published &lt;a href="https://static.simonwillison.net/static/2018/pybay-datasette/"&gt;my annotated slides&lt;/a&gt; from that talk. I was &lt;a href="https://changelog.com/podcast/296#t=00:54:45"&gt;interviewed about Datasette&lt;/a&gt; for the Changelog podcast in May and &lt;a href="https://simonwillison.net/2018/May/9/changelog/"&gt;my notes from that conversation&lt;/a&gt; include some of my favourite demos.&lt;/p&gt;
&lt;p&gt;Datasette now has an official Twitter account - you can follow &lt;a href="https://twitter.com/datasetteproj"&gt;@datasetteproj&lt;/a&gt; there for updates about the project.&lt;/p&gt;</summary><category term="projects"></category><category term="sqlite"></category><category term="datasette"></category></entry><entry><title>Letterboxing on Lundy</title><link href="http://simonwillison.net/2018/Sep/18/letterboxing-lundy/#atom-entries" rel="alternate"></link><published>2018-09-18T17:09:21+00:00</published><updated>2018-09-18T17:09:21+00:00</updated><id>http://simonwillison.net/2018/Sep/18/letterboxing-lundy/#atom-entries</id><summary type="html">&lt;p&gt;Last week Natalie and I spent a delightful two days with our friends Hannah and Adam on the beautiful island of &lt;a href="https://en.wikipedia.org/wiki/Lundy"&gt;Lundy&lt;/a&gt; in the Bristol Channel, 12 miles off the coast of North Devon.&lt;/p&gt;
&lt;p&gt;I’ve been wanting to visit Lundy for years. The island is managed by the &lt;a href="https://www.landmarktrust.org.uk/"&gt;Landmark Trust&lt;/a&gt;, a UK charity who look after historic buildings and make them available as holiday rentals.&lt;/p&gt;
&lt;p&gt;Our first experience with the Landmark Trust was the original /dev/fort &lt;a href="https://devfort.com/cohort/1/the-first-dev-fort"&gt;back in 2008&lt;/a&gt; when we rented &lt;a href="https://www.landmarktrust.org.uk/search-and-book/properties/fort-clonque-7423"&gt;a Napoleonic Sea Fortress&lt;/a&gt; on Alderney in the Channel Islands. Ever since then I’ve been keeping an eye out for opportunities to try out more of their properties: just two weeks ago we stayed in &lt;a href="https://www.landmarktrust.org.uk/search-and-book/properties/wortham-manor-13309"&gt;Wortham Manor&lt;/a&gt; and used it as a staging ground to help prepare a family wedding.&lt;/p&gt;

&lt;div class="gallery"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_6897.jpg?w=200&amp;amp;auto=compress" alt="Wortham Manor" title="Wortham Manor" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_6777.jpg?w=200&amp;amp;auto=compress" alt="Dining room" title="The dining room" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_6782.jpg?w=200&amp;amp;auto=compress" alt="A group in the kitchen" title="The kitchen" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_6950.jpg?w=200&amp;amp;auto=compress" alt="Wedding preparations outside the manor" title="Wedding preparations outside the manor" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_6982.jpg?w=200&amp;amp;auto=compress" alt="Open doors" title="Doors" /&gt;
&lt;/div&gt;

&lt;p&gt;I cannot recommend the Landmark Trust experience strongly enough: each property is unique and fascinating, they are kept in great condition and if you split the cost of a larger rental among a group of friends the price can be comparable to a youth hostel.&lt;/p&gt;
&lt;p&gt;Lundy is their Crown Jewels: they’ve been looking after the island since the 1960s and now offer &lt;a href="https://www.landmarktrust.org.uk/Search-and-book/location/lundy/"&gt;23 self-catering properties there&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We took the ferry out on Tuesday morning (a truly horrific two hour voyage) and back again on Thursday evening (thankfully much calmer). Once on Lundy we stayed in &lt;a href="https://www.landmarktrust.org.uk/search-and-book/properties/castle-keep-south-5720"&gt;Castle Keep South&lt;/a&gt;, a two bedroom house in the keep of a castle built in the 13th century by Henry III, after he retook the island from the apparently traitorous William de Marisco (who was then hanged, drawed and quartered for good measure - apparently one of the &lt;a href="https://en.wikipedia.org/wiki/Hanged,_drawn_and_quartered#Treason_in_England"&gt;first ever uses&lt;/a&gt; of that punishment). Lundy has some &lt;em&gt;very&lt;/em&gt; interesting history attached to it.&lt;/p&gt;

&lt;div class="gallery"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_0029.JPG?w=200&amp;amp;auto=compress" alt="Marisco Castle" title="Marisco Castle" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_1278.JPG?w=200&amp;amp;auto=compress" alt="Rocks outside the castle" title="Rocks outside the castle" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_1939.JPG?w=200&amp;amp;auto=compress" alt="Inside the castle" title="Inside the castle" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_5999.JPG?w=200&amp;amp;auto=compress" alt="The castle" title="The castle" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_8959.JPG?w=200&amp;amp;auto=compress" alt="The castle on the cliffs" title="The castle on the cliffs" /&gt;
&lt;/div&gt;

&lt;p&gt;The island itself is utterly spectacular. Three miles long, half a mile wide, surrounded by craggy cliffs and mostly topped with ferns and bracken. Not a lot of trees except for the more sheltered eastern side. A charming population of sheep, goats, Lundy Ponies and some highland cattle with extremely intimidating horns.&lt;/p&gt;

&lt;div class="gallery"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_9502.jpg?w=200&amp;amp;auto=compress" alt="Scenery" title="Scenery" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_0275.JPG?w=200&amp;amp;auto=compress" alt="Scenery" title="Scenery" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_4340.JPG?w=200&amp;amp;auto=compress" alt="A fine highland cow" title="A fine highland cow" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_4346.JPG?w=200&amp;amp;auto=compress" alt="A Lundy pony" title="A Lundy pony" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_5270.JPG?w=200&amp;amp;auto=compress" alt="Dozy sheep" title="Dozy sheep" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_5358.JPG?w=200&amp;amp;auto=compress" alt="Goat" title="Goat" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_9368.jpg?w=200&amp;amp;auto=compress" alt="Bracken" title="Bracken" /&gt;
&lt;/div&gt;

&lt;div class="big-image"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_4319.jpg?w=800" alt="A highland cow that looks like Boris Johnson" title="Boris" style="max-width: 100%" /&gt;
&lt;/div&gt;

&lt;p&gt;(“They’re complete softies. We call that one Boris because he looks like Boris Johnson” - a lady who works in the Tavern)&lt;/p&gt;
&lt;p&gt;Lundy has three light houses (two operational, one retired), the aforementioned castle, a charming little village, a church and numerous fascinating ruins and isolated buildings, many of which you can stay in. It has the remains of two crashed WWII German Heinkel He 111 bombers (which we eventually tracked down).&lt;/p&gt;

&lt;div class="gallery"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_2618.JPG?w=200&amp;amp;auto=compress" alt="Stairs" title="Stairs" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7600.jpg?w=200&amp;amp;auto=compress" alt="An old lighthouse" title="The Old Light" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_0787.JPG?w=200&amp;amp;auto=compress" alt="A newer lighthouse" title="North Light" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_4349.JPG?w=200&amp;amp;auto=compress" alt="Rusty aircraft remains" title="Remains of a Heinkel" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_4351.JPG?w=200&amp;amp;auto=compress" alt="Isolated building" title="An isolated building" /&gt;
&lt;/div&gt;

&lt;p&gt;It also hosts what is quite possibly the world’s best Letterboxing trail.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Letterboxing_37"&gt;&lt;/a&gt;Letterboxing?&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Letterboxing_(hobby)"&gt;Letterboxing&lt;/a&gt; is an outdoor activity that is primarily pursued in the UK. It consists of weatherproof boxes hidden in remote locations, usually under a pile of rocks, containing a notebook and a custom stamp. The location of the boxes is provided by a set of clues. Given the clues, your challenge is to find all of the boxes and collect their stamps in your notebook.&lt;/p&gt;
&lt;p&gt;On Lundy the clues can be purchased from the village shop.&lt;/p&gt;
&lt;p&gt;I had dabbled with Letterboxing a tiny bit in the past but it hadn’t really clicked with me until Natalie (a keen letterboxer) encouraged us to give it a go on Lundy.&lt;/p&gt;
&lt;p&gt;It ended up occupying almost every waking moment of our time there, and taking us to every far-flung corner of the island.&lt;/p&gt;
&lt;p&gt;There are 28 letterboxes on Lundy. We managed to get 27 of them - and we would have got them all, if the last one hadn’t been located on a beach that was shut off from the public due to grey seals using it to raise their newly born pups! The pups were cute enough that we forgave them.&lt;/p&gt;

&lt;div class="gallery"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_2907.JPG?w=200&amp;amp;auto=compress" alt="A baby seal on a beach" title="Baby seal" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_3455.JPG?w=200&amp;amp;auto=compress" alt="No access: Seal Breeding Area sign" title="No access - Seal breeding area" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_3978.JPG?w=200&amp;amp;auto=compress" alt="A baby seal blocks the road" title="Baby seal blocking the road" /&gt;
&lt;/div&gt;

&lt;p&gt;To give you an idea for how it works, here’s the clue for letterbox 27, “The Ugly”:&lt;/p&gt;

&lt;div class="big-image"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7769.jpg?w=800" alt="The Ugly: From the lookout hut near the flagpole on the east side of Millcombe, walk in the direction of the South Light until it is on a bearing of 130°, Bramble Villa 210° and Millcombe due west. The letterbox is beneah you." style="max-width: 100%" /&gt;
&lt;/div&gt;

&lt;p&gt;There were letterboxes in lighthouses, letterboxes in ruins, letterboxes perilously close to cliff-faces, letterboxes in church pews, letterboxes in quarries, letterboxes in caves. If you thought that letterboxing was for kids, after scrabbling down more perilous cliff paths than I can count I can assure you it isn't!&lt;/p&gt;

&lt;div class="gallery"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_2179.JPG?w=200&amp;amp;auto=compress" alt="Natalie finds a letterbox" title="Natalie finds a letterbox" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_6548.JPG?w=200&amp;amp;auto=compress" alt="A letterbox near a cave" title="A letterbox near a cave" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7661.jpg?w=200&amp;amp;auto=compress" alt="Stamping a letterbox" title="Stamping a letterbox" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7727.jpg?w=200&amp;amp;auto=compress" alt="Hunting high and low" title="Hunting high and low" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7783.jpg?w=200&amp;amp;auto=compress" alt="Hannah found another one" title="Hannah found another one" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7820.jpg?w=200&amp;amp;auto=compress" alt="Adam finds one on a slope" title="Adam finds one on a slope" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7843.jpg?w=200&amp;amp;auto=compress" alt="Stamping a letterbox together" title="Stamping a letterbox together" /&gt;
&lt;/div&gt;

&lt;p&gt;On Thursday I clocked up 24,000 steps walking 11 miles and burned 1,643 calories. For comparison, when I ran the half marathon last year I only burned 1,222. These GPS tracks from my Apple Watch give a good impression of how far we ended up walking on our second day of searching.&lt;/p&gt;

&lt;div class="gallery" data-row-height="200"&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7935.PNG?w=200&amp;amp;auto=compress" alt="Apple watch GPS trace" title="GPS trace #1" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7936.PNG?w=200&amp;amp;auto=compress" alt="Apple watch GPS trace" title="GPS trace #2" /&gt;
    &lt;img src="https://simonwillisonnet.imgix.net/static/2018/lundy/IMG_7937.PNG?w=200&amp;amp;auto=compress" alt="Apple watch GPS trace" title="GPS trace #3" /&gt;
&lt;/div&gt;

&lt;p&gt;When we checked the letterboxing log book in the Tavern on Wednesday evening we found most people who attempt to hit all 28 letterboxes spread it out over a much more sensible timeframe. I’m not sure that I would recommend trying to fit it in to just two days, but it’s hard to imagine a better way of adding extra purpose to an exploration of the island.&lt;/p&gt;
&lt;p&gt;Should you attempt letterboxing on Lundy (and if you can get out there you really should consider it), a few tips:&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;If in doubt, look for the paths. Most of the harder to find letterboxes were at least located near an obvious worn path.&lt;/li&gt;
    &lt;li&gt;“Earthquake” is a nightmare. The clue really didn’t help us - we ended up performing a vigorous search of most of the area next to (not inside) the earthquake fault.&lt;/li&gt;
    &lt;li&gt;The iPhone compass app is really useful for finding bearings. We didn’t use a regular compass at all.&lt;/li&gt;
    &lt;li&gt;If you get stuck, check for extra clues in the letterboxing log book in the tavern. This helped us crack Earthquake.&lt;/li&gt;
    &lt;li&gt;There’s more than one pond. The quarry pond is very obvious once you find it.&lt;/li&gt;
    &lt;li&gt;Take as many different maps as you can find - many of the clues reference named landmarks that may not appear on the letterboxing clue map. We forgot to grab an offline copy of Lundy in the Google Maps app and regretted it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you find yourself in Ilfracombe on the way to or from Lundy, the &lt;a href="http://www.ilfracombemuseum.co.uk/"&gt;Ilfracombe Museum&lt;/a&gt; is well worth your time. It’s a classic example in the genre of “eccentric collects a wide variety of things, builds a museum for them”. Highlights include a cupboard full of pickled bats and a drawer full of 100-year-old wedding cake samples.&lt;/p&gt;</summary><category term="travel"></category></entry><entry><title>The subset of reStructuredText worth committing to memory</title><link href="http://simonwillison.net/2018/Aug/25/restructuredtext/#atom-entries" rel="alternate"></link><published>2018-08-25T18:44:29+00:00</published><updated>2018-08-25T18:44:29+00:00</updated><id>http://simonwillison.net/2018/Aug/25/restructuredtext/#atom-entries</id><summary type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/ReStructuredText"&gt;reStructuredText&lt;/a&gt; is the standard for documentation in the Python world.&lt;/p&gt;
&lt;p&gt;It’s a bit weird. It’s like &lt;a href="https://en.wikipedia.org/wiki/Markdown"&gt;Markdown&lt;/a&gt; but older, more feature-filled and in my experience signifcantly harder to remember.&lt;/p&gt;
&lt;p&gt;There are plenty of guides and cheatsheets out there, but when writing simple documentation for software projects I think there’s a subset that is worth committing to memory. I’ll describe that subset here.&lt;/p&gt;
&lt;p&gt;First though: when writing reStructuredText having a live preview render is extremely useful. I use &lt;a href="http://rst.ninjs.org/"&gt;rst.ninjs.org&lt;/a&gt; for this. If you don’t trust that hosted version (it round-trips your documentation through the server in order to render it) you can run a local copy instead using the &lt;a href="https://github.com/anru/rsted"&gt;underlying source code&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Paragraphs_10"&gt;&lt;/a&gt;Paragraphs&lt;/h3&gt;
&lt;p&gt;Paragraphs work the same way as Markdown and plain text. They are nice and easy.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;This is the first paragraph. No need to wrap the text (though you can wrap at e.g. 80 characters without affecting rendering).

This is the second paragraph.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="Headings_20"&gt;&lt;/a&gt;Headings&lt;/h3&gt;
&lt;p&gt;reStructuredText section headings are a little surprising.&lt;/p&gt;
&lt;p&gt;Markdown has multiple levels of heading, each with a different number of prefix hashes:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Markdown heading level 1
## Markdown heading level 2
..
###### Markdown heading fevel 6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In reStructuredText there is no single format for these different levels. Instead, the format you use first will be treated as an H1, the next format as an H2 and so on. Here’s the &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#sections"&gt;description from the official documentation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sections are identified through their titles, which are marked up with adornment: “underlines” below the title text, or underlines and matching “overlines” above the title. An underline/overline is a single repeated punctuation character that begins in column 1 and forms a line extending at least as far as the right edge of the title text. Specifically, an underline/overline character may be any non-alphanumeric printable 7-bit ASCII character. […] There may be any number of levels of section titles, although some output formats may have limits (HTML has 6 levels).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is deeply confusing. I suggest instead standardizing on the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;=====================
 This is a heading 1
=====================
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This heading has = signs both above and below, and they extend past the text by a single character in each direction.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;This is a heading 2
===================

This is a heading 3
-------------------

This is a heading 4
~~~~~~~~~~~~~~~~~~~
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you need more levels, you can invent them using whatever character you like - but try to stay consistent within your project.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Bulleted_lists_54"&gt;&lt;/a&gt;Bulleted lists&lt;/h3&gt;
&lt;p&gt;As with headings, you can use a variety of characters for these. I suggest sticking with asterisks.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;A blank line is required before starting a bulleted list.

* A bullet point
* Another bullet point
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you decide to wrap your text (I tend not to) you must maintain the indentation on the wrapped lines:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;* A bulleted list item. Since the text is wrapped each subsequent
  line of text must be indented by two spaces.
* Second list item.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Nested lists are supported, but you MUST leave a blank line above the first  inner list bullet point or they won't work:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;* This is the first bullet list item. Here comes a sub-list:

  * Hello sublist
  * Sublist two

* Back to the parent list.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="Inline_markup_78"&gt;&lt;/a&gt;Inline markup&lt;/h3&gt;
&lt;p&gt;I only use three inline markup features: bold, italic and code.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;**Bold text** is surrounded by two asterisks.

*Italic text* is one asterisk.

``inline code`` uses two backticks at either side of the code.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="Links_90"&gt;&lt;/a&gt;Links&lt;/h3&gt;
&lt;p&gt;Links are my least favorite feature of reStructuredText. There are several different ways of including them, but the one I use most often (and hence have committed to memory) is this one:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;`a link, note the trailing underscores &amp;lt;http://example.com&amp;gt;`__
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So that’s a backtick at the start, then the link text, then the URL contained in greater than / less than symbols, then another backtick and then TWO underscores to finish it off.&lt;/p&gt;
&lt;p&gt;Why two underscores? Because if you only use one, the text part of the link is remembered and can be used to duplicate your link later on - see example below. In my experience this is more trouble than it’s worth.&lt;/p&gt;
&lt;p&gt;A more complex link syntax example (&lt;a href="http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#embedded-uris-and-aliases"&gt;documented here&lt;/a&gt;) looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;See the `Python home page`_ for info.

This link_ is an alias to the link above.

.. _Python home page: http://www.python.org
.. _link: `Python home page`_
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can’t remember this at all, so I stick with the anonymous hyperlink syntax instead.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Code_blocks_111"&gt;&lt;/a&gt;Code blocks&lt;/h3&gt;
&lt;p&gt;The easiest way to embed a block of code is like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;::

    # This is a code example
    print(&amp;quot;It needs to be indented&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;::&lt;/code&gt; indicates that a code block is coming up. The blank line after the &lt;code&gt;::&lt;/code&gt; before the indentation starts is required.&lt;/p&gt;
&lt;p&gt;Most renderers have the ability to apply syntax highlighting. To specify that a block should have syntax highlighting for a specific language, replace the &lt;code&gt;::&lt;/code&gt; in the above example with one of the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.. code-block:: sql

.. code-block:: javascript

.. code-block:: python
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;&lt;a id="Images_130"&gt;&lt;/a&gt;Images&lt;/h3&gt;
&lt;p&gt;There are &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/directives.html#images"&gt;plenty of options&lt;/a&gt; for embedding images, but the most basic syntax (worth remembering) looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.. image:: full_text_search.png
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will embed an image of that filename that sits in the same directory as the document itself.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Internal_references_138"&gt;&lt;/a&gt;Internal references&lt;/h3&gt;
&lt;p&gt;In my opinion this is the key feature that makes reStructuredText more powerful than Markdown for larger documentation projects.&lt;/p&gt;
&lt;p&gt;Again, there in a vast and complex array of options around this, but the key thing to remember is how to add a reference name to a specific section and how to link to that section later on.&lt;/p&gt;
&lt;p&gt;Names are applied to section headings, by adding some magic text before the heading itself. For example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.. _full_text_search:

Full-text search
================
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note the format: two periods, then a space, then an underscore, then the label, then a colon at the end.&lt;/p&gt;
&lt;p&gt;The label &lt;code&gt;full_text_search&lt;/code&gt; is now associated with that heading. I can link to it from any page in my documentation project like so:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;:ref:`full_text_search`
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the leading underscore isn’t included in this reference.&lt;/p&gt;
&lt;p&gt;The link text displayed will be the text of the heading, in this case “Full-text search”. If I want to replace that link text with something custom, I can do so like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Learn about the :ref:`search feature &amp;lt;full_text_search&amp;gt;`.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This syntax is similar to the inline hyperlink syntax described above.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Learning_more_165"&gt;&lt;/a&gt;Learning more&lt;/h3&gt;
&lt;p&gt;I extracted the patterns I describe in this post from the &lt;a href="https://datasette.readthedocs.io/"&gt;Datasette documentation&lt;/a&gt; - I encourage you to &lt;a href="https://github.com/simonw/datasette/tree/master/docs"&gt;dig around in the source code&lt;/a&gt; to see how it all works.&lt;/p&gt;
&lt;p&gt;The definitive guide to reStructuredText is &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html"&gt;the reStructuredText Markup Specification&lt;/a&gt;. My favourite of the various quick references is the &lt;a href="https://thomas-cokelaer.info/tutorials/sphinx/rest_syntax.html"&gt;Restructured Text (reST) and Sphinx CheatSheet&lt;/a&gt; by Thomas Cokelaer.&lt;/p&gt;

&lt;p&gt;I'm a huge fan of &lt;a href="https://readthedocs.org/"&gt;Read the Docs&lt;/a&gt; for hosting documentation - it's the key reason I use reStructuredText in my projects. Unsurprisingly, they offer &lt;a href="https://docs.readthedocs.io/en/latest/"&gt;extensive documentation&lt;/a&gt; to help you make the most of their platform.&lt;/p&gt;</summary><category term="documentation"></category><category term="python"></category><category term="restructuredtext"></category></entry><entry><title>Analyzing US Election Russian Facebook Ads</title><link href="http://simonwillison.net/2018/Aug/6/russian-facebook-ads/#atom-entries" rel="alternate"></link><published>2018-08-06T16:01:18+00:00</published><updated>2018-08-06T16:01:18+00:00</updated><id>http://simonwillison.net/2018/Aug/6/russian-facebook-ads/#atom-entries</id><summary type="html">&lt;p&gt;Two interesting data sources have emerged in the past few weeks concerning the Russian impact on the 2016 US elections.&lt;/p&gt;
&lt;p&gt;FiveThirtyEight &lt;a href="https://fivethirtyeight.com/features/why-were-sharing-3-million-russian-troll-tweets/"&gt;published nearly 3 million tweets&lt;/a&gt; from accounts associated with the Russian “Internet Research Agency” - see &lt;a href="https://simonwillison.net/2018/Aug/6/troll-tweets/"&gt;my article and searchable tweet archive here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Separately, the House Intelligence Committee Minority &lt;a href="https://democrats-intelligence.house.gov/social-media-content/"&gt;released 3,517 Facebook ads&lt;/a&gt; that were reported to have been bought by the Russian Internet Research Agency as a set of redacted PDF files.&lt;/p&gt;
&lt;h3&gt;&lt;a id="Exploring_the_Russian_Facebook_Ad_spend_18"&gt;&lt;/a&gt;Exploring the Russian Facebook Ad spend&lt;/h3&gt;
&lt;p&gt;The initial data was released as &lt;a href="https://democrats-intelligence.house.gov/social-media-content/social-media-advertisements.htm"&gt;zip files full of PDFs&lt;/a&gt;, one of the least friendly formats you can use to publish data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://twitter.com/edsu"&gt;Ed Summers&lt;/a&gt; took on the intimidating task of cleaning that up. &lt;a href="https://github.com/edsu/irads"&gt;His results are incredible&lt;/a&gt;: he used the &lt;a href="https://pypi.org/project/pytesseract/"&gt;pytesseract OCR library&lt;/a&gt; and &lt;a href="https://pypi.org/project/PyPDF2/"&gt;PyPDF2&lt;/a&gt; to extract both the images and the associated metadata and convert the whole lot into a single 3.9MB JSON file.&lt;/p&gt;
&lt;p&gt;I &lt;a href="https://github.com/simonw/russian-ira-facebook-ads-datasette"&gt;wrote some code&lt;/a&gt; to convert his JSON file to SQLite (more on the details later) and the result can be found here:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://russian-ira-facebook-ads.datasettes.com/"&gt;https://russian-ira-facebook-ads.datasettes.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here’s an &lt;a href="https://russian-ira-facebook-ads.datasettes.com/russian-ads-919cbfd/display_ads?_search=cops&amp;amp;_sort_desc=spend_usd"&gt;example search for “cops” ordered by the USD equivalent spent on the ad&lt;/a&gt; (some of the spends are in rubles, so I convert those to USD using today’s exchange rate of 0.016).&lt;/p&gt;
&lt;p&gt;&lt;img style="max-width: 100%" src="https://static.simonwillison.net/static/2018/ads-cops-sorted-by-usd.png" alt="Search ads for cops, order by USD descending" /&gt;&lt;/p&gt;
&lt;p&gt;One of the most interesting things about this data is that it includes the Facebook ad targetting options that were used to promote the ads. I’ve built a separate interface for browsing those - you can see &lt;a href="https://russian-ira-facebook-ads.datasettes.com/russian-ads-919cbfd/top_targets"&gt;the most frequently applied targets&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img style="max-width: 100%" src="https://static.simonwillison.net/static/2018/top-targets.png" alt="Top targets" /&gt;&lt;/p&gt;
&lt;p&gt;And by browsing &lt;a href="https://russian-ira-facebook-ads.datasettes.com/russian-ads-919cbfd/faceted-targets?targets=%5B%22d6ade%22%5D"&gt;through the different facets&lt;/a&gt; you can construct e.g. a search for all ads that targeted people interested in both &lt;code&gt;interests:Martin Luther King&lt;/code&gt; and  &lt;code&gt;interests:Police Brutality is a Crime&lt;/code&gt;: &lt;a href="https://russian-ira-facebook-ads.datasettes.com/russian-ads-919cbfd/display_ads?_targets_json=%5B%22d6ade%22%2C%2240c27%22%5D"&gt;https://russian-ira-facebook-ads.datasettes.com/russian-ads-919cbfd/display_ads?_targets_json=[&amp;quot;d6ade&amp;quot;%2C&amp;quot;40c27&amp;quot;]&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a id="New_tooling_under_the_hood_40"&gt;&lt;/a&gt;New tooling under the hood&lt;/h3&gt;
&lt;p&gt;I ended up spinning up several new projects to help process and explore this data.&lt;/p&gt;
&lt;h4&gt;&lt;a id="sqliteutils_44"&gt;&lt;/a&gt;sqlite-utils&lt;/h4&gt;
&lt;p&gt;The first is a new library called &lt;a href="https://sqlite-utils.readthedocs.io/en/latest/"&gt;sqlite-utils&lt;/a&gt;. If data is already in CSV I tend to convert it using csvs-to-sqlite, but if data is in a less tabular format (JSON or XML for example) I have to hand-write code. Here’s &lt;a href="https://github.com/simonw/register-of-members-interests/blob/2baf75956b8b9e93a3985ebeb2259f7f2af760c8/convert_xml_to_sqlite.py"&gt;a script&lt;/a&gt; I wrote to process the XML version of &lt;a href="https://simonwillison.net/2018/Apr/25/register-members-interests/"&gt;the UK Register of Members Interests for example&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My goal with sqlite-utils is to take some of the common patterns from those scripts and make them as easy to use as possible, in particular when running inside a Jupyter notebook. It’s still very early, but &lt;a href="https://github.com/simonw/russian-ira-facebook-ads-datasette/blob/336ba87ef8071e664441ad0a95e3b8d0a33f682a/fetch_and_build_russian_ads.py"&gt;the script I wrote&lt;/a&gt; to process the Russian ads JSON is a good example of the kind of thing I want to do with it.&lt;/p&gt;
&lt;h4&gt;&lt;a id="datasettejsonhtml_50"&gt;&lt;/a&gt;datasette-json-html&lt;/h4&gt;
&lt;p&gt;The second new tool is a new Datasette plugin (and &lt;a href="https://github.com/simonw/datasette/issues/352"&gt;corresponding plugin hook&lt;/a&gt;) called &lt;a href="https://github.com/simonw/datasette-json-html"&gt;datasette-json-html&lt;/a&gt;. I used this to solve the need to display both rendered images and customized links as part of the regular Datasette instance.&lt;/p&gt;
&lt;p&gt;It’s a pretty crazy solution (hence why it’s implemented as a plugin and not part of Datasette core) but it works surprisingly well. The basic idea is to support a mini JSON language which can be detected and rendered as HTML. A couple of examples:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;img_src&amp;quot;: &amp;quot;https://raw.githubusercontent.com/edsu/irads/03fb4b/site/images/0771.png&amp;quot;,
  &amp;quot;width&amp;quot;: 200
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is rendered as an HTML &lt;code&gt;&amp;lt;img src=&amp;quot;&amp;quot;&amp;gt;&lt;/code&gt; element.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[
  {
    &amp;quot;label&amp;quot;: &amp;quot;location:United States&amp;quot;,
    &amp;quot;href&amp;quot;: &amp;quot;/russian-ads/display_ads?_target=ec3ac&amp;quot;
  },
  {
    &amp;quot;label&amp;quot;: &amp;quot;interests:Martin Luther King&amp;quot;,
    &amp;quot;href&amp;quot;: &amp;quot;/russian-ads/display_ads?_target=d6ade&amp;quot;
  },
  {
    &amp;quot;label&amp;quot;: &amp;quot;interests:Jr.&amp;quot;,
    &amp;quot;href&amp;quot;: &amp;quot;/russian-ads/display_ads?_target=8e7b3&amp;quot;
  }
]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Is rendered as a comma-separated list of HTML links.&lt;/p&gt;
&lt;p&gt;Why use JSON for this? Because SQLite has some &lt;a href="https://www.sqlite.org/json1.html"&gt;incredibly powerful JSON features&lt;/a&gt;, making it trivial to output JSON as part of the result of a SQL query. Most interestingly of all it has &lt;code&gt;json_group_array()&lt;/code&gt; which can work as an aggregation function to combine a set of related rows into a single JSON array.&lt;/p&gt;
&lt;p&gt;The display_ads page shown above is powered by a SQL view. Here’s the relevant subset of that view:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;select ads.id,
    case when image is not null then
        json_object(&amp;quot;img_src&amp;quot;, &amp;quot;https://raw.githubusercontent.com/edsu/irads/03fb4b/site/&amp;quot; || image, &amp;quot;width&amp;quot;, 200)
    else
        &amp;quot;no image&amp;quot;
    end as img,
    json_group_array(
        json_object(
            &amp;quot;label&amp;quot;, targets.name,
            &amp;quot;href&amp;quot;, &amp;quot;/russian-ads/display_ads?_target=&amp;quot;
                || urllib_quote_plus(targets.id)
        )
    ) as targeting
from ads
    join ad_targets on ads.id = ad_targets.ad_id
    join targets on ad_targets.target_id = targets.id
group by ads.id limit 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m using SQLite’s JSON functions to dynamically assemble the JSON format that datasette-json-html knows how to render. I’m delighted at how well it works.&lt;/p&gt;
&lt;p&gt;I’ve turned off arbitrary SQL querying against the main Facebook ads Datasette instance, but there’s a copy running at &lt;a href="https://russian-ira-facebook-ads-sql-allowed.now.sh/russian-ads"&gt;https://russian-ira-facebook-ads-sql-allowed.now.sh/russian-ads&lt;/a&gt; if you want to play with these queries.&lt;/p&gt;
&lt;h4&gt;&lt;a id="Weird_implementation_details_106"&gt;&lt;/a&gt;Weird implementation details&lt;/h4&gt;
&lt;p&gt;The full source code for my implementation &lt;a href="https://github.com/simonw/russian-ira-facebook-ads-datasette"&gt;is available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I ended up using &lt;a href="https://github.com/simonw/datasette/commit/5116c4ec8aed5091e1f75415424b80f613518dc6"&gt;an experimental plugin hook&lt;/a&gt; to enable additional custom filtering on Datasette views in order to support showing ads against multiple m2m targets, but hopefully that will be made unnecessary as work on Datasette’s &lt;a href="https://github.com/simonw/datasette/issues/354"&gt;support for m2m relationships&lt;/a&gt; progresses.&lt;/p&gt;
&lt;p&gt;I also experimented with YAML to generate the &lt;code&gt;metadata.json&lt;/code&gt; file as JSON strings aren’t a great way of &lt;a href="https://github.com/simonw/russian-ira-facebook-ads-datasette/blob/336ba87ef8071e664441ad0a95e3b8d0a33f682a/russian-ads-metadata.yaml"&gt;representing multi-line HTML and SQL&lt;/a&gt;. And if you want to see some &lt;em&gt;really&lt;/em&gt; convoluted SQL have a look at how the &lt;a href="https://github.com/simonw/russian-ira-facebook-ads-datasette/blob/336ba87ef8071e664441ad0a95e3b8d0a33f682a/russian-ads-metadata.yaml#L52-L81"&gt;canned query&lt;/a&gt; for the &lt;a href="https://russian-ira-facebook-ads.datasettes.com/russian-ads-919cbfd/faceted-targets?targets=%5B%22371f0%22%2C%22cc5ed%22%5D"&gt;faceted targeting interface&lt;/a&gt; works.&lt;/p&gt;
&lt;p&gt;This was a really fun project, which further stretched my ideas about what Datasette should be capable of out of the box. I’m hoping that the &lt;a href="https://github.com/simonw/datasette/issues/354"&gt;m2m work&lt;/a&gt; will make a lot of these crazy hacks redundant.&lt;/p&gt;</summary><category term="politics"></category><category term="projects"></category><category term="datasette"></category></entry></feed>
